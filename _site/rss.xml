<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
<channel>
        <title>bbw's blog</title>
        <description>bbw's blog - bbw</description>
        <link>http://bbwff.github.io</link>
        <link>http://bbwff.github.io</link>
        <lastBuildDate>2017-07-03T14:35:50+08:00</lastBuildDate>
        <pubDate>2017-07-03T14:35:50+08:00</pubDate>
        <ttl>1800</ttl>


        <item>
                <title>é˜¿é‡Œä¸­é—´ä»¶æŠ€æœ¯å¤§èµ›æ€»ç»“</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;ç ”ä¸€ä¸‹å­¦æœŸå‚åŠ äº†å¤©æ± ä¸­é—´ä»¶å¤§èµ›å’Œé˜¿é‡ŒéŸ³ä¹è¶‹åŠ¿é¢„æµ‹å¤§èµ›ï¼Œæ€»ç»“ä¸‹ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Black&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;list&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mark&lt;/code&gt;&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/%E9%A1%B9%E7%9B%AE/2017/07/03/%E9%98%BF%E9%87%8C%E4%B8%AD%E9%97%B4%E4%BB%B6%E6%8A%80%E6%9C%AF%E5%A4%A7%E8%B5%9B%E6%80%BB%E7%BB%93</link>
                <guid>http://bbwff.github.io/%E9%A1%B9%E7%9B%AE/2017/07/03/é˜¿é‡Œä¸­é—´ä»¶æŠ€æœ¯å¤§èµ›æ€»ç»“</guid>
                <pubDate>2017-07-03T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Decaé¡¹ç›®æ€»ç»“</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;Decaé¡¹ç›®æ˜¯ç ”ç©¶ç”ŸæœŸé—´å‚åŠ çš„é‡è¦ç§‘ç ”é¡¹ç›®ï¼Œé¡¹ç›®ä¸»è¦æ˜¯é‡‡ç”¨å»å¯¹è±¡åŒ–çš„æ€æƒ³ï¼Œå‡å°‘å¤§æ•°æ®å¹³å°åœ¨è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œæ•°æ®çš„å æœ‰ç©ºé—´ä¸å¯¹è±¡çš„æ•°é‡ï¼Œä»è€Œå‡å°å†…å­˜çš„å‹åŠ›ï¼Œä¹Ÿå‡å°GCçš„å‹åŠ›ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å®ç°çš„åŠŸèƒ½&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;decaä¸»è¦å®ç°çš„åŠŸèƒ½å°±æ˜¯å‡å°äº†å¤§æ•°æ®å¹³å°åœ¨è¿è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­çš„æ•°æ®åœ¨å†…å­˜ä¸­çš„å ç”¨é‡ä»¥åŠåœ¨è¿è¡Œè¿‡ç¨‹ä¸­å¯¹è±¡çš„æ•°é‡ã€‚&lt;/p&gt;

&lt;p&gt;å½“å‰çš„ä¸»æµåˆ†å¸ƒå¼å†…å­˜è®¡ç®—ç³»ç»Ÿå‡é‡‡ç”¨&lt;code class=&quot;highlighter-rouge&quot;&gt;é«˜çº§æ‰˜ç®¡è¯­è¨€&lt;/code&gt;å¼€å‘ï¼Œè¿™æ ·å¼€å‘è¿›åº¦å¿«ï¼Œæ–¹ä¾¿éƒ¨ç½²å’Œç»´æŠ¤ã€‚&lt;/p&gt;

&lt;p&gt;GCæ˜¯æ‰˜ç®¡è¯­è¨€ï¼ˆJAVA,SCALAç­‰ï¼‰çš„è¿è¡Œæ—¶ç³»ç»Ÿè‡ªä¸»ç®¡ç†å¯¹è±¡çš„åŸºç¡€ï¼ŒGCæ“ä½œä¼šæ£€ç´¢å½“å‰å †ä¸­å­˜æ´»çš„å¯¹è±¡ï¼Œå¹¶é‡Šæ”¾å·²ç»æ­»äº¡å¯¹è±¡çš„ç©ºé—´ã€‚&lt;/p&gt;

&lt;p&gt;å¤§é‡æ•°æ®å‡ä»¥å¯¹è±¡å½¢å¼å­˜æ”¾åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¹å¯¼è‡´ä¸¤ä¸ªé—®é¢˜&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;å†…å­˜è†¨èƒ€é—®é¢˜&lt;/p&gt;

    &lt;p&gt;å¯¹è±¡å½¢å¼çš„å†…å­˜å¸ƒå±€ä¼šå­˜å‚¨å¤§é‡å¼•ç”¨ç»“æ„å’Œå…ƒæ•°æ®ï¼ˆå¯¹è±¡å¤´ï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥å­˜å‚¨æ•°æ®ï¼Œç©ºé—´åˆ©ç”¨ç‡è¾ƒä½ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Full GC é—®é¢˜&lt;/p&gt;

    &lt;p&gt;å†…å­˜è†¨èƒ€ä¼šå¯¼è‡´JVMæ›´åŠ é¢‘ç¹çš„è§¦å‘full gcï¼ˆæ£€ç´¢æ•´ä¸ªJVMå †å†…å­˜ï¼‰ï¼Œè€ŒGCå¼€é”€ä¸&lt;code class=&quot;highlighter-rouge&quot;&gt;å­˜æ´»å¯¹è±¡æ•°é‡&lt;/code&gt;æˆæ­£æ¯”ï¼Œå¯¼è‡´GCæ—¶é—´è¿‡é•¿ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;è¿™é‡Œå¼•ç”³ä¸€ä¸‹gcçš„åˆ†ç±»ï¼Œgcåˆ†ä¸ºminor gc, major gc å’Œ full gcã€‚å…¶ä¸­minoræ˜¯æ¸…ç†å¹´è½»ä»£ï¼Œmajoræ˜¯æ¸…ç†è€å¹´ä»£ï¼Œè€Œfullæ˜¯æ¸…ç†æ•´ä¸ªå †ç©ºé—´ã€‚&lt;/p&gt;

&lt;p&gt;å› æ­¤åœ¨é¢ä¸´ä½¿ç”¨å†…å­˜ç©ºé—´æœ‰é™çš„æƒ…å†µä¸‹ï¼Œå¿…é¡»åœ¨è½¯ä»¶å±‚é¢å¯¹å†…å­˜ç®¡ç†è¿›è¡Œä¼˜åŒ–ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æŠ€æœ¯å’Œæ¶æ„&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;é—®é¢˜åˆ†æ&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;sparkæ˜¯å¼€æºç³»ç»Ÿä¸­ä¸»å¯¼çš„æ•°æ®å¹¶è¡Œè®¡ç®—æ¡†æ¶ï¼Œæä¾›å‡½æ•°å¼ç¼–ç¨‹æ¨¡å‹RDDï¼Œå¹¶å¢åŠ äº†åŸºäºshuffleçš„GroupByç³»åˆ—è¿ç®—ç¬¦æ‰©å±•ï¼Œæ”¯æŒä¸­é—´æ•°æ®çš„å†…å­˜ç¼“å­˜å’ŒåŸºäºå“ˆå¸Œçš„shuffleèšåˆæ“ä½œã€‚&lt;/p&gt;

&lt;p&gt;sparkå°†æ•°æ®å°è£…åœ¨RDDä¸­ï¼Œç„¶åé€šè¿‡actionåˆ’åˆ†jobï¼Œå†é€šè¿‡shuffleæ“ä½œåˆ’åˆ†stageï¼Œç„¶ååœ¨jvmä¸­è¿è¡Œæ•°æ®ã€‚&lt;/p&gt;

&lt;p&gt;å› æ­¤sparkä¸­çš„å†…å­˜ä¸»è¦åˆ†ä¸ºä¸‰éƒ¨åˆ†ã€‚&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Cache RDDåˆ°å†…å­˜&lt;/p&gt;

    &lt;p&gt;è¿™éƒ¨åˆ†å†…å­˜éœ€è¦ä¸€ç›´ç»´æŠ¤ï¼Œåªè¦ç”¨æˆ·è¿›è¡Œunpersistæ“ä½œï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†å†…å­˜ç”Ÿå‘½å‘¨æœŸè¾ƒé•¿ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;shuffleæ“ä½œ(ç”Ÿå‘½å‘¨æœŸæ˜¯ä¸€ä¸ªstage)&lt;/p&gt;

    &lt;p&gt;shuffleæ“ä½œéœ€è¦è½ç£ç›˜ï¼Œè¿›è¡Œç£ç›˜I/O,å› æ­¤éœ€è¦ç»´æŠ¤æ‰€æœ‰ç£ç›˜I/Oçš„æ•°æ®ï¼Œç”Ÿå‘½å‘¨æœŸä¹Ÿé•¿ã€‚&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;stageå†…éƒ¨çš„æ“ä½œ&lt;/p&gt;

    &lt;p&gt;äº§ç”Ÿå¤§é‡çš„ä¸´æ—¶å¯¹è±¡ï¼Œå±äºå†…å­˜ä¸­çš„ä¸´æ—¶å¯¹è±¡ï¼Œå¾ˆå¿«ä¼šè¢«gcå›æ”¶ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;é•¿æ—¶é—´å­˜æ´»å¯¹è±¡&lt;/code&gt;ä¼šä¸€ç›´æ´»åœ¨å†…å­˜ä¸­ï¼Œæ¯æ¬¡Full GC è¦æ‰«æçš„å¯¹è±¡æ•°é‡å¾ˆå¤šï¼Œè®¡ç®—å¼€é”€å¾ˆå¤§ã€‚è€Œä¸”å¯¹è±¡ä¸€ç›´å­˜æ´»ï¼Œä¼šå¤§é‡å ç”¨å†…å­˜ï¼Œé¢‘ç¹å¯¼è‡´full gcã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;æ–¹æ³•è®¾è®¡&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æ ¸å¿ƒæ€æƒ³æ˜¯å‡å°‘æ•°æ®&lt;code class=&quot;highlighter-rouge&quot;&gt;å¯¹è±¡çš„æ•°é‡&lt;/code&gt;ï¼Œè€Œéæ•°æ®çš„å¤§å°ã€‚&lt;/p&gt;

&lt;p&gt;ä½¿ç”¨å¯¹è±¡æ‹†è§£ï¼Œæš´éœ²å‡ºæ•°æ®å¯¹è±¡ä¸­çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;è£¸æ•°æ®&lt;/code&gt;ï¼šåŸç”Ÿå­—æ®µç±»å‹ï¼›å»é™¤å¯¹è±¡å¤´å’Œå¼•ç”¨ç»“æ„ã€‚&lt;/p&gt;

&lt;p&gt;åŸºäºç”Ÿå‘½å‘¨æœŸçš„å†…å­˜ç®¡ç†ï¼šå°†ç›¸åŒ/ç›¸è¿‘ç”Ÿå‘½å‘¨æœŸçš„ä¸€ç»„æ•°æ®å¯¹è±¡ä¸­çš„è£¸æ•°æ®å­˜æ”¾åœ¨è¿ç»­çš„å†…å­˜å—ï¼ˆæ•°ç»„ï¼‰ä¸­ã€‚&lt;/p&gt;

&lt;p&gt;æ•°æ®æ— éœ€è®¿é—®æ—¶å³å¯ä¸€æ¬¡å›æ”¶æ•´ä¸ªå†…å­˜å—ç©ºé—´ã€‚&lt;/p&gt;

&lt;p&gt;è¿™æ ·GCçš„ç´¢å¼•ç”±å¤§é‡çš„å¯¹è±¡å˜ä¸ºå°‘é‡çš„å®¹å™¨ï¼Œgcå¼€é”€å¤§å¤§å‡å°ã€‚&lt;/p&gt;

&lt;p&gt;å°†UDT(ç”¨æˆ·å®šä¹‰ç±»å‹ï¼‰åˆ†ä¸ºä¸‰ç±»ï¼š&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;é™æ€å®šé•¿ï¼šåŸç”Ÿç±»å‹åŠå…¶ç»„åˆï¼Œå¦‚int, long, (int, long)&lt;/li&gt;
  &lt;li&gt;åŠ¨æ€å®šé•¿ï¼šåŸç”Ÿç±»å‹çš„æ•°ç»„åŠç»„åˆï¼Œå¦‚int[], (int[], long)&lt;/li&gt;
  &lt;li&gt;å˜é•¿å¯¹è±¡å’Œé€’å½’ç±»å‹å¯¹è±¡ï¼šå®ä¾‹åŒ–å¯¹è±¡é•¿åº¦ä¸ç¡®å®šï¼Œå¦‚TreeNodeï¼ˆTreeNodeé‡Œé¢æœ‰ä¸€ä¸ªTreeNodeå¼•ç”¨çš„left,right)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;å‰ä¸¤ç§å¯ä»¥å®‰å…¨æ‹†è§£ï¼Œç¬¬ä¸‰ç§ä¸è¡Œã€‚&lt;/p&gt;

&lt;p&gt;é’ˆå¯¹ä»¥ä¸Šçš„ä¸‰ç§å†…å­˜ï¼Œå…¶ä¸­cache RDDï¼Œå½“cacheçš„æ•°æ®å¯¹è±¡å¯ä»¥æ‹†è§£æ—¶å€™ï¼Œå¯ä»¥æ‹†è§£ä¸ºBytesæ•°ç»„ä¾æ¬¡å­˜æ”¾åœ¨pageä¸­ï¼ŒåŒæ—¶æ ¹æ®pageå¯¹è±¡ä¸­å¯¹è±¡offsetå¯ä»¥è·å¾—å¯¹è±¡æˆå‘˜å˜é‡ã€‚&lt;/p&gt;

&lt;p&gt;è€Œé’ˆå¯¹shuffleå†…å­˜ï¼Œä¹Ÿæ˜¯æ”¾åœ¨pageé‡Œé¢ï¼Œé’ˆå¯¹shuffleé˜¶æ®µçš„æ’åºï¼Œä½¿ç”¨æŒ‡é’ˆï¼Œé¿å…å¤§é‡æ•°æ®çš„ç§»åŠ¨ã€‚&lt;/p&gt;

&lt;p&gt;åœ¨shuffle é˜¶æ®µå­˜åœ¨å¾ˆå¤šå˜é•¿çš„æˆå‘˜ï¼Œåœ¨shuffleé˜¶æ®µï¼ŒreduceByKeyå°šèƒ½æ‹†è§£ï¼Œå› ä¸ºreduceä¹‹åçš„valueä¾ç„¶æ˜¯å®šé•¿çš„ã€‚ä½†æ˜¯é’ˆå¯¹groupByKeyè¿™ä¸ªç®—å­ï¼Œä»–çš„æ“ä½œå¯¹è±¡æ˜¯ï¼ˆK,combinerBuffer)ï¼Œcombineræ˜¯å˜é•¿çš„ï¼Œgroupä¹‹åä¹Ÿæ˜¯å˜é•¿ï¼Œæ˜¯ä¸ç¡®å®šçš„ã€‚&lt;/p&gt;

&lt;p&gt;Spark-1.4é‡ŒgroupByKeyåœ¨shuffle writeç«¯å¯ä»¥åˆ©ç”¨åˆ°å †å¤–çš„å†…å­˜ï¼Œä¹Ÿå°±æ˜¯tungsten-sortï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½ä¼šå†™åœ¨å †å¤–å¹¶åœ¨å †å¤–æ’åºï¼Œä½†æ˜¯shuffle-readç«¯Sparké»˜è®¤è¿˜æ˜¯ç”¨çš„HashShuffleReader,æ‰€æœ‰çš„èšåˆæ“ä½œéƒ½åœ¨å †å†…å®Œæˆï¼Œè¿™ä¸ªæˆ‘ä»¬å·²ç»å®ç°äº†readç«¯çš„å †å¤–ç‰ˆæœ¬ï¼Œèšåˆæ“ä½œè¿è¡Œåœ¨å †å¤–ã€‚å¤§è‡´ä»‹ç»ä¸‹åŸç†ï¼Œè¿™é‡Œå°±ç”¨åˆ°äº†VSTæ‹†è§£çš„åŸç†ï¼Œæˆ‘ä»¬çŸ¥é“shuffle readç«¯è¯»å‡ºæ¥çš„(K,C)å¯¹çš„åŸºæœ¬ç±»å‹ï¼Œäºæ˜¯å…ˆå®ç°äº†ä¸€ä¸ªç®€æ˜“çš„map(UnsafeUnfixedWidthAggregationFlintMap),å—¯åå­—ç•¥é•¿ã€‚ã€‚è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç³»ç»Ÿçš„å®šåˆ¶çš„mapï¼Œä¹Ÿæ˜¯ç”¨åˆ°äº†HashåŸç†ï¼Œä¸è¿‡æ‰€æœ‰æ“ä½œéƒ½æ˜¯åœ¨å †å¤–è¿›è¡Œã€‚è¿™ä¸ªmapç”¨äºå­˜å‚¨keyå’Œ&lt;strong&gt;valueAddress&lt;/strong&gt;ï¼Œè¿™ä¸ª&lt;strong&gt;valueAddress&lt;/strong&gt;æ˜¯ä¸€ä¸ªlongå‹å€¼ï¼Œæˆ‘ä»¬ä¼šå°†Kå¯¹åº”çš„ä¸€ç»„Valueåœ¨å †å¤–å¼€è¾Ÿä¸€ç‰‡ç–†åœŸç”¨äºå­˜å‚¨ä»–ä»¬ï¼Œå½“ç„¶æ¯æ¬¡æ–°æ¥valueæ—¶æˆ‘ä»¬ä¼šæ£€æŸ¥æ˜¯å¦æ‰©å®¹ï¼Œè‹¥æ‰©å®¹ä¼šæ”¹å˜è¿™å—ç–†åœŸ(å †å¤–ç©ºé—´)çš„èµ·å§‹åœ°å€ï¼Œå› ä¸ºæ¶‰åŠåˆ°å†…å­˜çš„æ‹·è´ï¼Œæ‰€ä»¥mapä¸­çš„&lt;strong&gt;valueAddress&lt;/strong&gt;å°±æ˜¯è¿™å—å­˜å‚¨åŒºåŸŸçš„åˆå§‹åç§»åœ°å€ã€‚&lt;strong&gt;valueAddress&lt;/strong&gt;æŒ‡å‘çš„å­˜å‚¨åŒºåŸŸç»“æ„ä¸ºï¼š&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://kzx1025.github.io/img/map.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;å¦‚æœæ¯ä¸ªpartitionç›¸åŒçš„keyä¸å¤šï¼Œè€Œä¸”æ¯ä¸ªkeyå­˜åœ¨å¤§é‡valueæ—¶ï¼Œé‡‡ç”¨mapsideCombineçš„groupBykeyæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚å¦‚æœä¸å­˜åœ¨hot keyï¼Œé‚£æ”¶ç›Šå°±å¾ˆå°ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ‹…å½“çš„è´£ä»»&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ä¸»è¦æ˜¯æ‹…ä»»shuffle groupByKey read é˜¶æ®µçš„å†…å­˜ä¼˜åŒ–ï¼Œæˆ‘å®ç°äº†readç«¯çš„å †å¤–ç‰ˆæœ¬ï¼Œèšåˆæ“ä½œè¿è¡Œåœ¨å †å¤–ã€‚å¤§è‡´ä»‹ç»ä¸‹åŸç†ï¼Œè¿™é‡Œå°±ç”¨åˆ°äº†VSTæ‹†è§£çš„åŸç†ï¼Œæˆ‘ä»¬çŸ¥é“shuffle readç«¯è¯»å‡ºæ¥çš„(K,C)å¯¹çš„åŸºæœ¬ç±»å‹ï¼Œäºæ˜¯å…ˆå®ç°äº†ä¸€ä¸ªç®€æ˜“çš„map(UnsafeUnfixedWidthAggregationFlintMap),å—¯åå­—ç•¥é•¿ã€‚ã€‚è¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç³»ç»Ÿçš„å®šåˆ¶çš„mapï¼Œä¹Ÿæ˜¯ç”¨åˆ°äº†HashåŸç†ï¼Œä¸è¿‡æ‰€æœ‰æ“ä½œéƒ½æ˜¯åœ¨å †å¤–è¿›è¡Œã€‚è¿™ä¸ªmapç”¨äºå­˜å‚¨keyå’Œ&lt;strong&gt;valueAddress&lt;/strong&gt;ï¼Œè¿™ä¸ª&lt;strong&gt;valueAddress&lt;/strong&gt;æ˜¯ä¸€ä¸ªlongå‹å€¼ï¼Œæˆ‘ä»¬ä¼šå°†Kå¯¹åº”çš„ä¸€ç»„Valueåœ¨å †å¤–å¼€è¾Ÿä¸€ç‰‡ç–†åœŸç”¨äºå­˜å‚¨ä»–ä»¬ï¼Œå½“ç„¶æ¯æ¬¡æ–°æ¥valueæ—¶æˆ‘ä»¬ä¼šæ£€æŸ¥æ˜¯å¦æ‰©å®¹ï¼Œè‹¥æ‰©å®¹ä¼šæ”¹å˜è¿™å—ç–†åœŸ(å †å¤–ç©ºé—´)çš„èµ·å§‹åœ°å€ï¼Œå› ä¸ºæ¶‰åŠåˆ°å†…å­˜çš„æ‹·è´ï¼Œæ‰€ä»¥mapä¸­çš„&lt;strong&gt;valueAddress&lt;/strong&gt;å°±æ˜¯è¿™å—å­˜å‚¨åŒºåŸŸçš„åˆå§‹åç§»åœ°å€ã€‚&lt;/p&gt;

&lt;p&gt;è¿™æ ·å°±å¤„ç†äº†å˜é•¿ç±»å‹çš„å¤„ç†ï¼Œä¹‹å‰æ˜¯åªå®ç°äº†reduceBykeyçš„ã€‚&lt;/p&gt;

&lt;p&gt;åé¢æˆ‘ä»¬ä¹Ÿå°è¯•äº†&lt;code class=&quot;highlighter-rouge&quot;&gt;åˆ—å¼å­˜å‚¨&lt;/code&gt;ï¼ŒæŠŠä¹‹å‰pageä¸­çš„æ•°ç»„å½¢å¼ï¼Œè½¬æ¢ä¸ºåˆ—å¼å­˜å‚¨ã€‚&lt;/p&gt;

&lt;p&gt;åŒæ—¶è´Ÿè´£å®éªŒçš„è®¾è®¡ï¼Œå®éªŒè¿‡ç¨‹ä¸­é‡åˆ°bugçš„è§£å†³ä»¥åŠgcç»Ÿè®¡åˆ†æçš„å·¥ä½œã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;éš¾ç‚¹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;sparkæ˜¯æƒ°æ€§æ‰§è¡Œï¼Œä»£ç ä¸­å……æ»¡ç€å„ç§å„æ ·çš„è¿­ä»£å™¨ï¼Œè¿½è¸ªä»£ç æ—¶éƒ½ä¸çŸ¥é“å“ªä¸ªè¿­ä»£å™¨è¢«è°ƒç”¨äº†ã€‚ä¿®æ”¹ä»£ç éœ€è¦è¿ç¯çš„ä¿®æ”¹å¤šä¸ªæ–‡ä»¶ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åå°±æ˜¯æœ‰æ—¶å€™å•æœºæµ‹è¯•å¯ä»¥é€šè¿‡ï¼Œä½†æ˜¯åˆ†å¸ƒå¼æ—¶å€™å°±ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;æ”¶è·&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mark&lt;/code&gt;&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/%E9%A1%B9%E7%9B%AE/2017/07/01/deca%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93</link>
                <guid>http://bbwff.github.io/%E9%A1%B9%E7%9B%AE/2017/07/01/decaé¡¹ç›®æ€»ç»“</guid>
                <pubDate>2017-07-01T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Facebookçš„å®æ—¶å¤„ç†</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#å¼•è¨€&quot; id=&quot;markdown-toc-å¼•è¨€&quot;&gt;å¼•è¨€&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#å‚è€ƒæ–‡çŒ®&quot; id=&quot;markdown-toc-å‚è€ƒæ–‡çŒ®&quot;&gt;å‚è€ƒæ–‡çŒ®&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;è¿™æ˜¯ç¿»è¯‘çš„ä¸€ç¯‡sigmod2016çš„è®ºæ–‡ï¼Œè§‰å¾—è›®æœ‰æ„ä¹‰ï¼Œå°±å‡†å¤‡ç¿»è¯‘è®°å½•ä¸€ä¸‹ã€‚&lt;/p&gt;

&lt;h3 id=&quot;å¼•è¨€&quot;&gt;å¼•è¨€&lt;/h3&gt;

&lt;p&gt;å®æ—¶å¤„ç†ç³»ç»Ÿçš„è®¾è®¡éœ€è¦è€ƒè™‘ä¸€ä¸‹å‡ ä¸ªæ–¹é¢ã€‚&lt;/p&gt;

&lt;p&gt;1ã€ä¾¿äºä½¿ç”¨ã€‚ä¾¿äºå¼€å‘åº”ç”¨ã€‚&lt;/p&gt;

&lt;p&gt;2ã€æ€§èƒ½ã€‚å¤šå°‘çš„å»¶è¿Ÿæ˜¯å¯ä»¥æ¥å—çš„ã€‚&lt;/p&gt;

&lt;p&gt;3ã€å®¹é”™ã€‚ä»€ä¹ˆé”™è¯¯å¯ä»¥å®¹å¿ï¼Œ&lt;/p&gt;

&lt;p&gt;4ã€å¯æ‰©å±•ã€‚å¹¶è¡Œã€‚&lt;/p&gt;

&lt;p&gt;5ã€æ­£ç¡®æ€§ã€‚ä¿è¯ä¸€å®šçš„å‡†ç¡®åº¦ã€‚&lt;/p&gt;

&lt;p&gt;è¿™ç¯‡æ–‡ç« ä»‹ç»å››æ¡æ•°æ®æµæ°´çº¿ã€‚&lt;/p&gt;

&lt;p&gt;1ã€chorosï¼Œè¿™æ˜¯ç”¨äºæ±‡èšFacebookç”¨æˆ·çš„å£°éŸ³ï¼šæ¯”å¦‚top5æœ€çƒ­è¯é¢˜ï¼Œè¿˜æœ‰ä¸–ç•Œæ¯ç²‰ä¸æ•°ç­‰ç­‰ã€‚&lt;/p&gt;

&lt;p&gt;2ã€ç§»åŠ¨ç«¯åˆ†æï¼Œç»™facebookå¼€å‘è€…åé¦ˆï¼Œç”¨æ¥è¯Šæ–­æ€§èƒ½å’Œå‡†ç¡®åº¦ï¼Œæ¯”å¦‚ä»€ä¹ˆæ—¶å€™æ˜¯cold timeï¼Œä»¥åŠFacebookå®¢æˆ·ç«¯çš„å´©æºƒç‡ã€‚&lt;/p&gt;

&lt;p&gt;3ã€ä¸ºæ¯ä¸ªç”¨æˆ·ç•Œé¢æä¾›å®æ—¶çš„èµï¼Œé˜…è¯»é‡ï¼Œè®¢é˜…é‡ã€‚&lt;/p&gt;

&lt;p&gt;4ã€&lt;/p&gt;

&lt;h3 id=&quot;å‚è€ƒæ–‡çŒ®&quot;&gt;å‚è€ƒæ–‡çŒ®&lt;/h3&gt;

&lt;p&gt;Chen, Guoqiang Jerry, et al. â€œRealtime data processing at Facebook.â€Â &lt;em&gt;Proceedings of the 2016 International Conference on Management of Data&lt;/em&gt;. ACM, 2016.&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/paper%20translation/2017/02/17/FaceBook%E7%9A%84%E5%AE%9E%E6%97%B6%E5%A4%84%E7%90%86</link>
                <guid>http://bbwff.github.io/paper%20translation/2017/02/17/FaceBookçš„å®æ—¶å¤„ç†</guid>
                <pubDate>2017-02-17T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>ä»£ç å°æŠ€å·§</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#é€»è¾‘è¿ç®—çš„çŸ­è·¯&quot; id=&quot;markdown-toc-é€»è¾‘è¿ç®—çš„çŸ­è·¯&quot;&gt;é€»è¾‘è¿ç®—çš„çŸ­è·¯&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#åŠ æ³•çš„å®ç°&quot; id=&quot;markdown-toc-åŠ æ³•çš„å®ç°&quot;&gt;åŠ æ³•çš„å®ç°&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#æ•°å€¼ç±»å‹çš„èŒƒå›´&quot; id=&quot;markdown-toc-æ•°å€¼ç±»å‹çš„èŒƒå›´&quot;&gt;æ•°å€¼ç±»å‹çš„èŒƒå›´&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#é€’å½’&quot; id=&quot;markdown-toc-é€’å½’&quot;&gt;é€’å½’&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#å‡å°‘æ²¡ç”¨åˆ†æ”¯å¯¹æ— ç”¨ä»£ç è¿›è¡Œåˆå¹¶&quot; id=&quot;markdown-toc-å‡å°‘æ²¡ç”¨åˆ†æ”¯å¯¹æ— ç”¨ä»£ç è¿›è¡Œåˆå¹¶&quot;&gt;å‡å°‘æ²¡ç”¨åˆ†æ”¯ï¼Œå¯¹æ— ç”¨ä»£ç è¿›è¡Œåˆå¹¶&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;åœ¨åšç‰›å®¢çš„åœ¨çº¿ç¼–ç¨‹é¢˜ï¼Œæœ‰äº›æ¯”è¾ƒå·§å¦™åœ°æŠ€å·§&lt;/p&gt;

&lt;h2 id=&quot;é€»è¾‘è¿ç®—çš„çŸ­è·¯&quot;&gt;é€»è¾‘è¿ç®—çš„çŸ­è·¯&lt;/h2&gt;

&lt;p&gt;ä»£ç é‡Œé¢çš„é€»è¾‘ä¸ï¼Œ&lt;em&gt;&amp;amp;&amp;amp;&lt;/em&gt;ï¼Œåªè¦å‰é¢å°±æ˜¯falseï¼Œåé¢å°±ä¸ä¼šæ‰§è¡Œäº†ï¼Œå¿…å®šå°±æ˜¯falseã€‚å¦‚æœå‰é¢æ˜¯çœŸï¼Œé‚£ä¹ˆåé¢è¿˜è¦æ‰§è¡Œï¼Œåé¢çš„çœŸå‡å†³å®šæ•´ä½“çš„çœŸå‡ã€‚&lt;/p&gt;

&lt;p&gt;æˆ–æ“ä½œï¼Œå¦‚æœå‰é¢ä¸ºçœŸï¼Œåé¢å°±ä¸ç”¨æ‰§è¡Œäº†ã€‚å‰é¢ä¸ºå‡ï¼Œåé¢å°±æ‰§è¡Œã€‚&lt;/p&gt;

&lt;p&gt;è¿™é‡Œæœ‰ä¸€é“é¢˜ç›®ã€‚&lt;/p&gt;

&lt;p&gt;æ±‚1+2+3+â€¦+nï¼Œè¦æ±‚ä¸èƒ½ä½¿ç”¨ä¹˜é™¤æ³•ã€forã€whileã€ifã€elseã€switchã€caseç­‰å…³é”®å­—åŠæ¡ä»¶åˆ¤æ–­è¯­å¥ï¼ˆA?B:Cï¼‰ã€‚&lt;/p&gt;

&lt;p&gt;è¿™é‡Œä¸èƒ½ä½¿ç”¨é‚£äº›åˆ¤æ–­å’Œå¾ªç¯çš„è¯­å¥ã€‚é‚£ä¹ˆæ€æ ·æ‰èƒ½å¾ªç¯æˆ–è€…åˆ¤æ–­é€’å½’æ±‚å’Œå‘¢ï¼Ÿ&lt;/p&gt;

&lt;p&gt;å¦‚æœèƒ½ä½¿ç”¨if else é‚£å°±å¾ˆç®€å•äº†ã€‚å°±æ˜¯n=1æ—¶å€™è¿”å›1ç»“æŸï¼Œå…¶ä»–å°±é€’å½’è°ƒç”¨ã€‚&lt;/p&gt;

&lt;p&gt;ä½†æ˜¯ä¸èƒ½ç”¨if elseï¼Œä¸‹é¢çš„ä»£ç å¾ˆå·§å¦™ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public int Sum_Solution(int n) {
    int sum=n;
    boolean flag=(sum&amp;gt;0)&amp;amp;&amp;amp;((sum+=Sum_Solution(n-1))&amp;gt;0);
    return sum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œï¼Œå°±æ˜¯å…ˆè®© sum=n,ç¬¬äºŒå¥æ˜¯ä¸è¯­å¥ï¼Œå¦‚æœsum&amp;gt;0ä¸æˆç«‹ï¼Œä¹Ÿå°±æ˜¯n=0æ—¶å€™ï¼Œåé¢çš„sum+=Sum_solution(n-1)å°±ä¸ä¼šæ‰§è¡Œï¼Œå°±ç›¸å½“äºåˆ°è¿™é‡Œæ˜¯æ ˆé¡¶äº†ã€‚ç„¶åå…¶ä»–æ—¶å€™sum&amp;gt;0éƒ½æ˜¯æˆç«‹çš„ï¼Œéƒ½ä¼šæ‰§è¡Œåé¢çš„ä»£ç ã€‚&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;æ‰€ä»¥è¿™é‡Œä¹Ÿå¯ä»¥ç”¨*&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;*æ¥å®ç°ã€‚å¦‚ä¸‹ã€‚&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public int Sum_Solution(int n) {
    int sum=n;
    boolean flag=(sum==00)||((sum+=Sum_Solution(n-1))&amp;gt;0);
    return sum;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;åŠ æ³•çš„å®ç°&quot;&gt;åŠ æ³•çš„å®ç°&lt;/h2&gt;

&lt;p&gt;è¿™ä¸ªåº”è¯¥æ˜¯ç»„æˆåŸç†çš„å†…å®¹å§ã€‚ã€‚æœ‰åŠ æ³•å™¨çš„ç¡¬ä»¶å®ç°è¿‡ç¨‹ã€‚ä¸è¿‡æˆ‘å·²ç»å¿˜äº†ã€‚&lt;/p&gt;

&lt;p&gt;é¢˜ç›®å¦‚ä¸‹ï¼šå†™ä¸€ä¸ªå‡½æ•°ï¼Œæ±‚ä¸¤ä¸ªæ•´æ•°ä¹‹å’Œï¼Œè¦æ±‚åœ¨å‡½æ•°ä½“å†…ä¸å¾—ä½¿ç”¨+ã€-ã€*ã€/å››åˆ™è¿ç®—ç¬¦å·ã€‚&lt;/p&gt;

&lt;p&gt;çœ‹äº†ä¸‹æœ‰äººçš„åˆ†æï¼š&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;é¦–å…ˆçœ‹åè¿›åˆ¶æ˜¯å¦‚ä½•åšçš„ï¼š 5+7=12ï¼Œä¸‰æ­¥èµ°
ç¬¬ä¸€æ­¥ï¼šç›¸åŠ å„ä½çš„å€¼ï¼Œä¸ç®—è¿›ä½ï¼Œå¾—åˆ°2ã€‚
ç¬¬äºŒæ­¥ï¼šè®¡ç®—è¿›ä½å€¼ï¼Œå¾—åˆ°10. å¦‚æœè¿™ä¸€æ­¥çš„è¿›ä½å€¼ä¸º0ï¼Œé‚£ä¹ˆç¬¬ä¸€æ­¥å¾—åˆ°çš„å€¼å°±æ˜¯æœ€ç»ˆç»“æœã€‚

ç¬¬ä¸‰æ­¥ï¼šé‡å¤ä¸Šè¿°ä¸¤æ­¥ï¼Œåªæ˜¯ç›¸åŠ çš„å€¼å˜æˆä¸Šè¿°ä¸¤æ­¥çš„å¾—åˆ°çš„ç»“æœ2å’Œ10ï¼Œå¾—åˆ°12ã€‚

åŒæ ·æˆ‘ä»¬å¯ä»¥ç”¨ä¸‰æ­¥èµ°çš„æ–¹å¼è®¡ç®—äºŒè¿›åˆ¶å€¼ç›¸åŠ ï¼š 5-101ï¼Œ7-111 ç¬¬ä¸€æ­¥ï¼šç›¸åŠ å„ä½çš„å€¼ï¼Œä¸ç®—è¿›ä½ï¼Œå¾—åˆ°010ï¼ŒäºŒè¿›åˆ¶æ¯ä½ç›¸åŠ å°±ç›¸å½“äºå„ä½åšå¼‚æˆ–æ“ä½œï¼Œ101^111ã€‚

ç¬¬äºŒæ­¥ï¼šè®¡ç®—è¿›ä½å€¼ï¼Œå¾—åˆ°1010ï¼Œç›¸å½“äºå„ä½åšä¸æ“ä½œå¾—åˆ°101ï¼Œå†å‘å·¦ç§»ä¸€ä½å¾—åˆ°1010ï¼Œ(101&amp;amp;111)&amp;lt;&amp;lt;1ã€‚

ç¬¬ä¸‰æ­¥é‡å¤ä¸Šè¿°ä¸¤æ­¥ï¼Œ å„ä½ç›¸åŠ  010^1010=1000ï¼Œè¿›ä½å€¼ä¸º100=(010&amp;amp;1010)&amp;lt;&amp;lt;1ã€‚
     ç»§ç»­é‡å¤ä¸Šè¿°ä¸¤æ­¥ï¼š1000^100 = 1100ï¼Œè¿›ä½å€¼ä¸º0ï¼Œè·³å‡ºå¾ªç¯ï¼Œ1100ä¸ºæœ€ç»ˆç»“æœã€‚
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ä»£ç å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public int Add(int num1,int num2) {
    if ( 0 == num2 ) return num1;

    int sum = num1 ^ num2;
    int carry = (num1 &amp;amp; num2) &amp;lt;&amp;lt; 1;
    return Add(sum, carry);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å…ˆäº¦æˆ–ï¼Œå†å·¦ç§»ï¼Œé€’å½’ã€‚&lt;/p&gt;

&lt;h2 id=&quot;æ•°å€¼ç±»å‹çš„èŒƒå›´&quot;&gt;æ•°å€¼ç±»å‹çš„èŒƒå›´&lt;/h2&gt;

&lt;p&gt;å¯¹äºä¸€ä¸ªæœ‰ç¬¦å·çš„æ•°ï¼Œå®ƒçš„æ•°å€¼èŒƒå›´ä¸º-n~n-1ã€‚æ‰€ä»¥å¦‚æœä½ è¦çš„ç»“æœæ˜¯-nï¼Œè€Œä½ æƒ³å…ˆç®—å‡ºnï¼Œç„¶åå†0-nã€‚è¿™æ ·çš„næ˜¯è¶Šç•Œçš„ï¼Œå¿…é¡»ç”¨å¦å¤–ä¸€ç§èŒƒå›´æ›´å¤§çš„ç±»å‹è¡¨ç¤ºï¼Œç„¶åè¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢ï¼Œæˆ–è€…ä¸€å¼€å§‹å°±ç”¨è´Ÿæ•°æ¥æ±‚ã€‚&lt;/p&gt;

&lt;h2 id=&quot;é€’å½’&quot;&gt;é€’å½’&lt;/h2&gt;

&lt;p&gt;é€’å½’æ˜¯ä¸€ä¸ªå¾ˆç¥å¥‡çš„ä¸œè¥¿ï¼Œä»–å¯ä»¥æŠŠä¸€äº›æ–¹æ³•å¤„ç†åˆ†æ”¯ç”¨ä»–æœ¬èº«æ¥è¡¨ç¤ºï¼Œæ¯”å¦‚å½“æŸä¸ªæ¡ä»¶ä¸‹ï¼Œä¼šæœ‰ä¸‰æ¡åˆ†æ”¯ï¼Œè¿™ä¸‰æ¡åˆ†æ”¯éƒ½å¯ä»¥ç”¨é€’å½’çš„æ–¹æ³•æ¥è¡¨ç¤ºã€‚&lt;/p&gt;

&lt;p&gt;é¢˜ç›®å¦‚ä¸‹ï¼šè¯·å®ç°ä¸€ä¸ªå‡½æ•°ç”¨æ¥åŒ¹é…åŒ…æ‹¬â€™.â€™å’Œâ€™&lt;em&gt;â€˜çš„æ­£åˆ™è¡¨è¾¾å¼ã€‚æ¨¡å¼ä¸­çš„å­—ç¬¦â€™.â€™è¡¨ç¤ºä»»æ„ä¸€ä¸ªå­—ç¬¦ï¼Œè€Œâ€™&lt;/em&gt;â€˜è¡¨ç¤ºå®ƒå‰é¢çš„å­—ç¬¦å¯ä»¥å‡ºç°ä»»æ„æ¬¡ï¼ˆåŒ…å«0æ¬¡ï¼‰ã€‚ åœ¨æœ¬é¢˜ä¸­ï¼ŒåŒ¹é…æ˜¯æŒ‡å­—ç¬¦ä¸²çš„æ‰€æœ‰å­—ç¬¦åŒ¹é…æ•´ä¸ªæ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œå­—ç¬¦ä¸²â€aaaâ€ä¸æ¨¡å¼â€a.aâ€å’Œâ€ab&lt;em&gt;ac&lt;/em&gt;aâ€åŒ¹é…ï¼Œä½†æ˜¯ä¸â€aa.aâ€å’Œâ€ab*aâ€å‡ä¸åŒ¹é…&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;å½“æ¨¡å¼ä¸­çš„ç¬¬äºŒä¸ªå­—ç¬¦ä¸æ˜¯â€œ*â€æ—¶ï¼š&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1ã€å¦‚æœå­—ç¬¦ä¸²ç¬¬ä¸€ä¸ªå­—ç¬¦å’Œæ¨¡å¼ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ç›¸åŒ¹é…ï¼Œé‚£ä¹ˆå­—ç¬¦ä¸²å’Œæ¨¡å¼éƒ½åç§»ä¸€ä¸ªå­—ç¬¦ï¼Œç„¶ååŒ¹é…å‰©ä½™çš„ã€‚&lt;/p&gt;

&lt;p&gt;2ã€å¦‚æœÂ å­—ç¬¦ä¸²ç¬¬ä¸€ä¸ªå­—ç¬¦å’Œæ¨¡å¼ä¸­çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ç›¸ä¸åŒ¹é…ï¼Œç›´æ¥è¿”å›falseã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;è€Œå½“æ¨¡å¼ä¸­çš„ç¬¬äºŒä¸ªå­—ç¬¦æ˜¯â€œ*â€æ—¶ï¼š&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å¦‚æœå­—ç¬¦ä¸²ç¬¬ä¸€ä¸ªå­—ç¬¦è·Ÿæ¨¡å¼ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸åŒ¹é…ï¼Œåˆ™æ¨¡å¼åç§»2ä¸ªå­—ç¬¦ï¼Œç»§ç»­åŒ¹é…ã€‚å¦‚æœå­—ç¬¦ä¸²ç¬¬ä¸€ä¸ªå­—ç¬¦è·Ÿæ¨¡å¼ç¬¬ä¸€ä¸ªå­—ç¬¦åŒ¹é…ï¼Œå¯ä»¥æœ‰3ç§åŒ¹é…æ–¹å¼ï¼š&lt;/p&gt;

&lt;p&gt;1ã€æ¨¡å¼åç§»2å­—ç¬¦ï¼Œç›¸å½“äºx*è¢«å¿½ç•¥ï¼›&lt;/p&gt;

&lt;p&gt;2ã€å­—ç¬¦ä¸²åç§»1å­—ç¬¦ï¼Œæ¨¡å¼åç§»2å­—ç¬¦ï¼›&lt;/p&gt;

&lt;p&gt;3ã€å­—ç¬¦ä¸²åç§»1å­—ç¬¦ï¼Œæ¨¡å¼ä¸å˜ï¼Œå³ç»§ç»­åŒ¹é…å­—ç¬¦ä¸‹ä¸€ä½ï¼Œå› ä¸º*å¯ä»¥åŒ¹é…å¤šä½ï¼›&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼šJavaé‡Œï¼Œè¦æ—¶åˆ»æ£€éªŒæ•°ç»„æ˜¯å¦è¶Šç•Œã€‚&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public boolean match(char[] str, char[] pattern) {
    if (str == null || pattern == null) {
        return false;
    }
    int strIndex = 0;
    int patternIndex = 0;
    return matchCore(str, strIndex, pattern, patternIndex);
}

public boolean matchCore(char[] str, int strIndex, char[] pattern, int patternIndex) {
    //æœ‰æ•ˆæ€§æ£€éªŒï¼šstråˆ°å°¾ï¼Œpatternåˆ°å°¾ï¼ŒåŒ¹é…æˆåŠŸ
    if (strIndex == str.length &amp;amp;&amp;amp; patternIndex == pattern.length) {
        return true;
    }
    //patternå…ˆåˆ°å°¾ï¼ŒåŒ¹é…å¤±è´¥
    if (strIndex != str.length &amp;amp;&amp;amp; patternIndex == pattern.length) {
        return false;
    }
    //æ¨¡å¼ç¬¬2ä¸ªæ˜¯*ï¼Œä¸”å­—ç¬¦ä¸²ç¬¬1ä¸ªè·Ÿæ¨¡å¼ç¬¬1ä¸ªåŒ¹é…,åˆ†3ç§åŒ¹é…æ¨¡å¼ï¼›å¦‚ä¸åŒ¹é…ï¼Œæ¨¡å¼åç§»2ä½
    if (patternIndex + 1 &amp;lt; pattern.length &amp;amp;&amp;amp; pattern[patternIndex + 1] == '*') {
        if ((strIndex != str.length &amp;amp;&amp;amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;amp;&amp;amp; strIndex != str.length)) {
            return matchCore(str, strIndex, pattern, patternIndex + 2)//æ¨¡å¼åç§»2ï¼Œè§†ä¸ºx*åŒ¹é…0ä¸ªå­—ç¬¦
                    || matchCore(str, strIndex + 1, pattern, patternIndex + 2)//è§†ä¸ºæ¨¡å¼åŒ¹é…1ä¸ªå­—ç¬¦
                    || matchCore(str, strIndex + 1, pattern, patternIndex);//*åŒ¹é…1ä¸ªï¼Œå†åŒ¹é…strä¸­çš„ä¸‹ä¸€ä¸ª
        } else {
            return matchCore(str, strIndex, pattern, patternIndex + 2);
        }
    }
    //æ¨¡å¼ç¬¬2ä¸ªä¸æ˜¯*ï¼Œä¸”å­—ç¬¦ä¸²ç¬¬1ä¸ªè·Ÿæ¨¡å¼ç¬¬1ä¸ªåŒ¹é…ï¼Œåˆ™éƒ½åç§»1ä½ï¼Œå¦åˆ™ç›´æ¥è¿”å›false
    if ((strIndex != str.length &amp;amp;&amp;amp; pattern[patternIndex] == str[strIndex]) || (pattern[patternIndex] == '.' &amp;amp;&amp;amp; strIndex != str.length)) {
        return matchCore(str, strIndex + 1, pattern, patternIndex + 1);
    }
    return false;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œå°±æ˜¯ç”¨é€’å½’å®ç°é‚£å‡ ä¸ªåˆ†æ”¯ã€‚&lt;/p&gt;

&lt;h2 id=&quot;å‡å°‘æ²¡ç”¨åˆ†æ”¯å¯¹æ— ç”¨ä»£ç è¿›è¡Œåˆå¹¶&quot;&gt;å‡å°‘æ²¡ç”¨åˆ†æ”¯ï¼Œå¯¹æ— ç”¨ä»£ç è¿›è¡Œåˆå¹¶&lt;/h2&gt;

&lt;p&gt;ä»¥å‰å†™é‚£ç§ä¸Šä¸‹å·¦å³å¯»è·¯çš„é¢˜ç›®ï¼Œæ€»æ˜¯å¾ˆå‚»çš„ï¼Œåˆ¤æ–­å¾€ä¸Šèµ°æ˜¯ä¸æ˜¯å¯èƒ½çš„ï¼Œå¾€ä¸‹èµ°æ˜¯ä¸æ˜¯å¯èƒ½çš„ï¼Œç„¶åå·¦å³æ˜¯ä¸æ˜¯å¯èƒ½çš„ï¼Œè¿™æ ·ä¼šé€ æˆå¤§é‡é‡å¤ä»£ç ï¼Œè€Œä¸”ä»£ç å¾€å¾€å®¹æ˜“å‡ºé”™ã€‚å…¶å®è¿™äº›éƒ½å¯ä»¥äº¤ç”±ç¨‹åºå¼€å¤´ç›´æ¥å»åˆ¤æ–­çš„ã€‚&lt;/p&gt;

&lt;p&gt;è¯·è®¾è®¡ä¸€ä¸ªå‡½æ•°ï¼Œç”¨æ¥åˆ¤æ–­åœ¨ä¸€ä¸ªçŸ©é˜µä¸­æ˜¯å¦å­˜åœ¨ä¸€æ¡åŒ…å«æŸå­—ç¬¦ä¸²æ‰€æœ‰å­—ç¬¦çš„è·¯å¾„ã€‚è·¯å¾„å¯ä»¥ä»çŸ©é˜µä¸­çš„ä»»æ„ä¸€ä¸ªæ ¼å­å¼€å§‹ï¼Œæ¯ä¸€æ­¥å¯ä»¥åœ¨çŸ©é˜µä¸­å‘å·¦ï¼Œå‘å³ï¼Œå‘ä¸Šï¼Œå‘ä¸‹ç§»åŠ¨ä¸€ä¸ªæ ¼å­ã€‚å¦‚æœä¸€æ¡è·¯å¾„ç»è¿‡äº†çŸ©é˜µä¸­çš„æŸä¸€ä¸ªæ ¼å­ï¼Œåˆ™è¯¥è·¯å¾„ä¸èƒ½å†è¿›å…¥è¯¥æ ¼å­ã€‚ ä¾‹å¦‚ a b c e s f c s a d e e çŸ©é˜µä¸­åŒ…å«ä¸€æ¡å­—ç¬¦ä¸²â€bcccedâ€çš„è·¯å¾„ï¼Œä½†æ˜¯çŸ©é˜µä¸­ä¸åŒ…å«â€abcbâ€è·¯å¾„ï¼Œå› ä¸ºå­—ç¬¦ä¸²çš„ç¬¬ä¸€ä¸ªå­—ç¬¦bå æ®äº†çŸ©é˜µä¸­çš„ç¬¬ä¸€è¡Œç¬¬äºŒä¸ªæ ¼å­ä¹‹åï¼Œè·¯å¾„ä¸èƒ½å†æ¬¡è¿›å…¥è¯¥æ ¼å­ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public class hasPath {
    public boolean hasPath(char[] matrix, int rows, int cols, char[] str)
    {
        if (str.length==0)
            return true;
        if (matrix.length==0||rows==0||cols==0||matrix.length!=rows*cols)
            return false;
        int[]mark=new int[rows*cols];
       for (int i=0;i&amp;lt;rows;i++){
           for (int j=0;j&amp;lt;cols;j++){
               if (helper(matrix,i,j,rows,cols,mark,str,0)==true)
                   return true;
           }
       }
        return false;
    }
    public boolean helper(char []matrix,int row,int col,int rows,int cols,int[]mark,char[]str,int k){

        int site=row*cols+col;
        if (row&amp;lt;0||row&amp;gt;=rows||col&amp;lt;0||col&amp;gt;=cols||mark[site]!=0||matrix[site]!=str[k])
            return false;
        if (str.length-1==k)
            return true;

        mark[site]=1;
        if (helper(matrix,row-1,col,rows,cols,mark,str,k+1)||
                helper(matrix,row+1,col,rows,cols,mark,str,k+1)||
                helper(matrix,row,col-1,rows,cols,mark,str,k+1)||
                helper(matrix,row,col+1,rows,cols,mark,str,k+1)
                )
            return true;
        mark[site]=0;
        return false;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™æ ·å°±æ¯”è¾ƒç®€æ´ã€‚æƒ³æƒ³è‡ªå·±ä»¥å‰å¥½å‚»ğŸ˜¯ã€‚&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/code/2017/01/02/%E4%BB%A3%E7%A0%81%E5%B0%8F%E6%8A%80%E5%B7%A7</link>
                <guid>http://bbwff.github.io/code/2017/01/02/ä»£ç å°æŠ€å·§</guid>
                <pubDate>2017-01-02T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Sparkæºç åˆ†æshuffleå®ç°</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shuffle&quot; id=&quot;markdown-toc-shuffle&quot;&gt;Shuffle&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bypassmergesortshufflewriter&quot; id=&quot;markdown-toc-bypassmergesortshufflewriter&quot;&gt;BypassMergeSortShuffleWriter&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sortshufflewriter&quot; id=&quot;markdown-toc-sortshufflewriter&quot;&gt;SortShuffleWriter&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#unsafeshufflewriter&quot; id=&quot;markdown-toc-unsafeshufflewriter&quot;&gt;unsafeShuffleWriter&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#blockstoreshufflereader&quot; id=&quot;markdown-toc-blockstoreshufflereader&quot;&gt;BlockStoreShuffleReader&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#æ€»ç»“&quot; id=&quot;markdown-toc-æ€»ç»“&quot;&gt;æ€»ç»“&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;spark shufféƒ¨åˆ†æ˜¯sparkæºç çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œshuffleå‘ç”Ÿåœ¨stageçš„äº¤ç•Œå¤„ï¼Œå¯¹äºsparkçš„æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œæºç æ›´æ–°åï¼Œsparkçš„shuffleæœºåˆ¶ä¹Ÿä¸ä¸€æ ·ï¼Œæœ¬æ–‡åˆ†æspark2.0çš„shuffleå®ç°ã€‚&lt;/p&gt;

&lt;p&gt;æœ¬æ–‡åŸºäºspark2.0ã€‚&lt;/p&gt;

&lt;h2 id=&quot;shuffle&quot;&gt;Shuffle&lt;/h2&gt;

&lt;p&gt;shuffleæ˜¯Mapreduceæ¡†æ¶ä¸­ä¸€ä¸ªç‰¹å®šçš„phaseï¼Œä»‹äºMapå’ŒReduceä¹‹é—´ã€‚shuffleçš„è‹±æ–‡æ„æ€æ˜¯æ··æ´—ï¼ŒåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œshuffle write å’Œshuffle readã€‚è¿™é‡Œæœ‰ä¸€ç¯‡æ–‡ç« :&lt;a href=&quot;http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/&quot;&gt;è¯¦ç»†æ¢ç©¶Sparkçš„shuffleå®ç°&lt;/a&gt;ï¼Œè¿™ç¯‡æ–‡ç« å†™äº2014å¹´ï¼Œè®²çš„æ˜¯æ—©æœŸç‰ˆæœ¬çš„shuffleå®ç°ã€‚éšç€æºç çš„æ›´æ–°ï¼Œshuffleæœºåˆ¶ä¹Ÿåšå‡ºäº†ç›¸åº”çš„ä¼˜åŒ–ï¼Œä¸‹é¢åˆ†æspark-2.0çš„shuffleæœºåˆ¶ã€‚&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;shuffleWriter&lt;/code&gt;æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå…·ä½“å®ç°æœ‰ä¸‰ç§ï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;BypassMergeSortShuffleWriter&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;sortShuffleWriter&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;UnsafeShuffleWriter&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;bypassmergesortshufflewriter&quot;&gt;BypassMergeSortShuffleWriter&lt;/h3&gt;

&lt;p&gt;-_-,æˆ‘å…ˆç¿»è¯‘ä¸‹è¿™ä¸ªç±»å¼€å¤´ç»™çš„æ³¨é‡Šï¼Œæ³¨é‡Šæ˜¯å¾ˆå¥½çš„å…¨å±€ç†è§£ä»£ç çš„å·¥å…·ï¼Œè¦å¥½å¥½ç†è§£ã€‚å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;p&gt;è¿™ä¸ªç±»å®ç°äº†åŸºäºsort-shuffleçš„hashé£æ ¼çš„shuffle fallback pathï¼ˆå›é€€è·¯å¾„ï¼Ÿæ€ä¹ˆç¿»ï¼‰ã€‚è¿™ä¸ªwriteè·¯å¾„æŠŠæ•°æ®å†™åˆ°ä¸åŒçš„æ–‡ä»¶é‡Œï¼Œæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªreduceåˆ†åŒºï¼Œç„¶åæŠŠè¿™äº›æ–‡ä»¶æ•´åˆåˆ°ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶çš„ä¸åŒåŒºåŸŸæœåŠ¡ä¸åŒçš„reducerã€‚æ•°æ®ä¸æ˜¯ç¼“å­˜åœ¨å†…å­˜ä¸­ã€‚è¿™ä¸ªç±»æœ¬è´¨ä¸Šå’Œä¹‹å‰çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;HashShuffleReader&lt;/code&gt;ï¼Œé™¤äº†è¿™ä¸ªç±»çš„è¾“å‡ºæ ¼å¼å¯ä»¥é€šè¿‡&lt;code class=&quot;highlighter-rouge&quot;&gt;org.apache.spark.shuffle.IndexShuffleBlockResolver&lt;/code&gt;æ¥è°ƒç”¨ã€‚è¿™ä¸ªå†™è·¯å¾„å¯¹äºæœ‰è®¸å¤šreduceåˆ†åŒºçš„shuffleæ¥è¯´æ˜¯ä¸é«˜æ•ˆçš„ï¼Œå› ä¸ºä»–åŒæ—¶æ‰“å¼€å¾ˆå¤šserializerså’Œæ–‡ä»¶æµã€‚å› æ­¤åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰ä¼šé€‰æ‹©è¿™ä¸ªè·¯å¾„ï¼š&lt;/p&gt;

&lt;p&gt;1ã€æ²¡æœ‰æ’åº  2ã€æ²¡æœ‰èšåˆæ“ä½œ  3ã€partitionçš„æ•°é‡å°äºbypassMergeThreshold&lt;/p&gt;

&lt;p&gt;è¿™ä¸ªä»£ç æ›¾ç»æ˜¯ExternalSorterçš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯ä¸ºäº†å‡å°‘ä»£ç å¤æ‚åº¦å°±ç‹¬ç«‹äº†å‡ºæ¥ã€‚å¥½ï¼Œç¿»è¯‘ç»“æŸã€‚-_-&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Override
public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {
  assert (partitionWriters == null);
  if (!records.hasNext()) {
    partitionLengths = new long[numPartitions];
    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, null);
    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
    return;
  }
  final SerializerInstance serInstance = serializer.newInstance();
  final long openStartTime = System.nanoTime();
  partitionWriters = new DiskBlockObjectWriter[numPartitions];
  for (int i = 0; i &amp;lt; numPartitions; i++) {
    final Tuple2&amp;lt;TempShuffleBlockId, File&amp;gt; tempShuffleBlockIdPlusFile =
      blockManager.diskBlockManager().createTempShuffleBlock();
    final File file = tempShuffleBlockIdPlusFile._2();
    final BlockId blockId = tempShuffleBlockIdPlusFile._1();
    partitionWriters[i] =
      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);
  }
  // Creating the file to write to and creating a disk writer both involve interacting with
  // the disk, and can take a long time in aggregate when we open many files, so should be
  // included in the shuffle write time.
  writeMetrics.incWriteTime(System.nanoTime() - openStartTime);

  while (records.hasNext()) {
    final Product2&amp;lt;K, V&amp;gt; record = records.next();
    final K key = record._1();
    partitionWriters[partitioner.getPartition(key)].write(key, record._2());
  }

  for (DiskBlockObjectWriter writer : partitionWriters) {
    writer.commitAndClose();
  }

  File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);
  File tmp = Utils.tempFileWith(output);
  try {
    partitionLengths = writePartitionedFile(tmp);
    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);
  } finally {
    if (tmp.exists() &amp;amp;&amp;amp; !tmp.delete()) {
      logger.error(&quot;Error while deleting temp file {}&quot;, tmp.getAbsolutePath());
    }
  }
  mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å‰é¢éƒ½å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æ ¹æ®keyçš„å“ˆå¸Œå€¼å†™åˆ°ä¸åŒçš„æ–‡ä»¶é‡Œé¢ï¼Œç„¶åå°±æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;writePartitionedFile&lt;/code&gt;å’Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;writeIndexFileAndCommit&lt;/code&gt;ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Concatenate all of the per-partition files into a single combined file.
 *
 * @return array of lengths, in bytes, of each partition of the file (used by map output tracker).
 */
private long[] writePartitionedFile(File outputFile) throws IOException {
  // Track location of the partition starts in the output file
  final long[] lengths = new long[numPartitions];
  if (partitionWriters == null) {
    // We were passed an empty iterator
    return lengths;
  }

  final FileOutputStream out = new FileOutputStream(outputFile, true);
  final long writeStartTime = System.nanoTime();
  boolean threwException = true;
  try {
    for (int i = 0; i &amp;lt; numPartitions; i++) {
      final File file = partitionWriters[i].fileSegment().file();
      if (file.exists()) {
        final FileInputStream in = new FileInputStream(file);
        boolean copyThrewException = true;
        try {
          lengths[i] = Utils.copyStream(in, out, false, transferToEnabled);
          copyThrewException = false;
        } finally {
          Closeables.close(in, copyThrewException);
        }
        if (!file.delete()) {
          logger.error(&quot;Unable to delete file for partition {}&quot;, i);
        }
      }
    }
    threwException = false;
  } finally {
    Closeables.close(out, threwException);
    writeMetrics.incWriteTime(System.nanoTime() - writeStartTime);
  }
  partitionWriters = null;
  return lengths;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™ä¸ªå°±æ˜¯æŒ‰é¡ºåºæŠŠä¹‹å‰å†™çš„åˆ†åŒºæ–‡ä»¶é‡Œçš„æ•°æ®åˆå¹¶åˆ°ä¸€ä¸ªå¤§æ–‡ä»¶é‡Œé¢ï¼Œç„¶åè¿”å›æ¯ä¸ªåˆ†åŒºæ–‡ä»¶çš„é•¿åº¦ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Write an index file with the offsets of each block, plus a final offset at the end for the
 * end of the output file. This will be used by getBlockData to figure out where each block
 * begins and ends.
 *
 * It will commit the data and index file as an atomic operation, use the existing ones, or
 * replace them with new ones.
 *
 * Note: the `lengths` will be updated to match the existing index file if use the existing ones.
 * */
def writeIndexFileAndCommit(
    shuffleId: Int,
    mapId: Int,
    lengths: Array[Long],
    dataTmp: File): Unit = {
  val indexFile = getIndexFile(shuffleId, mapId)
  val indexTmp = Utils.tempFileWith(indexFile)
  try {
    val out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexTmp)))
    Utils.tryWithSafeFinally {
      // We take in lengths of each block, need to convert it to offsets.
      var offset = 0L
      out.writeLong(offset)
      for (length &amp;lt;- lengths) {
        offset += length
        out.writeLong(offset)
      }
    } {
      out.close()
    }

    val dataFile = getDataFile(shuffleId, mapId)
    // There is only one IndexShuffleBlockResolver per executor, this synchronization make sure
    // the following check and rename are atomic.
    synchronized {
      val existingLengths = checkIndexAndDataFile(indexFile, dataFile, lengths.length)
      if (existingLengths != null) {
        // Another attempt for the same task has already written our map outputs successfully,
        // so just use the existing partition lengths and delete our temporary map outputs.
        System.arraycopy(existingLengths, 0, lengths, 0, lengths.length)
        if (dataTmp != null &amp;amp;&amp;amp; dataTmp.exists()) {
          dataTmp.delete()
        }
        indexTmp.delete()
      } else {
        // This is the first successful attempt in writing the map outputs for this task,
        // so override any existing index and data files with the ones we wrote.
        if (indexFile.exists()) {
          indexFile.delete()
        }
        if (dataFile.exists()) {
          dataFile.delete()
        }
        if (!indexTmp.renameTo(indexFile)) {
          throw new IOException(&quot;fail to rename file &quot; + indexTmp + &quot; to &quot; + indexFile)
        }
        if (dataTmp != null &amp;amp;&amp;amp; dataTmp.exists() &amp;amp;&amp;amp; !dataTmp.renameTo(dataFile)) {
          throw new IOException(&quot;fail to rename file &quot; + dataTmp + &quot; to &quot; + dataFile)
        }
      }
    }
  } finally {
    if (indexTmp.exists() &amp;amp;&amp;amp; !indexTmp.delete()) {
      logError(s&quot;Failed to delete temporary index file at ${indexTmp.getAbsolutePath}&quot;)
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œä¸Šæ¥å…ˆå†™indexTmpï¼Œæ˜¯æŠŠåˆ†åŒºæ–‡ä»¶é•¿åº¦å†™è¿›å»ï¼Œä¾¿äºç´¢å¼•éœ€è¦çš„é‚£éƒ¨åˆ†æ•°æ®ã€‚ç„¶åå°±åˆ¤æ–­è¿™ä¸ªä»»åŠ¡æ˜¯ä¸æ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œåˆ°è¿™é‡Œï¼Œå¦‚æœä¹‹å‰æ‰§è¡ŒæˆåŠŸè¿‡ï¼Œé‚£å°±ä¸ç”¨å†™äº†ï¼Œç›´æ¥ç”¨ä»¥å‰çš„ç»“æœå°±è¡Œã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œåˆ°è¿™é‡Œï¼Œé‚£ä¹ˆå°±æŠŠä¹‹å‰çš„indexTmpé‡å‘½åä¸ºindexFileï¼ŒdataTmpé‡å‘½åä¸ºdataFileç„¶åè¿”å›ã€‚&lt;/p&gt;

&lt;p&gt;è¿™é‡Œè¦æ³¨æ„ä¸‹ï¼Œæ¯ä¸ªexecutorä¸Šé¢åªæœ‰ä¸€ä¸ª&lt;code class=&quot;highlighter-rouge&quot;&gt;IndexShuffleBlockResolver&lt;/code&gt;ï¼Œè¿™ä¸ªç®¡ç†è¿™ä¸ªexecutorä¸Šæ‰€æœ‰çš„indexFile.&lt;/p&gt;

&lt;p&gt;ç­‰è¿™ä¸ªindexFileä¹Ÿå†™å¥½ä¹‹åï¼Œå°±è¿”å›&lt;code class=&quot;highlighter-rouge&quot;&gt;mapStatus&lt;/code&gt;ã€‚shuffleWriteå°±ç»“æŸäº†ã€‚&lt;/p&gt;

&lt;h3 id=&quot;sortshufflewriter&quot;&gt;SortShuffleWriter&lt;/h3&gt;

&lt;p&gt;é¦–å…ˆæè¿°ä¸‹å¤§æ¦‚ã€‚å› ä¸ºæ˜¯sortï¼Œæ‰€ä»¥è¦æ’åºï¼Œè¿™é‡Œå°±ç”¨åˆ°äº†ExternalSoterè¿™ä¸ªæ•°æ®ç»“æ„ã€‚ç„¶åæŠŠè¦å¤„ç†çš„æ•°æ®å…¨éƒ¨æ’å…¥åˆ°ExternalSorteré‡Œé¢ï¼Œåœ¨æ’å…¥çš„è¿‡ç¨‹ä¸­æ˜¯ä¸æ’åºçš„ï¼Œå°±æ˜¯æ’å…¥ï¼Œæ’å…¥æ•°æ®æ˜¯(partitionId,key,value)ã€‚ç„¶åæ˜¯è°ƒç”¨&lt;code class=&quot;highlighter-rouge&quot;&gt; sorter.writePartitionedFile&lt;/code&gt;,åœ¨è¿™é‡Œä¼šæ’åºï¼Œä¼šæŒ‰ç…§partitionIdå’Œkeyï¼ˆæˆ–è€…keyçš„hashcodeï¼‰è¿›è¡Œæ’åºï¼Œå…¶ä»–çš„å°±å’Œä¸Šé¢bypassShuffleWriterçš„å·®ä¸å¤šäº†ï¼Œæœ€åä¹Ÿæ˜¯å†™åˆ°ä¸€ä¸ªindexFileé‡Œé¢ã€‚è¿”å›mapStatusã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/** Write a bunch of records to this task's output */
override def write(records: Iterator[Product2[K, V]]): Unit = {
  sorter = if (dep.mapSideCombine) {
    require(dep.aggregator.isDefined, &quot;Map-side combine without Aggregator specified!&quot;)
    new ExternalSorter[K, V, C](
      context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
  } else {
    // In this case we pass neither an aggregator nor an ordering to the sorter, because we don't
    // care whether the keys get sorted in each partition; that will be done on the reduce side
    // if the operation being run is sortByKey.
    new ExternalSorter[K, V, V](
      context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)
  }
  sorter.insertAll(records)

  // Don't bother including the time to open the merged output file in the shuffle write time,
  // because it just opens a single file, so is typically too fast to measure accurately
  // (see SPARK-3570).
  val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
  val tmp = Utils.tempFileWith(output)
  try {
    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
    val partitionLengths = sorter.writePartitionedFile(blockId, tmp)
    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)
    mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)
  } finally {
    if (tmp.exists() &amp;amp;&amp;amp; !tmp.delete()) {
      logError(s&quot;Error while deleting temp file ${tmp.getAbsolutePath}&quot;)
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œé¢ExternalSorteræ˜¯æ ¸å¿ƒã€‚çœ‹å®ƒçš„æºç ï¼Œå®ƒå­˜æ•°æ®æ˜¯ä½¿ç”¨çš„ä¸¤ç§æ•°æ®ç»“æ„ã€‚&lt;code class=&quot;highlighter-rouge&quot;&gt;PartitionedAppendOnlyMap&lt;/code&gt;ã€&lt;code class=&quot;highlighter-rouge&quot;&gt;PartitionedPairBuffer&lt;/code&gt;ï¼Œå…¶ä¸­æœ‰èšåˆæ“ä½œä½¿ç”¨mapï¼Œæ²¡æœ‰èšåˆæ“ä½œä½¿ç”¨bufferã€‚PartitionedAppendOnlyMap ç»§æ‰¿äº†SizeTrackingAppendOnlyMap å’ŒWritablePartitionedPairCollection ã€‚ å…¶ä¸­SizeTrackingAppendOnlyMapæ˜¯ç”¨äºé¢„æµ‹ç©ºé—´ï¼ˆSizeTrackerï¼‰ï¼Œç„¶ååŠ å­˜å‚¨æ•°æ®ï¼ˆAppendOnlyMapï¼‰,ç„¶åWritablePartitionedPairCollectionæ˜¯ç”¨äºæ’å…¥æ•°æ®æ—¶å€™æ’å…¥partitionIdï¼ˆinsert(partition: Int, key: K, value: V)ï¼‰åŠ ä¸Šé‡Œé¢å®ç°äº†å¯¹æ•°æ®æŒ‰ç…§partitionIdå’ŒKeyæ’åºçš„æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;æˆ‘ä¸»è¦æ˜¯å¯¹AppendOnlyMapæ€ä¹ˆå­˜å‚¨æ•°æ®æ¯”è¾ƒæ„Ÿå…´è¶£ã€‚çœ‹ä¸‹AppendOnlyMapã€‚&lt;/p&gt;

&lt;p&gt;çœ‹æºç ï¼Œå®ƒå­˜å‚¨æ•°æ®æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;private var data = new Array[AnyRef](2 * capacity)&lt;/code&gt;,æ˜¯ä½¿ç”¨æ•°ç»„å­˜å‚¨çš„ï¼Œkeyå’ŒvalueæŒ¨ç€ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†èŠ‚çœç©ºé—´ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åmapçš„Updateå’ŒchangeValueå‡½æ•°æ˜¯å·®ä¸å¤šçš„ï¼Œåªä¸è¿‡åè€…çš„changeValueæ˜¯ç”±è®¡ç®—å‡½æ•°è®¡ç®—çš„valueï¼Œæ‰€ä»¥æˆ‘ä»¬å°±çœ‹updateæ–¹æ³•ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/** Set the value for a key */
def update(key: K, value: V): Unit = {
  assert(!destroyed, destructionMessage)
  val k = key.asInstanceOf[AnyRef]
  if (k.eq(null)) {
    if (!haveNullValue) {
      incrementSize()
    }
    nullValue = value
    haveNullValue = true
    return
  }
  var pos = rehash(key.hashCode) &amp;amp; mask
  var i = 1
  while (true) {
    val curKey = data(2 * pos)
    if (curKey.eq(null)) {
      data(2 * pos) = k
      data(2 * pos + 1) = value.asInstanceOf[AnyRef]
      incrementSize()  // Since we added a new key
      return
    } else if (k.eq(curKey) || k.equals(curKey)) {
      data(2 * pos + 1) = value.asInstanceOf[AnyRef]
      return
    } else {
      val delta = i
      pos = (pos + delta) &amp;amp; mask
      i += 1
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;çœ‹æºç å¯ä»¥çœ‹å‡ºï¼Œè¿™é‡Œæ’å…¥æ•°æ®ï¼Œé‡‡ç”¨çš„äºŒæ¬¡æ¢æµ‹æ³•ã€‚java.util.collectionçš„HashMapåœ¨hashå†²çªæ—¶å€™é‡‡ç”¨çš„æ˜¯é“¾æ¥æ³•ï¼Œè€Œè¿™é‡Œçš„äºŒæ¬¡æ¢æµ‹æ³•ç¼ºç‚¹å°±æ˜¯åˆ é™¤å…ƒç´ æ—¶å€™æ¯”è¾ƒå¤æ‚ï¼Œä¸èƒ½ç®€å•çš„æŠŠæ•°ç»„ä¸­çš„ç›¸åº”ä½ç½®è®¾ä¸ºnullï¼Œè¿™æ ·å°±æ²¡åŠæ³•æŸ¥æ‰¾å…ƒç´ ï¼Œé€šå¸¸æ˜¯æŠŠè¢«åˆ é™¤çš„å…ƒç´ æ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä½†æ˜¯åˆéœ€è¦å æ®é¢å¤–çš„ç©ºé—´ã€‚ä½†æ˜¯æ­¤å¤„æ˜¯appendOnlyMapï¼Œä¹Ÿå°±æ˜¯åªä¼šè¿½åŠ ï¼ˆæ’å…¥æˆ–è€…æ›´æ–°ï¼‰ï¼Œä¸ä¼šåˆ é™¤ï¼Œæ‰€ä»¥è¿™ä¸ªè‡ªå®šä¹‰çš„mapæ›´çœå†…å­˜ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åè¿™ä¸ªAppendOnlyMapä¼šåœ¨growMapçš„æ—¶å€™é‡æ–°hashã€‚åœ¨sorter.insertallæ—¶å€™æ˜¯ä¸æ’åºçš„ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åwritePartitionedFile é‡Œé¢è°ƒç”¨&lt;code class=&quot;highlighter-rouge&quot;&gt;collection.destructiveSortedWritablePartitionedIterator(comparator)	&lt;/code&gt;ä¼šå¯¹æ•°æ®æ’åºï¼Œä¹‹åå°±è·Ÿä¸Šä¸€å°èŠ‚é‡Œé¢çš„writePartitionedFileå·®ä¸å¤šäº†ï¼Œæ— éå°±æ˜¯æŠŠå†…å­˜é‡Œé¢çš„æ•°æ®å’Œspillçš„æ•°æ®åˆå¹¶ä¹‹åå†™å…¥å¤§æ–‡ä»¶é‡Œé¢ï¼Œä¹‹åçš„writeIndexFileæ˜¯ä¸€æ ·çš„ï¼Œå°±ä¸ç»†è¯´ã€‚&lt;/p&gt;

&lt;h3 id=&quot;unsafeshufflewriter&quot;&gt;unsafeShuffleWriter&lt;/h3&gt;

&lt;p&gt;è¿™é‡Œä¹‹æ‰€ä»¥å«ä½œunsafeï¼Œæ˜¯å› ä¸ºè¦æ“çºµå †å¤–å†…å­˜ï¼ŒæŠŠæ•°æ®å†™åˆ°å †å¤–ï¼Œå †å¤–å†…å­˜æ˜¯ä¸å—jvmæ§åˆ¶çš„ï¼Œéœ€è¦æ‰‹åŠ¨è¿›è¡Œç”³è¯·å†…å­˜ä¸é‡Šæ”¾å†…å­˜ç©ºé—´ï¼Œæ‰€ä»¥æ˜¯unsafeçš„ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Override
public void write(scala.collection.Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {
  // Keep track of success so we know if we encountered an exception
  // We do this rather than a standard try/catch/re-throw to handle
  // generic throwables.
  boolean success = false;
  try {
    while (records.hasNext()) {
      insertRecordIntoSorter(records.next());
    }
    closeAndWriteOutput();
    success = true;
  } finally {
    if (sorter != null) {
      try {
        sorter.cleanupResources();
      } catch (Exception e) {
        // Only throw this error if we won't be masking another
        // error.
        if (success) {
          throw e;
        } else {
          logger.error(&quot;In addition to a failure during writing, we failed during &quot; +
                       &quot;cleanup.&quot;, e);
        }
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;é™¤äº†æ˜¯å†™åˆ°å †å¤–ï¼Œå…¶ä»–åº”è¯¥è·ŸsortShuffleWriter å·®ä¸å¤šå§ï¼Œæ‡’å¾—å†™äº†ï¼Œä»¥åå‘ç°æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„å†è¡¥å……ã€‚&lt;/p&gt;

&lt;h3 id=&quot;blockstoreshufflereader&quot;&gt;BlockStoreShuffleReader&lt;/h3&gt;

&lt;p&gt;å‰é¢ä¸‰ä¸ªshuffleWriterï¼Œshuffleåˆ†ä¸ºshuffleWriterå’ŒshuffleReaderã€‚shuffleReadråªæœ‰ä¸€ä¸ªå…·ä½“å®ç°ç±»å°±æ˜¯BlockStoreShuffleReaderã€‚çœ‹å¼€å¤´æ³¨é‡Šä¸ºï¼šè¯»å–ï¼ˆstartPartitionå’ŒendPartitionï¼‰ä¹‹é—´çš„partitionçš„æ•°æ®ï¼Œä»å…¶ä»–èŠ‚ç‚¹ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/** Read the combined key-values for this reduce task */
override def read(): Iterator[Product2[K, C]] = {
  val blockFetcherItr = new ShuffleBlockFetcherIterator(
    context,
    blockManager.shuffleClient,
    blockManager,
    mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),
    // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility
    SparkEnv.get.conf.getSizeAsMb(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;48m&quot;) * 1024 * 1024,
    SparkEnv.get.conf.getInt(&quot;spark.reducer.maxReqsInFlight&quot;, Int.MaxValue))

  // Wrap the streams for compression based on configuration
  val wrappedStreams = blockFetcherItr.map { case (blockId, inputStream) =&amp;gt;
    serializerManager.wrapForCompression(blockId, inputStream)
  }

  val serializerInstance = dep.serializer.newInstance()

  // Create a key/value iterator for each stream
  val recordIter = wrappedStreams.flatMap { wrappedStream =&amp;gt;
    // Note: the asKeyValueIterator below wraps a key/value iterator inside of a
    // NextIterator. The NextIterator makes sure that close() is called on the
    // underlying InputStream when all records have been read.
    serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator
  }

  // Update the context task metrics for each record read.
  val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()
  val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](
    recordIter.map { record =&amp;gt;
      readMetrics.incRecordsRead(1)
      record
    },
    context.taskMetrics().mergeShuffleReadMetrics())

  // An interruptible iterator must be used here in order to support task cancellation
  val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)

  val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
    if (dep.mapSideCombine) {
      // We are reading values that are already combined
      val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
      dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
    } else {
      // We don't know the value type, but also don't care -- the dependency *should*
      // have made sure its compatible w/ this aggregator, which will convert the value
      // type to the combined type C
      val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]
      dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
    }
  } else {
    require(!dep.mapSideCombine, &quot;Map-side combine without Aggregator specified!&quot;)
    interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
  }

  // Sort the output if there is a sort ordering defined.
  dep.keyOrdering match {
    case Some(keyOrd: Ordering[K]) =&amp;gt;
      // Create an ExternalSorter to sort the data. Note that if spark.shuffle.spill is disabled,
      // the ExternalSorter won't spill to disk.
      val sorter =
        new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
      sorter.insertAll(aggregatedIter)
      context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
      context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
      context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)
      CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
    case None =&amp;gt;
      aggregatedIter
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;é¦–å…ˆæ˜¯å»ºç«‹ä¸€ä¸ª&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleBlockFetcherIterator&lt;/code&gt;ï¼Œä¼ å…¥çš„å‚æ•°æœ‰&lt;code class=&quot;highlighter-rouge&quot;&gt;mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition)&lt;/code&gt;,è¿™ä¸ªæ˜¯å¿…é¡»çš„ï¼Œåªå–éœ€è¦çš„partitionçš„æ•°æ®ã€‚&lt;/p&gt;

&lt;p&gt;ç‚¹è¿›å»ShuffleBlockFetcherIteratorè¿™ä¸ªç±»ï¼Œå‘ç°è¿™ä¸ªç±»ä¼šè‡ªåŠ¨è°ƒç”¨initialize()æ–¹æ³•ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private[this] def initialize(): Unit = {
  // Add a task completion callback (called in both success case and failure case) to cleanup.
  context.addTaskCompletionListener(_ =&amp;gt; cleanup())

  // Split local and remote blocks.
  val remoteRequests = splitLocalRemoteBlocks()
  // Add the remote requests into our queue in a random order
  fetchRequests ++= Utils.randomize(remoteRequests)
  assert ((0 == reqsInFlight) == (0 == bytesInFlight),
    &quot;expected reqsInFlight = 0 but found reqsInFlight = &quot; + reqsInFlight +
    &quot;, expected bytesInFlight = 0 but found bytesInFlight = &quot; + bytesInFlight)

  // Send out initial requests for blocks, up to our maxBytesInFlight
  fetchUpToMaxBytes()

  val numFetches = remoteRequests.size - fetchRequests.size
  logInfo(&quot;Started &quot; + numFetches + &quot; remote fetches in&quot; + Utils.getUsedTimeMs(startTime))

  // Get Local Blocks
  fetchLocalBlocks()
  logDebug(&quot;Got local blocks in &quot; + Utils.getUsedTimeMs(startTime))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™ä¸ªæ–¹æ³•é‡Œé¢ä¼š&lt;code class=&quot;highlighter-rouge&quot;&gt;fetchUpToMaxBytes()&lt;/code&gt;å’Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;fetchLocalBlocks()&lt;/code&gt;,ä¸€ä¸ªæ˜¯å–è¿œç¨‹æ•°æ®ä¸€ä¸ªæ˜¯å–æœ¬åœ°æ•°æ®ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def fetchUpToMaxBytes(): Unit = {
  // Send fetch requests up to maxBytesInFlight
  while (fetchRequests.nonEmpty &amp;amp;&amp;amp;
    (bytesInFlight == 0 ||
      (reqsInFlight + 1 &amp;lt;= maxReqsInFlight &amp;amp;&amp;amp;
        bytesInFlight + fetchRequests.front.size &amp;lt;= maxBytesInFlight))) {
    sendRequest(fetchRequests.dequeue())
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œä¼šè®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼Œé¿å…è¿‡åº¦è´Ÿè½½çš„ã€‚&lt;code class=&quot;highlighter-rouge&quot;&gt;sendRequest&lt;/code&gt;æ¥è¯·æ±‚æ•°æ®ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private[this] def sendRequest(req: FetchRequest) {
  logDebug(&quot;Sending request for %d blocks (%s) from %s&quot;.format(
    req.blocks.size, Utils.bytesToString(req.size), req.address.hostPort))
  bytesInFlight += req.size
  reqsInFlight += 1

  // so we can look up the size of each blockID
  val sizeMap = req.blocks.map { case (blockId, size) =&amp;gt; (blockId.toString, size) }.toMap
  val remainingBlocks = new HashSet[String]() ++= sizeMap.keys
  val blockIds = req.blocks.map(_._1.toString)

  val address = req.address
  shuffleClient.fetchBlocks(address.host, address.port, address.executorId, blockIds.toArray,
    new BlockFetchingListener {
      override def onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit = {
        // Only add the buffer to results queue if the iterator is not zombie,
        // i.e. cleanup() has not been called yet.
        ShuffleBlockFetcherIterator.this.synchronized {
          if (!isZombie) {
            // Increment the ref count because we need to pass this to a different thread.
            // This needs to be released after use.
            buf.retain()
            remainingBlocks -= blockId
            results.put(new SuccessFetchResult(BlockId(blockId), address, sizeMap(blockId), buf,
              remainingBlocks.isEmpty))
            logDebug(&quot;remainingBlocks: &quot; + remainingBlocks)
          }
        }
        logTrace(&quot;Got remote block &quot; + blockId + &quot; after &quot; + Utils.getUsedTimeMs(startTime))
      }

      override def onBlockFetchFailure(blockId: String, e: Throwable): Unit = {
        logError(s&quot;Failed to get block(s) from ${req.address.host}:${req.address.port}&quot;, e)
        results.put(new FailureFetchResult(BlockId(blockId), address, e))
      }
    }
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;åé¢ä¸€å¤§å †ä»£ç ï¼Œåæ­£å°±æ˜¯å–æ•°æ®å—ï¼Œå°±ä¸ç»†çœ‹äº†ã€‚&lt;/p&gt;

&lt;p&gt;å–å®Œæ•°æ®ä¹‹åï¼Œå°±é€šè¿‡dep.mapSideCombineåˆ¤æ–­æ˜¯å¦åœ¨mapç«¯åšäº†èšåˆæ“ä½œï¼Œå¦‚æœåšäº†èšåˆæ“ä½œï¼Œè¿™é‡Œçš„(k,v)çš„vå°±æ˜¯CompactBufferç±»å‹ï¼Œå°±è°ƒç”¨combineCombinersByKeyï¼Œå¦‚æœåœ¨mapç«¯æ²¡æœ‰èšåˆï¼Œå°±è¿˜æ˜¯valueç±»å‹ï¼Œå°±combineValuesByKeyã€‚&lt;/p&gt;

&lt;p&gt;ä¹‹åå°±åˆ¤æ–­æ˜¯å¦å®šä¹‰äº†æ’åºï¼Œå¦‚æœéœ€è¦æ’åºå°±ç”¨ExternalSorteræ’åºã€‚&lt;/p&gt;

&lt;p&gt;åˆ°è¿™é‡Œshuffleè¿‡ç¨‹å°±ç»“æŸå•¦ã€‚&lt;/p&gt;

&lt;h2 id=&quot;æ€»ç»“&quot;&gt;æ€»ç»“&lt;/h2&gt;

&lt;p&gt;å‰ä¸¤ç§shuffleWriterï¼ˆUnsafeShuffleWriteræ²¡ç»†çœ‹ï¼‰é‡Œçš„shuffleWriteç«¯æœ€åå¾—åˆ°çš„æ–‡ä»¶éƒ½åªæ˜¯ä¸€ä¸ªIndexFileï¼Œè¿™è·Ÿ&lt;a href=&quot;http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/&quot;&gt;æ—©æœŸçš„shuffleæœºåˆ¶&lt;/a&gt;è¿˜æ˜¯ä¸ä¸€æ ·çš„ã€‚&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/spark/2016/12/26/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90Shuffle%E5%AE%9E%E7%8E%B0</link>
                <guid>http://bbwff.github.io/spark/2016/12/26/sparkæºç åˆ†æShuffleå®ç°</guid>
                <pubDate>2016-12-26T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Sparkå†…å­˜é¢„æµ‹</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sizetracker&quot; id=&quot;markdown-toc-sizetracker&quot;&gt;sizeTracker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sizeestimator&quot; id=&quot;markdown-toc-sizeestimator&quot;&gt;SizeEstimator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;sparkæ˜¯ä¸€ä¸ªå†…å­˜è®¡ç®—æ¡†æ¶ï¼Œå› æ­¤å†…å­˜æ˜¯é‡è¦çš„èµ„æºï¼Œåˆç†çš„ä½¿ç”¨çš„å†…å­˜åœ¨sparkåº”ç”¨åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­éå¸¸é‡è¦ã€‚åœ¨ä½¿ç”¨å†…å­˜çš„è¿‡ç¨‹ï¼Œsparkä¼šé‡‡ç”¨æŠ½æ ·çš„æ–¹æ³•é¢„æµ‹å‡ºæ‰€éœ€è¦çš„å†…å­˜ï¼Œå¹¶é¢„å…ˆåˆ†é…å†…å­˜ã€‚æœ¬æ–‡ä¼šå°±å†…å­˜é¢„æµ‹æœºåˆ¶è¿›è¡Œæºç çš„è§£è¯»ã€‚&lt;/p&gt;

&lt;h2 id=&quot;sizetracker&quot;&gt;sizeTracker&lt;/h2&gt;

&lt;p&gt;sparké‡Œé¢å†…å­˜é¢„æµ‹æœ‰ä¸€ä¸ªtraitï¼Œå«åš&lt;code class=&quot;highlighter-rouge&quot;&gt; SizeTracker&lt;/code&gt;ï¼Œç„¶åæœ‰ä¸€äº›ç±»å®ç°äº†å®ƒï¼Œæ¯”å¦‚PartitionedAppendOnlyMapã€SizeTrackingAppendOnlyMapã€‚&lt;/p&gt;

&lt;p&gt;SizeTrackerçš„estimateSizeæ–¹æ³•å°±æ˜¯é¢„æµ‹å½“å‰é›†åˆçš„sizeã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Estimate the current size of the collection in bytes. O(1) time.
 */
def estimateSize(): Long = {
  assert(samples.nonEmpty)
  val extrapolatedDelta = bytesPerUpdate * (numUpdates - samples.last.numUpdates)
  (samples.last.size + extrapolatedDelta).toLong
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å…¶å®è¿™ä¸ªsizeTrackerç±»æœ‰å››ä¸ªæ–¹æ³•ï¼Œå…¶ä»–ä¸‰ä¸ªæ–¹æ³•åˆ†åˆ«æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;resetSamples&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;afterUpdate&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;takeSample&lt;/code&gt;.çœ‹äº†ä¸‹SizeTrackingAppendOnlyMapçš„æµç¨‹ï¼ŒafterUpdataæ–¹æ³•æ˜¯åœ¨updateæˆ–è€…changeValueä¹‹åä¼šè°ƒç”¨ï¼Œå…¶å®updataå’ŒchangeValueæ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Œåªä¸è¿‡ä¸€ä¸ªæ˜¯ç›´æ¥æ›´æ–°k-vï¼Œå¦ä¸€ä¸ªæ˜¯ä½¿ç”¨ä¸€ä¸ªå‡½æ•°è®¡ç®—åæ›´æ–°k-vã€‚ç„¶åresetSamplesæ˜¯åœ¨growTableä¹‹åè°ƒç”¨ï¼ˆSizeTrackingAppendOnlyMapçš„growTableå°±æ˜¯ç©ºé—´ç¿»ä¸€å€ï¼‰ã€‚&lt;/p&gt;

&lt;p&gt;çœ‹ä¸‹sizeTrackeré‡Œé¢çš„å‚æ•°ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Controls the base of the exponential which governs the rate of sampling.
 * E.g., a value of 2 would mean we sample at 1, 2, 4, 8, ... elements.
 */
private val SAMPLE_GROWTH_RATE = 1.1

/** Samples taken since last resetSamples(). Only the last two are kept for extrapolation. */
private val samples = new mutable.Queue[Sample]

/** The average number of bytes per update between our last two samples. */
private var bytesPerUpdate: Double = _

/** Total number of insertions and updates into the map since the last resetSamples(). */
private var numUpdates: Long = _

/** The value of 'numUpdates' at which we will take our next sample. */
private var nextSampleNum: Long = _
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SAMPLE_GROWTH_RATE&lt;/code&gt;æ˜¯ä¸€ä¸ªæ–œç‡ï¼Œä»£è¡¨ä¸‹æ¬¡æŠ½æ ·æ—¶å€™æ›´æ–°çš„æ¬¡æ•°åº”è¯¥æ˜¯è¿™æ¬¡æŠ½æ ·æ›´æ–°æ¬¡æ•°çš„1.1å€ï¼Œæ¯”å¦‚ä¸Šæ¬¡æ˜¯æ›´æ–°10000æ¬¡æ—¶å€™æŠ½æ ·ï¼Œä¸‹æ¬¡æŠ½æ ·å°±å¾—æ˜¯æ›´æ–°11000æ¬¡æ—¶å€™å†æŠ½æ ·ï¼Œå¯ä»¥é¿å…æ¯æ¬¡æ›´æ–°éƒ½æŠ½æ ·ï¼Œå‡å°‘æŠ½æ ·èŠ±é”€ã€‚&lt;code class=&quot;highlighter-rouge&quot;&gt;samples&lt;/code&gt;æ˜¯ä¸€ä¸ªé˜Ÿåˆ—ï¼Œ é‡Œé¢çš„ç±»å‹æ˜¯æ ·ä¾‹ç±»&lt;code class=&quot;highlighter-rouge&quot;&gt;sample&lt;/code&gt;ã€‚ç„¶å&lt;code class=&quot;highlighter-rouge&quot;&gt;bytesPerUpdate&lt;/code&gt;æ˜¯æŠ½æ ·ä¹‹åå¾—åˆ°åŒºé—´å¢é•¿é‡/ä¸ªæ•°å¢é•¿é‡ï¼Œå°±æ˜¯ä¸€ä¸ªæ–œç‡ã€‚ç„¶å&lt;code class=&quot;highlighter-rouge&quot;&gt;numUpdates&lt;/code&gt;å°±æ˜¯ä»£è¡¨æŠ½æ ·é›†åˆé‡Œé¢å…ƒç´ ä¸ªæ•°ï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;nextSampleNum&lt;/code&gt;ä»£è¡¨ä¸‹æ¬¡è¦æŠ½æ ·çš„æ—¶å€™é›†åˆçš„ä¸ªæ•°ï¼Œå‰é¢è¯´è¿‡ï¼Œå°±æ˜¯æ­¤æ¬¡æŠ½æ ·æ—¶å€™çš„ä¸ªæ•°*1.1.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Reset samples collected so far.
 * This should be called after the collection undergoes a dramatic change in size.
 */
protected def resetSamples(): Unit = {
  numUpdates = 1
  nextSampleNum = 1
  samples.clear()
  takeSample()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;resetSamplesä¼šåœ¨æ¯æ¬¡ç¿»å€å¢é•¿åï¼Œé‡ç½®æŠ½æ ·å‚æ•°ï¼Œæ²¡å•¥å¥½è¯´çš„ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Callback to be invoked after every update.
 */
protected def afterUpdate(): Unit = {
  numUpdates += 1
  if (nextSampleNum == numUpdates) {
    takeSample()
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™ä¸ªæ˜¯æ¯æ¬¡æ›´æ–°åï¼Œéƒ½æ›´æ–°æ¬¡æ•°+1ï¼Œç„¶åå½“ä»–ç­‰äºä¸‹æ¬¡æŠ½æ ·æ¬¡æ•°æ—¶å€™å°±è¿›è¡ŒæŠ½æ ·ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Take a new sample of the current collection's size.
 */
private def takeSample(): Unit = {
  samples.enqueue(Sample(SizeEstimator.estimate(this), numUpdates))
  // Only use the last two samples to extrapolate
  if (samples.size &amp;gt; 2) {
    samples.dequeue()
  }
  val bytesDelta = samples.toList.reverse match {
    case latest :: previous :: tail =&amp;gt;
      (latest.size - previous.size).toDouble / (latest.numUpdates - previous.numUpdates)
    // If fewer than 2 samples, assume no change
    case _ =&amp;gt; 0
  }
  bytesPerUpdate = math.max(0, bytesDelta)
  nextSampleNum = math.ceil(numUpdates * SAMPLE_GROWTH_RATE).toLong
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;æŠ½æ ·å°±æ˜¯æ‰¾å‡ºæœ€è¿‘çš„ä¸¤ä¸ªsampleï¼Œç„¶åè®¡ç®—å¢é•¿æ–œç‡ï¼Œsizeå¢é•¿é‡/numå¢é•¿é‡ï¼Œç„¶åæŠŠä¸‹æ¬¡æŠ½æ ·çš„æ¬¡æ•°*1.1æ›´æ–°ä¸‹ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Estimate the current size of the collection in bytes. O(1) time.
 */
def estimateSize(): Long = {
  assert(samples.nonEmpty)
  val extrapolatedDelta = bytesPerUpdate * (numUpdates - samples.last.numUpdates)
  (samples.last.size + extrapolatedDelta).toLong
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ç„¶åè¿™ä¸ªestimateSize å°±æ˜¯ä¸Šæ¬¡çš„size+å¢é•¿ç‡*å¢é•¿é‡ã€‚å¢é•¿ç‡å’Œsizeå°±æ˜¯ä¸Šæ¬¡æŠ½æ ·å¾—åˆ°çš„ã€‚&lt;/p&gt;

&lt;p&gt;å¯ä»¥çœ‹åˆ°åœ¨takeSampleæ–¹æ³•é‡Œé¢åŠ å…¥é˜Ÿåˆ—æ—¶å€™sizeçš„é¢„æµ‹ç”¨åˆ°äº†&lt;code class=&quot;highlighter-rouge&quot;&gt;SizeEstimator.estimate&lt;/code&gt;.çœ‹ä¸‹è¿™ä¸ªSizeEstimatorç±»ã€‚&lt;/p&gt;

&lt;h2 id=&quot;sizeestimator&quot;&gt;SizeEstimator&lt;/h2&gt;

&lt;p&gt;çœ‹ä¸‹è¿™ç±»çš„estimateæ–¹æ³•ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def estimate(obj: AnyRef, visited: IdentityHashMap[AnyRef, AnyRef]): Long = {
  val state = new SearchState(visited)
  state.enqueue(obj)
  while (!state.isFinished) {
    visitSingleObject(state.dequeue(), state)
  }
  state.size
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œä¸»è¦æ˜¯è°ƒç”¨visitSingleObjectã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def visitSingleObject(obj: AnyRef, state: SearchState) {
  val cls = obj.getClass
  if (cls.isArray) {
    visitArray(obj, cls, state)
  } else if (cls.getName.startsWith(&quot;scala.reflect&quot;)) {
    // Many objects in the scala.reflect package reference global reflection objects which, in
    // turn, reference many other large global objects. Do nothing in this case.
  } else if (obj.isInstanceOf[ClassLoader] || obj.isInstanceOf[Class[_]]) {
    // Hadoop JobConfs created in the interpreter have a ClassLoader, which greatly confuses
    // the size estimator since it references the whole REPL. Do nothing in this case. In
    // general all ClassLoaders and Classes will be shared between objects anyway.
  } else {
    obj match {
      case s: KnownSizeEstimation =&amp;gt;
        state.size += s.estimatedSize
      case _ =&amp;gt;
        val classInfo = getClassInfo(cls)
        state.size += alignSize(classInfo.shellSize)
        for (field &amp;lt;- classInfo.pointerFields) {
          state.enqueue(field.get(obj))
        }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å¦‚æœæ˜¯Arrayç±»å‹ï¼Œå°±visitArrayã€‚å¦‚æœæ˜¯scala.reflectå¼€å¤´çš„ç±»ï¼Œå› ä¸ºè¿™ä¸ªåŒ…é‡Œé¢æ¶‰åŠå…¨å±€åå°„å¯¹è±¡ï¼Œå› æ­¤æ¶‰åŠå¾ˆå¤šå…¶ä»–çš„å¤§å¯¹è±¡ï¼Œæ‰€ä»¥è¿™ç§å¯¹è±¡ä¸åšä»»ä½•æ“ä½œã€‚ç„¶åå¦‚æœæ˜¯classLoaderç±»å‹ï¼Œhadoop ä½œä¸šåœ¨è§£é‡Šå™¨ä¸­åˆ›å»ºäº†classLoaderï¼Œå› ä¸ºæ¶‰åŠæ•´ä¸ªREPLï¼ˆè¯»å–-æ±‚å€¼-å¤„ç†-å¾ªç¯ï¼‰ï¼Œæ‰€ä»¥å¾ˆéš¾å¤„ç†ã€‚ä¸€èˆ¬ï¼Œæ‰€æœ‰classLoaderå’Œclasseséƒ½æ˜¯å…±äº«çš„ã€‚ç„¶åæœ‰çš„å°±æ˜¯å·²ç»é¢„æµ‹è¿‡çš„ï¼Œç›´æ¥è¯»å–ã€‚ç„¶åå…¶ä»–ç±»å‹ï¼Œå°±æ˜¯æ‹†è§£ï¼Œæ‹†æˆå®é™…å¯¹è±¡å’Œå¼•ç”¨ï¼Œå®é™…å¯¹è±¡ç®—å‡ºsizeç›¸åŠ ï¼Œç„¶åæŒ‡é’ˆç±»å‹å°±æŠŠå®ƒæŒ‡å‘çš„å¯¹è±¡åŠ å…¥stateé˜Ÿåˆ—ï¼Œç„¶åå†è¿›å…¥whileå¾ªç¯ã€‚ç›´åˆ°state isFinishedã€‚&lt;/p&gt;

&lt;p&gt;æ¥ä¸‹æ¥çœ‹çœ‹visitArray.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Estimate the size of arrays larger than ARRAY_SIZE_FOR_SAMPLING by sampling.
private val ARRAY_SIZE_FOR_SAMPLING = 400
private val ARRAY_SAMPLE_SIZE = 100 // should be lower than ARRAY_SIZE_FOR_SAMPLING

private def visitArray(array: AnyRef, arrayClass: Class[_], state: SearchState) {
  val length = ScalaRunTime.array_length(array)
  val elementClass = arrayClass.getComponentType()

  // Arrays have object header and length field which is an integer
  var arrSize: Long = alignSize(objectSize + INT_SIZE)

  if (elementClass.isPrimitive) {
    arrSize += alignSize(length.toLong * primitiveSize(elementClass))
    state.size += arrSize
  } else {
    arrSize += alignSize(length.toLong * pointerSize)
    state.size += arrSize

    if (length &amp;lt;= ARRAY_SIZE_FOR_SAMPLING) {
      var arrayIndex = 0
      while (arrayIndex &amp;lt; length) {
        state.enqueue(ScalaRunTime.array_apply(array, arrayIndex).asInstanceOf[AnyRef])
        arrayIndex += 1
      }
    } else {
      // Estimate the size of a large array by sampling elements without replacement.
      // To exclude the shared objects that the array elements may link, sample twice
      // and use the min one to calculate array size.
      val rand = new Random(42)
      val drawn = new OpenHashSet[Int](2 * ARRAY_SAMPLE_SIZE)
      val s1 = sampleArray(array, state, rand, drawn, length)
      val s2 = sampleArray(array, state, rand, drawn, length)
      val size = math.min(s1, s2)
      state.size += math.max(s1, s2) +
        (size * ((length - ARRAY_SAMPLE_SIZE) / (ARRAY_SAMPLE_SIZE))).toLong
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™æ®µä»£ç ï¼Œé¦–å…ˆè¦æŠŠ&lt;code class=&quot;highlighter-rouge&quot;&gt;Array çš„object å¤´éƒ¨,é•¿åº¦ filed&lt;/code&gt;ç®—è¿›å»ï¼Œç„¶åå¦‚æœarrayé‡Œé¢çš„å…ƒç´ æ˜¯åŸºæœ¬ç±»å‹ï¼Œé‚£ä¹ˆé•¿åº¦å°±å›ºå®šï¼Œå°±å¯ä»¥ç›´æ¥ç®—å‡ºæ¥ã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœä¸æ˜¯åŸºæœ¬ç±»å‹ï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;å°±æœ‰æŒ‡å‘å¯¹è±¡çš„å¼•ç”¨ï¼Ÿ&lt;/code&gt;æ‰€ä»¥ä»£ç é‡Œé¢å…ˆæŠŠlengthä¸ªæŒ‡é’ˆå ç”¨çš„ç©ºé—´åŠ ä¸Šã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœè¿™æ—¶å€™æ•°ç»„é•¿åº¦ï¼Œå°äºé‡‡æ ·æ—¶å€™æ•°ç»„é•¿åº¦é‚£ä¸ªç•Œé™ï¼Œå°±æŠŠæ•°ç»„é‡Œé¢å¼•ç”¨æŒ‡å‘çš„å¯¹è±¡åŠ å…¥stateé˜Ÿåˆ—ï¼Œä¹Ÿå°±æ˜¯å°äºç•Œé™å°±å…¨éƒ¨è®¡ç®—sizeã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœæ•°ç»„é•¿åº¦å¤§äºé‡‡æ ·æ—¶å€™æ•°ç»„é•¿åº¦çš„ç•Œé™ï¼Œå°±å‡†å¤‡é‡‡æ ·ã€‚ç„¶åé‡‡æ ·ä¸¤ç»„ï¼Œä¸¤ç»„é‡‡æ ·æ•°æ®éƒ½æ˜¯ä¸é‡å¤çš„ã€‚è®¡ç®—å…¬å¼å¦‚ä¸‹:&lt;code class=&quot;highlighter-rouge&quot;&gt;math.max(s1, s2) + (math.min(s1, s2) * ((length - ARRAY_SAMPLE_SIZE) / (ARRAY_SAMPLE_SIZE)))&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;è¿™ä¸ªè®¡ç®—å…¬å¼ä¸çŸ¥é“æœ‰ä»€ä¹ˆåˆç†çš„åœ°æ–¹ï¼Œåæ­£sparkç”¨è¿™ä¸ªå…¬å¼ï¼Œåº”è¯¥æ˜¯æœ‰ä¸€å®šé“ç†ã€‚&lt;/p&gt;

&lt;p&gt;å°±æ˜¯  &lt;code class=&quot;highlighter-rouge&quot;&gt;math.min(s1,s2)*(length-ARRAY_SAMPLE_SIZE)+abs(s1-s2)&lt;/code&gt;ï¼Œè¿™åº”è¯¥æ˜¯ä¸ºäº†ä¸è®©å†…å­˜é¢„ä¼°è¿‡å¤§ï¼Œä»¥å…å ç”¨å¤ªå¤šï¼ŒåŒæ—¶ç”¨ä¸€ä¸ªå°çš„å¢é‡å¯¹è¿™ä¸ªåå°çš„é¢„ä¼°è¿›è¡Œè¡¥å¿ã€‚&lt;/p&gt;

</description>
                <link>http://bbwff.github.io/spark/2016/12/26/spark%E5%86%85%E5%AD%98%E9%A2%84%E6%B5%8B</link>
                <guid>http://bbwff.github.io/spark/2016/12/26/sparkå†…å­˜é¢„æµ‹</guid>
                <pubDate>2016-12-26T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Sparkåº”ç”¨æ‰§è¡Œæµç¨‹</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#word-count&quot; id=&quot;markdown-toc-word-count&quot;&gt;Word Count&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ç†è®ºå‰–æ&quot; id=&quot;markdown-toc-ç†è®ºå‰–æ&quot;&gt;ç†è®ºå‰–æ&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#æºç å‰–æ&quot; id=&quot;markdown-toc-æºç å‰–æ&quot;&gt;æºç å‰–æ&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#æäº¤job&quot; id=&quot;markdown-toc-æäº¤job&quot;&gt;æäº¤job&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#åˆ’åˆ†stage&quot; id=&quot;markdown-toc-åˆ’åˆ†stage&quot;&gt;åˆ’åˆ†stage&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#æäº¤tasks&quot; id=&quot;markdown-toc-æäº¤tasks&quot;&gt;æäº¤tasks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#æ‰§è¡Œtask&quot; id=&quot;markdown-toc-æ‰§è¡Œtask&quot;&gt;æ‰§è¡Œtask&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#shufflemaptask&quot; id=&quot;markdown-toc-shufflemaptask&quot;&gt;ShuffleMapTask&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#resulttask&quot; id=&quot;markdown-toc-resulttask&quot;&gt;ResultTask&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#rdd-è¿­ä»£é“¾&quot; id=&quot;markdown-toc-rdd-è¿­ä»£é“¾&quot;&gt;rdd è¿­ä»£é“¾&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#æ£€æŸ¥ç‚¹&quot; id=&quot;markdown-toc-æ£€æŸ¥ç‚¹&quot;&gt;æ£€æŸ¥ç‚¹&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#compute-é“¾&quot; id=&quot;markdown-toc-compute-é“¾&quot;&gt;compute é“¾&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#å‚è€ƒ&quot; id=&quot;markdown-toc-å‚è€ƒ&quot;&gt;å‚è€ƒ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;ä»æœ€ç®€å•çš„sparkåº”ç”¨WordCountå…¥æ‰‹ï¼Œåˆ†ærddé“¾ï¼Œåˆ†æjobå¦‚ä½•æäº¤ï¼Œtaskå¦‚ä½•æäº¤ï¼Œä»å…¨å±€äº†è§£sparkåº”ç”¨çš„æ‰§è¡Œæµç¨‹ã€‚&lt;/p&gt;

&lt;h2 id=&quot;word-count&quot;&gt;Word Count&lt;/h2&gt;

&lt;p&gt;word countæ˜¯spark æœ€åŸºæœ¬çš„å°ç¨‹åºï¼Œä¸»è¦åŠŸèƒ½å°±æ˜¯ç»Ÿè®¡ä¸€ä¸ªæ–‡ä»¶é‡Œé¢å„ä¸ªå•è¯å‡ºç°çš„ä¸ªæ•°ã€‚ä»£ç å¾ˆç®€æ´ï¼Œå¦‚ä¸‹ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import org.apache.spark.{SparkConf, SparkContext}

object SparkWC {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf()
    val sparkContext = new SparkContext(sparkConf)
    sparkContext.textFile(args(0))
          .flatMap(line =&amp;gt; line.split(&quot; &quot;))
          .map(word =&amp;gt; (word, 1))
          .reduceByKey(_ + _)
          .saveAsTextFile(args(1))
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ç†è®ºå‰–æ&quot;&gt;ç†è®ºå‰–æ&lt;/h3&gt;
&lt;p&gt;é‡Œé¢çš„RDDé“¾ï¼Œç”¨ä»–ä»¬çš„æ“ä½œè¡¨ç¤ºï¼Œå°±æ˜¯textFile-&amp;gt;flatMap-&amp;gt;map-&amp;gt;reduceBykey-&amp;gt;saveAsTextFile.&lt;/p&gt;

&lt;p&gt;sparké‡Œé¢æœ‰ä¸¤ç§æ“ä½œï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt; å’Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;transformation&lt;/code&gt;ï¼Œå…¶ä¸­actionä¼šè§¦å‘æäº¤jobçš„æ“ä½œï¼Œtransformationä¸ä¼šè§¦å‘jobï¼Œåªæ˜¯è¿›è¡Œrddçš„è½¬æ¢ã€‚è€Œä¸åŒtransformationæ“ä½œçš„rddé“¾ä¸¤ç«¯çš„ä¾èµ–å…³ç³»ä¹Ÿä¸åŒï¼Œsparkä¸­çš„rddä¾èµ–æœ‰ä¸¤ç§ï¼Œåˆ†åˆ«æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;narrow dependency&lt;/code&gt; å’Œ &lt;code class=&quot;highlighter-rouge&quot;&gt;wide dependency&lt;/code&gt; ,è¿™ä¸¤ç§ä¾èµ–å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
&lt;br /&gt;
&lt;img src=&quot;http://ogk82bfkr.bkt.clouddn.com/upload/narrow-depen.png&quot; height=&quot;200&quot; width=&quot;200&quot; /&gt;
Â Â Â Â Â Â 
&lt;img src=&quot;http://ogk82bfkr.bkt.clouddn.com/upload/wide-depen.png&quot; height=&quot;200&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;å·¦è¾¹å›¾æ˜¯çª„ä¾èµ–ï¼Œå³è¾¹å›¾æ˜¯å®½ä¾èµ–ï¼Œçª„ä¾èµ–é‡Œé¢çš„partitionçš„å¯¹åº”é¡ºåºæ˜¯ä¸å˜çš„ï¼Œæ¬¾ä¾èµ–ä¼šæ¶‰åŠshuffleæ“ä½œï¼Œä¼šé€ æˆpartitionæ··æ´—ï¼Œå› æ­¤å¾€å¾€ä»¥æ¬¾ä¾èµ–åˆ’åˆ†stageã€‚åœ¨ä¸Šé¢çš„æ“ä½œä¸­ï¼ŒsaveAsTextFileæ˜¯actionï¼ŒreduceByKeyæ˜¯å®½ä¾èµ–ï¼Œå› æ­¤è¿™ä¸ªåº”ç”¨æ€»å…±æœ‰1ä¸ªjobï¼Œä¸¤ä¸ªstageï¼Œç„¶ååœ¨ä¸åŒçš„stageä¸­ä¼šæ‰§è¡Œtasksã€‚&lt;/p&gt;

&lt;h3 id=&quot;æºç å‰–æ&quot;&gt;æºç å‰–æ&lt;/h3&gt;

&lt;p&gt;ä»rddé“¾å¼€å§‹åˆ†æã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair =&amp;gt; pair._2.toString).setName(path)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;textFile è¿™ä¸ªç®—å­çš„è¿”å›ç»“æœæ˜¯ä¸€ä¸ªRDDï¼Œç„¶åRDDé“¾å°±å¼€å§‹äº†ï¼Œå¯ä»¥çœ‹å‡ºæ¥ä»–è°ƒç”¨äº†ä¸€äº›æ–°çš„å‡½æ•°ï¼Œæ¯”å¦‚hadoopFileå•¥çš„ï¼Œè¿™äº›æˆ‘ä»¬éƒ½ä¸ç®¡ï¼Œå› ä¸ºä»–ä»¬éƒ½æ²¡æœ‰è§¦å‘ commitJobï¼Œæ‰€ä»¥è¿™äº›ä¸­é—´è¿‡ç¨‹æˆ‘ä»¬å°±çœç•¥ï¼Œç›´åˆ°saveAsTextFileè¿™ä¸ªactionã€‚&lt;/p&gt;

&lt;h3 id=&quot;æäº¤job&quot;&gt;æäº¤job&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  def saveAsTextFile(path: String): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    //
    // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit
    // Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`
    // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an
    // Ordering for `NullWritable`. That's why the compiler will generate different anonymous
    // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.
    //
    // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate
    // same bytecodes for `saveAsTextFile`.
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =&amp;gt;
      val text = new Text()
      iter.map { x =&amp;gt;
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)
  }
  
  
  //æ¥ä¸‹æ¥è°ƒç”¨è¿™ä¸ª
  def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]](
      path: String)(implicit fm: ClassTag[F]): Unit = self.withScope {
    saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])
  }
  
//çœç•¥ä¸€éƒ¨åˆ†è°ƒç”¨è¿‡ç¨‹
...
...

//æœ€åè°ƒç”¨è¿™ä¸ªå‡½æ•°
  def saveAsHadoopDataset(conf: JobConf): Unit = self.withScope {
    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).
    val hadoopConf = conf
    val outputFormatInstance = hadoopConf.getOutputFormat
    val keyClass = hadoopConf.getOutputKeyClass
    val valueClass = hadoopConf.getOutputValueClass
    if (outputFormatInstance == null) {
      throw new SparkException(&quot;Output format class not set&quot;)
    }
    if (keyClass == null) {
      throw new SparkException(&quot;Output key class not set&quot;)
    }
    if (valueClass == null) {
      throw new SparkException(&quot;Output value class not set&quot;)
    }
    SparkHadoopUtil.get.addCredentials(hadoopConf)

    logDebug(&quot;Saving as hadoop file of type (&quot; + keyClass.getSimpleName + &quot;, &quot; +
      valueClass.getSimpleName + &quot;)&quot;)

    if (isOutputSpecValidationEnabled) {
      // FileOutputFormat ignores the filesystem parameter
      val ignoredFs = FileSystem.get(hadoopConf)
      hadoopConf.getOutputFormat.checkOutputSpecs(ignoredFs, hadoopConf)
    }

    val writer = new SparkHadoopWriter(hadoopConf)
    writer.preSetup()

    val writeToFile = (context: TaskContext, iter: Iterator[(K, V)]) =&amp;gt; {
      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it
      // around by taking a mod. We expect that no task will be attempted 2 billion times.
      val taskAttemptId = (context.taskAttemptId % Int.MaxValue).toInt

      val outputMetricsAndBytesWrittenCallback: Option[(OutputMetrics, () =&amp;gt; Long)] =
        initHadoopOutputMetrics(context)

      writer.setup(context.stageId, context.partitionId, taskAttemptId)
      writer.open()
      var recordsWritten = 0L

      Utils.tryWithSafeFinallyAndFailureCallbacks {
        while (iter.hasNext) {
          val record = iter.next()
          writer.write(record._1.asInstanceOf[AnyRef], record._2.asInstanceOf[AnyRef])

          // Update bytes written metric every few records
          maybeUpdateOutputMetrics(outputMetricsAndBytesWrittenCallback, recordsWritten)
          recordsWritten += 1
        }
      }(finallyBlock = writer.close())
      writer.commit()
      outputMetricsAndBytesWrittenCallback.foreach { case (om, callback) =&amp;gt;
        om.setBytesWritten(callback())
        om.setRecordsWritten(recordsWritten)
      }
    }

    self.context.runJob(self, writeToFile)
    writer.commitJob()
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;ä¸Šé¢æ˜¯saveAstextFileçš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸­é—´çœç•¥äº†ä¸€ä¸ªå‡½æ•°ï¼Œçœ‹ä»£ç çš„æœ€åä¸¤è¡Œã€‚å¯ä»¥çœ‹å‡ºè°ƒç”¨äº†&lt;code class=&quot;highlighter-rouge&quot;&gt; self.context.runJob()&lt;/code&gt;å¯ä»¥çŸ¥é“è¿™é‡Œè§¦å‘äº†jobçš„æäº¤ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&amp;gt; U,
      partitions: Seq[Int],
      resultHandler: (Int, U) =&amp;gt; Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException(&quot;SparkContext has been shutdown&quot;)
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo(&quot;Starting job: &quot; + callSite.shortForm)
    if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) {
      logInfo(&quot;RDD's recursive dependencies:\n&quot; + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint() //æ˜¯å¦cache rdd
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å¯ä»¥çœ‹å‡ºä¸Šé¢ä»£ç æœ‰ &lt;code class=&quot;highlighter-rouge&quot;&gt;dagScheduler.runJob&lt;/code&gt;ï¼Œå¼€å§‹è¿›è¡Œè°ƒåº¦ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&amp;gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&amp;gt; Unit,
      properties: Properties): Unit = {
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    // Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,
    // which causes concurrent SQL executions to fail if a fork-join pool is used. Note that
    // due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it's
    // safe to pass in null here. For more detail, see SPARK-13747.
    val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait]
    waiter.completionFuture.ready(Duration.Inf)(awaitPermission)
    waiter.completionFuture.value.get match {
      case scala.util.Success(_) =&amp;gt;
        logInfo(&quot;Job %d finished: %s, took %f s&quot;.format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =&amp;gt;
        logInfo(&quot;Job %d failed: %s, took %f s&quot;.format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.
        val callerStackTrace = Thread.currentThread().getStackTrace.tail
        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)
        throw exception
    }
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;åœ¨ dagScheduler.runJob()é‡Œé¢æœ‰ &lt;code class=&quot;highlighter-rouge&quot;&gt;submitJob&lt;/code&gt;çš„æ“ä½œï¼Œæäº¤jobã€‚
çœ‹ä¸‹é¢&lt;code class=&quot;highlighter-rouge&quot;&gt;submitJob&lt;/code&gt;çš„ä»£ç ã€‚&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&amp;gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&amp;gt; Unit,
      properties: Properties): JobWaiter[U] = {
    // Check to make sure we are not launching a task on a partition that does not exist.
    val maxPartitions = rdd.partitions.length
    partitions.find(p =&amp;gt; p &amp;gt;= maxPartitions || p &amp;lt; 0).foreach { p =&amp;gt;
      throw new IllegalArgumentException(
        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +
          &quot;Total number of partitions: &quot; + maxPartitions)
    }

    val jobId = nextJobId.getAndIncrement()
    if (partitions.size == 0) {
      // Return immediately if the job is running 0 tasks
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }

    assert(partitions.size &amp;gt; 0)
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&amp;gt; _]
    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
    waiter
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;ç„¶åeventProcessLoop.post(JobSubmitted â€¦ ç„¶åå°±æœ‰å¾ªç¯ç¨‹åºå¤„ç† è¿™ä¸ªpostã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
  case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&amp;gt;
    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;åˆ’åˆ†stage&quot;&gt;åˆ’åˆ†stage&lt;/h3&gt;

&lt;p&gt;æäº¤å®Œjobä¹‹åï¼Œä¼šå¯¹stageè¿›è¡Œåˆ’åˆ†ã€‚&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;handleJobSubmitted&lt;/code&gt;,å¦‚ä¸‹ä»£ç ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private[scheduler] def handleJobSubmitted(jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) =&amp;gt; _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) {
  var finalStage: ResultStage = null
  try {
    // New stage creation may throw an exception if, for example, jobs are run on a
    // HadoopRDD whose underlying HDFS files have been deleted.
    finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)
  } catch {
    case e: Exception =&amp;gt;
      logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)
      listener.jobFailed(e)
      return
  }

  val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
  clearCacheLocs()
  logInfo(&quot;Got job %s (%s) with %d output partitions&quot;.format(
    job.jobId, callSite.shortForm, partitions.length))
  logInfo(&quot;Final stage: &quot; + finalStage + &quot; (&quot; + finalStage.name + &quot;)&quot;)
  logInfo(&quot;Parents of final stage: &quot; + finalStage.parents)
  logInfo(&quot;Missing parents: &quot; + getMissingParentStages(finalStage))

  val jobSubmissionTime = clock.getTimeMillis()
  jobIdToActiveJob(jobId) = job
  activeJobs += job
  finalStage.setActiveJob(job)
  val stageIds = jobIdToStageIds(jobId).toArray
  val stageInfos = stageIds.flatMap(id =&amp;gt; stageIdToStage.get(id).map(_.latestInfo))
  listenerBus.post(
    SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))
  submitStage(finalStage)

  submitWaitingStages()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œå…ˆæ˜¯æ‰¾åˆ°æœ€åä¸€ä¸ªstageï¼Œ finalStageï¼Œç„¶åå°±ç”ŸæˆstageIdè¿˜æœ‰stageçš„ä¸€äº›ä¿¡æ¯ï¼Œç„¶åpost å‡ºjobå¼€å§‹çš„æ¶ˆæ¯ï¼Œç„¶åæäº¤æœ€åä¸€ä¸ªstageï¼Œæœ€åä¸€è¡Œæ˜¯æäº¤ç­‰å¾…çš„stagesã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/** Submits stage, but first recursively submits any missing parents. */
private def submitStage(stage: Stage) {
  val jobId = activeJobForStage(stage)
  if (jobId.isDefined) {
    logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)
    if (!waitingStages(stage) &amp;amp;&amp;amp; !runningStages(stage) &amp;amp;&amp;amp; !failedStages(stage)) {
      val missing = getMissingParentStages(stage).sortBy(_.id)
      logDebug(&quot;missing: &quot; + missing)
      if (missing.isEmpty) {
        logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)
        submitMissingTasks(stage, jobId.get)
      } else {
        for (parent &amp;lt;- missing) {
          submitStage(parent)
        }
        waitingStages += stage
      }
    }
  } else {
    abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œå°±æ˜¯é€’å½’æäº¤ä¹‹å‰éƒ½æ²¡æœ‰æäº¤çš„stageï¼Œå› ä¸ºä¹‹å‰æ˜¯æäº¤æœ€åä¸€ä¸ªstageå—ï¼Œä½†æ˜¯å‰é¢stageä¹Ÿæ²¡æ“ä½œï¼Œæ‰€ä»¥è¦ä¸æ–­åœ°æäº¤parentStageï¼Œç›´åˆ°jobçš„å¤´éƒ¨ã€‚å¦‚æœè¯´è¿™ä¸ªstageæ²¡æœ‰æœªå®Œæˆçš„parentStageï¼Œé‚£å°±ä»£è¡¨å®ƒå‰é¢éƒ½æ‰§è¡Œå®Œæ¯•ã€‚&lt;/p&gt;

&lt;h3 id=&quot;æäº¤tasks&quot;&gt;æäº¤tasks&lt;/h3&gt;

&lt;p&gt;æ‰¾åˆ°æœ€å¼€å§‹è¿˜æ²¡å®Œæˆçš„stageï¼Œé‚£ä¹ˆæäº¤è¿™ä¸ªstageçš„Tasksã€‚è°ƒç”¨çš„å‡½æ•°æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;submitMissingTasks(stage,jobId.get)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;ä¸‹é¢æ˜¯ è¿™ä¸ªå‡½æ•°çš„ä»£ç ï¼Œæœ‰ç‚¹é•¿ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def submitMissingTasks(stage: Stage, jobId: Int) {
  logDebug(&quot;submitMissingTasks(&quot; + stage + &quot;)&quot;)
  // Get our pending tasks and remember them in our pendingTasks entry
  stage.pendingPartitions.clear()

  // First figure out the indexes of partition ids to compute.
  val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()

  // Use the scheduling pool, job group, description, etc. from an ActiveJob associated
  // with this Stage
  val properties = jobIdToActiveJob(jobId).properties

  runningStages += stage
  // SparkListenerStageSubmitted should be posted before testing whether tasks are
  // serializable. If tasks are not serializable, a SparkListenerStageCompleted event
  // will be posted, which should always come after a corresponding SparkListenerStageSubmitted
  // event.
  stage match {
    case s: ShuffleMapStage =&amp;gt;
      outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
    case s: ResultStage =&amp;gt;
      outputCommitCoordinator.stageStart(
        stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
  }
  val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
    stage match {
      case s: ShuffleMapStage =&amp;gt;
        partitionsToCompute.map { id =&amp;gt; (id, getPreferredLocs(stage.rdd, id))}.toMap
      case s: ResultStage =&amp;gt;
        val job = s.activeJob.get
        partitionsToCompute.map { id =&amp;gt;
          val p = s.partitions(id)
          (id, getPreferredLocs(stage.rdd, p))
        }.toMap
    }
  } catch {
    case NonFatal(e) =&amp;gt;
      stage.makeNewStageAttempt(partitionsToCompute.size)
      listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
      abortStage(stage, s&quot;Task creation failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
      runningStages -= stage
      return
  }

  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)
  listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))

  // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.
  // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast
  // the serialized copy of the RDD and for each task we will deserialize it, which means each
  // task gets a different copy of the RDD. This provides stronger isolation between tasks that
  // might modify state of objects referenced in their closures. This is necessary in Hadoop
  // where the JobConf/Configuration object is not thread-safe.
  var taskBinary: Broadcast[Array[Byte]] = null
  try {
    // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
    // For ResultTask, serialize and broadcast (rdd, func).
    val taskBinaryBytes: Array[Byte] = stage match {
      case stage: ShuffleMapStage =&amp;gt;
        JavaUtils.bufferToArray(
          closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
      case stage: ResultStage =&amp;gt;
        JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
    }

    taskBinary = sc.broadcast(taskBinaryBytes)
  } catch {
    // In the case of a failure during serialization, abort the stage.
    case e: NotSerializableException =&amp;gt;
      abortStage(stage, &quot;Task not serializable: &quot; + e.toString, Some(e))
      runningStages -= stage

      // Abort execution
      return
    case NonFatal(e) =&amp;gt;
      abortStage(stage, s&quot;Task serialization failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
      runningStages -= stage
      return
  }

  val tasks: Seq[Task[_]] = try {
    stage match {
      case stage: ShuffleMapStage =&amp;gt;
        partitionsToCompute.map { id =&amp;gt;
          val locs = taskIdToLocations(id)
          val part = stage.rdd.partitions(id)
          new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
            taskBinary, part, locs, stage.latestInfo.taskMetrics, properties)
        }

      case stage: ResultStage =&amp;gt;
        val job = stage.activeJob.get
        partitionsToCompute.map { id =&amp;gt;
          val p: Int = stage.partitions(id)
          val part = stage.rdd.partitions(p)
          val locs = taskIdToLocations(id)
          new ResultTask(stage.id, stage.latestInfo.attemptId,
            taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics)
        }
    }
  } catch {
    case NonFatal(e) =&amp;gt;
      abortStage(stage, s&quot;Task creation failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
      runningStages -= stage
      return
  }

  if (tasks.size &amp;gt; 0) {
    logInfo(&quot;Submitting &quot; + tasks.size + &quot; missing tasks from &quot; + stage + &quot; (&quot; + stage.rdd + &quot;)&quot;)
    stage.pendingPartitions ++= tasks.map(_.partitionId)
    logDebug(&quot;New pending partitions: &quot; + stage.pendingPartitions)
    taskScheduler.submitTasks(new TaskSet(
      tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
    stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
  } else {
    // Because we posted SparkListenerStageSubmitted earlier, we should mark
    // the stage as completed here in case there are no tasks to run
    markStageAsFinished(stage, None)

    val debugString = stage match {
      case stage: ShuffleMapStage =&amp;gt;
        s&quot;Stage ${stage} is actually done; &quot; +
          s&quot;(available: ${stage.isAvailable},&quot; +
          s&quot;available outputs: ${stage.numAvailableOutputs},&quot; +
          s&quot;partitions: ${stage.numPartitions})&quot;
      case stage : ResultStage =&amp;gt;
        s&quot;Stage ${stage} is actually done; (partitions: ${stage.numPartitions})&quot;
    }
    logDebug(debugString)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ä¸Šé¢çš„ä»£ç å‡ºç°äº†å¤šæ¬¡&lt;code class=&quot;highlighter-rouge&quot;&gt;ResultStage&lt;/code&gt;å’Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleMapStage&lt;/code&gt;ï¼Œå…ˆä»‹ç»ä¸€ä¸‹è¿™ä¸ªstageã€‚&lt;/p&gt;

&lt;p&gt;å‰é¢æˆ‘ä»¬è¯´è¿‡ï¼ŒWordCountåªæœ‰ä¸€ä¸ªjobï¼Œç„¶åreduceByKeyæ˜¯shuffleæ“ä½œï¼Œä»¥è¿™ä¸ªä¸ºstageçš„è¾¹ç•Œã€‚é‚£ä¹ˆå‰é¢çš„stageå°±æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleMapStage&lt;/code&gt;ï¼Œåé¢çš„stageå°±æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;ResultStage&lt;/code&gt;.å› ä¸ºå‰é¢ä¼šæœ‰shuffleæ“ä½œï¼Œè€Œåé¢æ˜¯æ•´ä¸ªjobçš„è®¡ç®—ç»“æœï¼Œæ‰€ä»¥å«ResultStage.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ResultStage&lt;/code&gt;æ˜¯æœ‰ä¸€ä¸ªå‡½æ•°ï¼Œåº”ç”¨äºrddçš„ä¸€äº›partitionæ¥è®¡ç®—å‡ºè¿™ä¸ªactionçš„ç»“æœã€‚ä½†æœ‰äº›actionå¹¶ä¸æ˜¯åœ¨æ¯ä¸ªpartitionéƒ½æ‰§è¡Œçš„ï¼Œæ¯”å¦‚&lt;code class=&quot;highlighter-rouge&quot;&gt;first()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;æ¥ä¸‹æ¥ä»‹ç»ä¸‹è¿™ä¸ªå‡½æ•°çš„æ‰§è¡Œæµç¨‹ã€‚&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;é¦–å…ˆæ˜¯è®¡ç®—å‡º &lt;code class=&quot;highlighter-rouge&quot;&gt;paritionsToCompute&lt;/code&gt;ï¼Œå³ç”¨äºè®¡ç®—çš„partitionï¼Œæ•°æ®ã€‚&lt;/li&gt;
  &lt;li&gt;ç„¶åå°±æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;outputCommitCoordinator.stageStart&lt;/code&gt;,è¿™ä¸ªç±»æ˜¯ç”¨æ¥è¾“å‡ºåˆ°hdfsä¸Šçš„ï¼Œç„¶åstageStartçš„ä¸¤ä¸ªå‚æ•°ï¼Œå°±æ˜¯ç”¨äºå‘å‡ºä¿¡æ¯ï¼Œä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯stageIdå’Œä»–è¦ç”¨äºè®¡ç®—çš„partitionæ•°ç›®ã€‚&lt;/li&gt;
  &lt;li&gt;ç„¶åå°±æ˜¯è®¡ç®—è¿™ä¸ªstageç”¨äºè®¡ç®—çš„TaskIdå¯¹åº”çš„taskæ‰€åœ¨çš„locationã€‚å› ä¸ºTaskIdå’ŒpartitionIdæ˜¯å¯¹åº”çš„ï¼Œæ‰€ä»¥ä¹Ÿå°±æ˜¯è®¡ç®—partitionIdå¯¹åº”çš„taskLocationã€‚ç„¶åtaskLocationæ˜¯ä¸€ä¸ªhostæˆ–è€…æ˜¯ä¸€ä¸ªï¼ˆhost,executorIdï¼‰äºŒå…ƒç»„ã€‚&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)&lt;/code&gt;è¿™é‡Œåˆ›å»ºæ–°çš„attempt å°±æ˜¯ä»£è¡¨è¿™ä¸ªstageæ‰§è¡Œäº†å‡ æ¬¡ã€‚å› ä¸ºstageå¯èƒ½ä¼šå¤±è´¥çš„ã€‚å¦‚æœå¤±è´¥å°±è¦æ¥ç€æ‰§è¡Œï¼Œè¿™ä¸ªattemptä»0å¼€å§‹ã€‚&lt;/li&gt;
  &lt;li&gt;ç„¶åå°±æ˜¯åˆ›å»ºå¹¿æ’­å˜é‡ï¼Œç„¶åbraocastã€‚å¹¿æ’­æ˜¯ç”¨äºexecutoræ¥è§£ætasksã€‚é¦–å…ˆè¦åºåˆ—åŒ–ï¼Œç»™æ¯ä¸ªtaskéƒ½ä¸€ä¸ªå®Œæ•´çš„rddï¼Œè¿™æ ·å¯ä»¥è®©taskç‹¬ç«‹æ€§æ›´å¼ºï¼Œè¿™å¯¹äºéçº¿ç¨‹å®‰å…¨æ˜¯æœ‰å¿…è¦çš„ã€‚å¯¹äºShuffleMapTaskæˆ‘ä»¬åºåˆ—åŒ–çš„æ•°æ®æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;(rdd,shuffleDep)&lt;/code&gt;ï¼Œå¯¹äºresultTask,åºåˆ—åŒ–æ•°æ®ä¸º&lt;code class=&quot;highlighter-rouge&quot;&gt;(rdd,func)&lt;/code&gt;ã€‚&lt;/li&gt;
  &lt;li&gt;ç„¶åæ˜¯åˆ›å»ºtasksï¼Œå½“ç„¶Tasksåˆ†ä¸ºshuffleMapTaskå’ŒresultTaskï¼Œè¿™éƒ½æ˜¯è·Ÿstageç±»å‹å¯¹åº”çš„ã€‚è¿™é‡Œåˆ›å»ºtasksï¼Œéœ€è¦ç”¨åˆ°ä¸€ä¸ªå‚æ•°&lt;code class=&quot;highlighter-rouge&quot;&gt;stage.latestInfo.attemptId&lt;/code&gt;,è¿™é‡Œæ˜¯å‰é¢æåˆ°çš„ã€‚&lt;/li&gt;
  &lt;li&gt;åˆ›å»ºå®Œtaskså°±æ˜¯åé¢çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;taskScheduler.submitTasks()&lt;/code&gt;ï¼Œè¿™æ ·ä»»åŠ¡å°±äº¤ç”±taskSchedulerè°ƒåº¦äº†ã€‚&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def submitTasks(taskSet: TaskSet) {
  val tasks = taskSet.tasks
  logInfo(&quot;Adding task set &quot; + taskSet.id + &quot; with &quot; + tasks.length + &quot; tasks&quot;)
  this.synchronized {
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    val stage = taskSet.stageId
    val stageTaskSets =
      taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
    stageTaskSets(taskSet.stageAttemptId) = manager
    val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =&amp;gt;
      ts.taskSet != taskSet &amp;amp;&amp;amp; !ts.isZombie
    }
    if (conflictingTaskSet) {
      throw new IllegalStateException(s&quot;more than one active taskSet for stage $stage:&quot; +
        s&quot; ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(&quot;,&quot;)}&quot;)
    }
    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)

    if (!isLocal &amp;amp;&amp;amp; !hasReceivedTask) {
      starvationTimer.scheduleAtFixedRate(new TimerTask() {
        override def run() {
          if (!hasLaunchedTask) {
            logWarning(&quot;Initial job has not accepted any resources; &quot; +
              &quot;check your cluster UI to ensure that workers are registered &quot; +
              &quot;and have sufficient resources&quot;)
          } else {
            this.cancel()
          }
        }
      }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
    }
    hasReceivedTask = true
  }
  backend.reviveOffers()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™æ®µä»£ç å‰é¢éƒ¨åˆ†å°±æ˜¯å…ˆåˆ›å»ºtaskManagerï¼Œç„¶ååˆ¤æ–­æ˜¯å¦æœ‰è¶…è¿‡ä¸€ä¸ªæ•°ç›®çš„taskså­˜åœ¨ï¼Œå¦‚æœå†²çªå°±æŠ¥å¼‚å¸¸ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åæŠŠè¿™ä¸ªTaskSetManageråŠ å…¥&lt;code class=&quot;highlighter-rouge&quot;&gt;schedulableBuilder&lt;/code&gt;ï¼Œè¿™ä¸ªå˜é‡åœ¨åˆå§‹åŒ–æ—¶å€™ä¼šé€‰æ‹©è°ƒåº¦ç­–ç•¥ï¼Œæ¯”å¦‚fifoå•¥çš„ï¼ŒåŠ å…¥ä¹‹åå°±ä¼šæŒ‰ç…§ç›¸åº”çš„ç­–ç•¥è¿›è¡Œè°ƒåº¦ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åä¹‹åçš„åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°ï¼Œå’Œæ˜¯å¦å·²ç»æ¥æ”¶è¿‡ä»»åŠ¡ï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;isLocal&lt;/code&gt;ä»£è¡¨æœ¬åœ°æ¨¡å¼ã€‚å¦‚æœéæœ¬åœ°æ¨¡å¼ï¼Œè€Œä¸”è¿˜æ²¡æ¥æ”¶åˆ°è¿‡ä»»åŠ¡ï¼Œå°±ä¼šå»ºç«‹ä¸€ä¸ªTimerTaskï¼Œç„¶åä¸€ç›´æŸ¥çœ‹æœ‰æ²¡æœ‰æ¥æ”¶åˆ°ä»»åŠ¡ï¼Œå› ä¸ºå¦‚æœæ²¡ä»»åŠ¡å°±æ˜¯ç©ºè½¬å—ã€‚&lt;/p&gt;

&lt;p&gt;æœ€åbackendå°±ä¼šè®©è¿™ä¸ªtaskså”¤é†’ã€‚&lt;code class=&quot;highlighter-rouge&quot;&gt;backend.reviveOffers()&lt;/code&gt;,è¿™é‡Œæˆ‘ä»¬çš„backendé€šå¸¸æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;CoarseGrainedSchedulerBackend&lt;/code&gt;ï¼Œåœ¨æ‰§è¡ŒreviveOffersä¹‹åï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;driverEndpoint&lt;/code&gt;ä¼šsendæ¶ˆæ¯ï¼Œç„¶åbackendçš„receiveå‡½æ•°ä¼šæ¥æ”¶åˆ°æ¶ˆæ¯ï¼Œç„¶åæ‰§è¡Œæ“ä½œã€‚çœ‹CoarseGrainedSchedulerBackend çš„receiveå‡½æ•°ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def receive: PartialFunction[Any, Unit] = {
...
case ReviveOffers =&amp;gt;
  makeOffers()
...
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def makeOffers() {
  // Filter out executors under killing
  val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
  val workOffers = activeExecutors.map { case (id, executorData) =&amp;gt;
    new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
  }.toSeq
  launchTasks(scheduler.resourceOffers(workOffers))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ä¸Šé¢ä»£ç æ˜¾ç¤ºç­›é€‰å‡ºå­˜æ´»çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;Executors&lt;/code&gt;ï¼Œç„¶åå°±åˆ›å»ºå‡º&lt;code class=&quot;highlighter-rouge&quot;&gt;workerOffers&lt;/code&gt;,å‚æ•°æ˜¯executorId,host,frescoers.&lt;/p&gt;

&lt;h3 id=&quot;æ‰§è¡Œtask&quot;&gt;æ‰§è¡Œtask&lt;/h3&gt;

&lt;p&gt;ç„¶åå°±launchTasksã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
  for (task &amp;lt;- tasks.flatten) {
    val serializedTask = ser.serialize(task)
    if (serializedTask.limit &amp;gt;= maxRpcMessageSize) {
      scheduler.taskIdToTaskSetManager.get(task.taskId).foreach { taskSetMgr =&amp;gt;
        try {
          var msg = &quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot; +
            &quot;spark.rpc.message.maxSize (%d bytes). Consider increasing &quot; +
            &quot;spark.rpc.message.maxSize or using broadcast variables for large values.&quot;
          msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize)
          taskSetMgr.abort(msg)
        } catch {
          case e: Exception =&amp;gt; logError(&quot;Exception in error callback&quot;, e)
        }
      }
    }
    else {
      val executorData = executorDataMap(task.executorId)
      executorData.freeCores -= scheduler.CPUS_PER_TASK

      logInfo(s&quot;Launching task ${task.taskId} on executor id: ${task.executorId} hostname: &quot; +
        s&quot;${executorData.executorHost}.&quot;)

      executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ä¸Šé¢çš„ä»£ç æ˜¾ç¤ºå°†taskåºåˆ—åŒ–ï¼Œç„¶åæ ¹æ®task.executorId ç»™ä»–åˆ†é…executorï¼Œç„¶åå°±&lt;code class=&quot;highlighter-rouge&quot;&gt;executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;è¿™é‡Œæœ‰ä¸€ä¸ª&lt;code class=&quot;highlighter-rouge&quot;&gt;executorEndPoint&lt;/code&gt;,ä¹‹å‰å‰é¢æœ‰driverEndPoint(å‡ºç°åœ¨backend.reviveOfferé‚£é‡Œ)ï¼Œè¿™ä¸¤ä¸ªç«¯å£çš„åŸºç±»éƒ½æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;RpcEndpointRef&lt;/code&gt;ã€‚RpcEndpointRefæ˜¯RpcEndPointçš„è¿œç¨‹å¼•ç”¨ï¼Œæ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚&lt;/p&gt;

&lt;p&gt;RpcEndpointæ˜¯ RPC[Remote Procedure Call ï¼šè¿œç¨‹è¿‡ç¨‹è°ƒç”¨]ä¸­å®šä¹‰äº†æ”¶åˆ°çš„æ¶ˆæ¯å°†è§¦å‘å“ªä¸ªæ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;åŒæ—¶æ¸…æ¥šçš„é˜è¿°äº†ç”Ÿå‘½å‘¨æœŸï¼Œæ„é€ -&amp;gt; onStart -&amp;gt; receive* -&amp;gt; onStop&lt;/p&gt;

&lt;p&gt;è¿™é‡Œreceive* æ˜¯æŒ‡receive å’Œ receiveAndReplyã€‚&lt;/p&gt;

&lt;p&gt;ä»–ä»¬çš„åŒºåˆ«æ˜¯ï¼š&lt;/p&gt;

&lt;p&gt;receiveæ˜¯æ— éœ€ç­‰å¾…ç­”å¤ï¼Œè€ŒreceiveAndReplyæ˜¯ä¼šé˜»å¡çº¿ç¨‹ï¼Œç›´è‡³æœ‰ç­”å¤çš„ã€‚(å‚è€ƒï¼šhttp://www.07net01.com/2016/04/1434116.html)&lt;/p&gt;

&lt;p&gt;ç„¶åè¿™é‡Œçš„driverEndPointå°±æ˜¯ä»£è¡¨è¿™ä¸ªä¿¡æ¯ä¼šå‘ç»™&lt;code class=&quot;highlighter-rouge&quot;&gt;CoarseGrainedSchedulerBackEnd&lt;/code&gt;ï¼ŒexecutorEndPointå°±æ˜¯å‘ç»™&lt;code class=&quot;highlighter-rouge&quot;&gt;coarseGrainedExecutorBackEnd&lt;/code&gt;å½“ç„¶å°±æ˜¯å‘ç»™&lt;code class=&quot;highlighter-rouge&quot;&gt;coarseGrainedExecutorBackEnd&lt;/code&gt;ã€‚æ¥ä¸‹æ¥å»çœ‹ç›¸åº”çš„recieveä»£ç ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def receive: PartialFunction[Any, Unit] = {
...

    case LaunchTask(data) =&amp;gt;
      if (executor == null) {
        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)
      } else {
        val taskDesc = ser.deserialize[TaskDescription](data.value)
        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)
        executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,
          taskDesc.name, taskDesc.serializedTask)
      }
...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œå…ˆå°†ä¼ è¿‡æ¥çš„æ•°æ®ååºåˆ—åŒ–ï¼Œç„¶å&lt;code class=&quot;highlighter-rouge&quot;&gt;executor.launchTask&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def launchTask(
    context: ExecutorBackend,
    taskId: Long,
    attemptNumber: Int,
    taskName: String,
    serializedTask: ByteBuffer): Unit = {
  val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,
    serializedTask)
  runningTasks.put(taskId, tr)
  threadPool.execute(tr)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™é‡Œæ–°å»ºäº†taskRunnerï¼Œç„¶åä¹‹åäº¤ç”±çº¿ç¨‹æ± æ¥è¿è¡Œï¼Œçº¿ç¨‹æ± æ—¢ç„¶è¦è¿è¡ŒtaskRunnerï¼Œå¿…å®šæ˜¯è¿è¡ŒtaskRunnerçš„runæ–¹æ³•ã€‚çœ‹taskRunnerçš„runæ–¹æ³•ï¼Œä»£ç å¤ªé•¿ï¼Œæ‡’å¾—è´´ï¼Œå¤§æ¦‚æè¿°ä¸‹ã€‚&lt;/p&gt;

&lt;p&gt;ä¸»è¦å°±æ˜¯è®¾ç½®å‚æ•°ï¼Œå±æ€§ï¼Œååºåˆ—åŒ–å‡ºtaskç­‰ç­‰ï¼Œä¹‹åå°±è¦è°ƒç”¨task.runTaskæ–¹æ³•ã€‚è¿™é‡Œçš„taskå¯èƒ½æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleMapTask&lt;/code&gt;ä¹Ÿå¯èƒ½æ˜¯&lt;code class=&quot;highlighter-rouge&quot;&gt;ResultTask&lt;/code&gt;ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ†åˆ«çœ‹è¿™ä¸¤ç§taskçš„runæ–¹æ³•ã€‚&lt;/p&gt;

&lt;h4 id=&quot;shufflemaptask&quot;&gt;ShuffleMapTask&lt;/h4&gt;

&lt;p&gt;å…ˆçœ‹&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleMapTask&lt;/code&gt;ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def runTask(context: TaskContext): MapStatus = {
  // Deserialize the RDD using the broadcast variable.
  val deserializeStartTime = System.currentTimeMillis()
  val ser = SparkEnv.get.closureSerializer.newInstance()
  val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
    ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
  _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime

  var writer: ShuffleWriter[Any, Any] = null
  try {
    val manager = SparkEnv.get.shuffleManager
    writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
    writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &amp;lt;: Product2[Any, Any]]])
    writer.stop(success = true).get
  } catch {
    case e: Exception =&amp;gt;
      try {
        if (writer != null) {
          writer.stop(success = false)
        }
      } catch {
        case e: Exception =&amp;gt;
          log.debug(&quot;Could not stop writer&quot;, e)
      }
      throw e
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å‰é¢éƒ¨åˆ†ä»£ç å°±æ˜¯ååºåˆ—åŒ–é‚£äº›ï¼Œä¸»è¦çœ‹ä¸­é—´çš„ä»£ç ã€‚è·å¾—shuffleManager,ç„¶ågetWriterã€‚å› ä¸ºshuffleMapTaskæœ‰Shuffleæ“ä½œï¼Œæ‰€ä»¥è¦shuffleWriteã€‚&lt;/p&gt;

&lt;h4 id=&quot;resulttask&quot;&gt;ResultTask&lt;/h4&gt;

&lt;p&gt;çœ‹ä¸‹ResultTaskçš„runTaskã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  override def runTask(context: TaskContext): U = {
    // Deserialize the RDD and the func using the broadcast variables.
    val deserializeStartTime = System.currentTimeMillis()
    val ser = SparkEnv.get.closureSerializer.newInstance()
    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&amp;gt; U)](
      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime

    func(context, rdd.iterator(partition, context))
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;è·Ÿé‚£ä¸ªå·®ä¸å¤šï¼Œåªä¸è¿‡ä¸æ˜¯shuffleWriteï¼Œæ˜¯func.&lt;/p&gt;

&lt;h4 id=&quot;rdd-è¿­ä»£é“¾&quot;&gt;rdd è¿­ä»£é“¾&lt;/h4&gt;

&lt;p&gt;çœ‹è¿™è¡Œä»£ç &lt;code class=&quot;highlighter-rouge&quot;&gt;writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &amp;lt;: Product2[Any, Any]]])&lt;/code&gt;ï¼Œçœ‹writeæ–¹æ³•é‡Œé¢çš„å‚æ•°ï¼Œrdd.iteratoræ–¹æ³•ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
  if (storageLevel != StorageLevel.NONE) {
    getOrCompute(split, context)
  } else {
    computeOrReadCheckpoint(split, context)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™ä¸ªæ–¹æ³•ï¼Œæ˜¯ä»åé¢çš„rddå¼€å§‹è¿­ä»£ï¼Œé¦–å…ˆåˆ¤æ–­è¿™ä¸ªrddæ˜¯å¦æ˜¯å·²ç»è¢«cacheã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœå·²ç»è¢«cacheï¼ŒgetOrComputeï¼Œç›´æ¥getï¼Œæˆ–è€…å¦‚æœæ²¡æ‰¾åˆ°å°±é‡ç®—ä¸€éï¼Œè¿™ä¸ªä»£ç æ¯”è¾ƒç®€å•ï¼Œæˆ‘å°±ä¸è´´äº†ã€‚&lt;/p&gt;

&lt;p&gt;å¦‚æœæ²¡æœ‰è¢«cacheï¼Œåˆ™è°ƒç”¨&lt;code class=&quot;highlighter-rouge&quot;&gt;computeOrReadCheckpoint&lt;/code&gt;ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
{
  if (isCheckpointedAndMaterialized) {
    firstParent[T].iterator(split, context)
  } else {
    compute(split, context)
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ï¼Œå…ˆä»‹ç»ä¸‹æ£€æŸ¥ç‚¹ã€‚&lt;/p&gt;

&lt;h4 id=&quot;æ£€æŸ¥ç‚¹&quot;&gt;æ£€æŸ¥ç‚¹&lt;/h4&gt;

&lt;p&gt;æ£€æŸ¥ç‚¹æœºåˆ¶çš„å®ç°å’ŒæŒä¹…åŒ–çš„å®ç°æœ‰ç€è¾ƒå¤§çš„åŒºåˆ«ã€‚æ£€æŸ¥ç‚¹å¹¶éç¬¬ä¸€æ¬¡è®¡ç®—å°±å°†ç»“æœè¿›è¡Œå­˜å‚¨ï¼Œè€Œæ˜¯ç­‰åˆ°ä¸€ä¸ªä½œä¸šç»“æŸåå¯åŠ¨ä¸“é—¨çš„ä¸€ä¸ªä½œä¸šå®Œæˆå­˜å‚¨çš„æ“ä½œã€‚&lt;/p&gt;

&lt;p&gt;checkPointæ“ä½œçš„å®ç°åœ¨RDDç±»ä¸­ï¼Œ&lt;em&gt;checkPoint&lt;/em&gt;æ–¹æ³•ä¼šå®ä¾‹åŒ–ReliableRDDCheckpointDataç”¨äºæ ‡è®°å½“å‰çš„RDD&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/**
 * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
 * directory set with `SparkContext#setCheckpointDir` and all references to its parent
 * RDDs will be removed. This function must be called before any job has been
 * executed on this RDD. It is strongly recommended that this RDD is persisted in
 * memory, otherwise saving it on a file will require recomputation.
 */
def checkpoint(): Unit = RDDCheckpointData.synchronized {
  // NOTE: we use a global lock here due to complexities downstream with ensuring
  // children RDD partitions point to the correct parent partitions. In the future
  // we should revisit this consideration.
  if (context.checkpointDir.isEmpty) {
    throw new SparkException(&quot;Checkpoint directory has not been set in the SparkContext&quot;)
  } else if (checkpointData.isEmpty) {
    checkpointData = Some(new ReliableRDDCheckpointData(this))
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;RDDCheckpointDataç±»å†…éƒ¨æœ‰ä¸€ä¸ªæšä¸¾ç±»å‹ &lt;code class=&quot;highlighter-rouge&quot;&gt;CheckpointStateÂ &lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/** 
 * Enumeration to manage state transitions of an RDD through checkpointing 
 * [ Initialized --&amp;gt; checkpointing in progress --&amp;gt; checkpointed ]. 
 */  
private[spark] object CheckpointState extends Enumeration {  
  type CheckpointState = Value  
  val Initialized, CheckpointingInProgress, Checkpointed = Value  
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ç”¨äºè¡¨ç¤ºRDDæ£€æŸ¥ç‚¹çš„å½“å‰çŠ¶æ€ï¼Œå…¶å€¼æœ‰Initialized ã€CheckpointingInProgressã€ checkpointedã€‚å…¶è½¬æ¢è¿‡ç¨‹å¦‚ä¸‹
(1)InitializedçŠ¶æ€&lt;/p&gt;

&lt;p&gt;è¯¥çŠ¶æ€æ˜¯å®ä¾‹åŒ–ReliableRDDCheckpointDataåçš„é»˜è®¤çŠ¶æ€ï¼Œç”¨äºæ ‡è®°å½“å‰çš„RDDå·²ç»å»ºç«‹äº†æ£€æŸ¥ç‚¹(è¾ƒv1.4.xå°‘ä¸€ä¸ªMarkForCheckPiontçŠ¶æ€)&lt;/p&gt;

&lt;p&gt;(2)CheckpointingInProgressçŠ¶æ€&lt;/p&gt;

&lt;p&gt;æ¯ä¸ªä½œä¸šç»“æŸåéƒ½ä¼šå¯¹ä½œä¸šçš„æœ«RDDè°ƒç”¨å…¶doCheckPointæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šé¡ºç€RDDçš„å…³ç³»ä¾èµ–é“¾å¾€å‰éå†ï¼Œç›´åˆ°é‡è§å†…éƒ¨RDDCheckpointDataå¯¹è±¡è¢«æ ‡è®°ä¸ºInitializedçš„ä¸ºæ­¢ï¼Œæ­¤æ—¶å°†RDDçš„RDDCheckpointDataå¯¹è±¡æ ‡è®°ä¸ºCheckpointingInProgressï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªä½œä¸šå®Œæˆæ•°æ®çš„å†™å…¥æ“ä½œã€‚&lt;/p&gt;

&lt;p&gt;(3)CheckpointedçŠ¶æ€&lt;/p&gt;

&lt;p&gt;æ–°å¯åŠ¨ä½œä¸šå®Œæˆæ•°æ®å†™å…¥æ“ä½œä¹‹åï¼Œå°†å»ºç«‹æ£€æŸ¥ç‚¹çš„RDDçš„æ‰€æœ‰ä¾èµ–å…¨éƒ¨æ¸…é™¤ï¼Œå°†RDDå†…éƒ¨çš„RDDCheckpointDataå¯¹è±¡æ ‡è®°ä¸ºCheckpointedï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;å°†çˆ¶RDDé‡æ–°è®¾ç½®ä¸ºä¸€ä¸ªCheckPointRDDå¯¹è±¡ï¼Œçˆ¶RDDçš„computeæ–¹æ³•ä¼šç›´æ¥ä»ç³»ç»Ÿä¸­è¯»å–æ•°æ®&lt;/code&gt;ã€‚&lt;/p&gt;

&lt;p&gt;å¦‚ä¸Šåªç®€å•åœ°ä»‹ç»äº†ç›¸å…³æ¦‚å¿µï¼Œè¯¦ç»†ä»‹ç»è¯·å‚çœ‹ï¼š&lt;a href=&quot;https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md&quot;&gt;https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;compute-é“¾&quot;&gt;compute é“¾&lt;/h3&gt;
&lt;p&gt;ä¸Šé¢æœ‰æ£€æŸ¥ç‚¹çš„å°±ç›´æ¥å»çˆ¶Rddçš„computeè¯»å–æ•°æ®äº†ã€‚è€Œéæ£€æŸ¥ç‚¹ï¼Œå°±computeï¼Œcomputeæ˜¯ä¸€ä¸ªé“¾ã€‚
æ‹¿&lt;code class=&quot;highlighter-rouge&quot;&gt;MapPartitionsRDD&lt;/code&gt;ä¸¾ä¸ªä¾‹å­ï¼Œçœ‹çœ‹å®ƒçš„computeæ–¹æ³•ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def compute(split: Partition, context: TaskContext): Iterator[U] =
f(context, split.index, firstParent[T].iterator(split, context))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;çœ‹è¿™é‡Œ computeè¿˜æ˜¯è°ƒç”¨äº†iteratorï¼Œæ‰€ä»¥è¿˜æ˜¯æ¥ç€å¾€å‰æ‰¾äº†ï¼Œç›´åˆ°æ‰¾åˆ°checkpointæˆ–è€…å°±æ˜¯åˆ°rddå¤´ã€‚&lt;/p&gt;

&lt;p&gt;å†çœ‹çœ‹å…¶ä»–çš„rddçš„computeæ–¹æ³•å§ã€‚&lt;/p&gt;

&lt;p&gt;çœ‹çœ‹&lt;code class=&quot;highlighter-rouge&quot;&gt;ShuffleRdd&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
  val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
  SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)
    .read()
    .asInstanceOf[Iterator[(K, C)]]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ç„¶åè¿™é‡ŒshuffleRddçš„computeæ–¹æ³•å°±æ˜¯ä»shuffle é‚£é‡Œread æ•°æ®ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªstageçš„å¼€å§‹äº†ã€‚&lt;/p&gt;

&lt;p&gt;å½“ç„¶ä¸€ä¸ªstageçš„å¼€å§‹æœªå¿…æ˜¯shuffleReadå¼€å§‹å•¦ï¼Œæ¯”å¦‚textFileï¼Œå®ƒæœ€ç»ˆæ˜¯è¿”å›ä¸€ä¸ªHadoopRddï¼Œç„¶åä»–çš„computeæ–¹æ³•ï¼Œå°±æ˜¯è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ã€‚&lt;/p&gt;

&lt;h2 id=&quot;å‚è€ƒ&quot;&gt;å‚è€ƒ&lt;/h2&gt;

&lt;p&gt;Â &lt;a href=&quot;http://blog.csdn.net/jiangpeng59/article/details/53213694&quot;&gt;Sparkæ ¸å¿ƒRDDï¼šè®¡ç®—å‡½æ•°compute&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Â &lt;a href=&quot;http://blog.csdn.net/jiangpeng59/article/details/53212416&quot;&gt;SparkåŸºç¡€éšç¬”ï¼šæŒä¹…åŒ–&amp;amp;æ£€æŸ¥ç‚¹&lt;/a&gt;&lt;/p&gt;

</description>
                <link>http://bbwff.github.io/spark/2016/12/22/spark%E5%BA%94%E7%94%A8%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B</link>
                <guid>http://bbwff.github.io/spark/2016/12/22/sparkåº”ç”¨æ‰§è¡Œæµç¨‹</guid>
                <pubDate>2016-12-22T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>sparkç»Ÿä¸€å†…å­˜ç®¡ç†</title>
                <description>
&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;sparkç»Ÿä¸€å†…å­˜ç®¡ç†æ˜¯spark1.6.0çš„æ–°ç‰¹æ€§ï¼Œæ˜¯å¯¹shuffle memory å’Œ storage memory è¿›è¡Œç»Ÿä¸€çš„ç®¡ç†ï¼Œæ‰“ç ´äº†ä»¥å¾€çš„å‚æ•°é™åˆ¶ã€‚&lt;/p&gt;

&lt;h2 id=&quot;éç»Ÿä¸€å†…å­˜ç®¡ç†&quot;&gt;éç»Ÿä¸€å†…å­˜ç®¡ç†&lt;/h2&gt;

&lt;p&gt;sparkåœ¨1.6 ä¹‹å‰éƒ½æ˜¯éç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œé€šè¿‡è®¾ç½®&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.shuffle.memoryFraction&lt;/code&gt; å’Œ &lt;code class=&quot;highlighter-rouge&quot;&gt;spark.storage.memoryFraction&lt;/code&gt;æ¥è®¾ç½®shuffle å’Œstorageçš„memory å¤§å°ã€‚çœ‹ä¸‹&lt;code class=&quot;highlighter-rouge&quot;&gt;StaticMemoryManager&lt;/code&gt;çš„è·å¾—æœ€å¤§shuffleå’Œstorage memoryçš„å‡½æ•°ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private def getMaxStorageMemory(conf: SparkConf): Long = {
  val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)
  val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)
  val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)
  (systemMaxMemory * memoryFraction * safetyFraction).toLong
}

/**
 * Return the total amount of memory available for the execution region, in bytes.
 */
private def getMaxExecutionMemory(conf: SparkConf): Long = {
  val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)
...
  val memoryFraction = conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2)
  val safetyFraction = conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8)
  (systemMaxMemory * memoryFraction * safetyFraction).toLong
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;å¯ä»¥çœ‹å‡ºï¼Œ&lt;code class=&quot;highlighter-rouge&quot;&gt;systemMaxMemory&lt;/code&gt;æ˜¯é€šè¿‡å‚æ•°&lt;code class=&quot;highlighter-rouge&quot;&gt;spark.testing.memory&lt;/code&gt;æ¥è·å¾—ï¼Œå¦‚æœè¿™ä¸ªå‚æ•°æ²¡æœ‰è®¾ç½®ï¼Œå°±å–è™šæ‹Ÿæœºå†…å­˜ï¼Œç„¶åshuffle å’Œ storageéƒ½æœ‰å®‰å…¨ç³»æ•°ï¼Œæœ€åå¯ç”¨çš„æœ€å¤§å†…å­˜éƒ½æ˜¯ï¼šç³»ç»Ÿæœ€å¤§å†…å­˜*æ¯”ä¾‹ç³»æ•°*å®‰å…¨ç³»æ•°ã€‚&lt;/p&gt;

&lt;h2 id=&quot;ç»Ÿä¸€å†…å­˜ç®¡ç†&quot;&gt;ç»Ÿä¸€å†…å­˜ç®¡ç†&lt;/h2&gt;

&lt;p&gt;spark 1.6.0 å‡ºç°äº†ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œæ˜¯æ‰“ç ´äº†shuffle å†…å­˜å’Œstorageå†…å­˜çš„é™æ€é™åˆ¶ã€‚é€šä¿—çš„æè¿°ï¼Œå°±æ˜¯å¦‚æœstorageå†…å­˜ä¸å¤Ÿï¼Œè€Œshuffleå†…å­˜å‰©ä½™å°±èƒ½å€Ÿå†…å­˜ï¼Œå¦‚æœshuffleå†…å­˜ä¸è¶³ï¼Œæ­¤æ—¶å¦‚æœstorageå·²ç»è¶…å‡ºäº†&lt;code class=&quot;highlighter-rouge&quot;&gt;storageRegionSize&lt;/code&gt;ï¼Œé‚£ä¹ˆå°±é©±é€å½“å‰ä½¿ç”¨storageå†…å­˜-&lt;code class=&quot;highlighter-rouge&quot;&gt;storageRegionSize&lt;/code&gt;ï¼Œå¦‚æœstorage ä½¿ç”¨æ²¡æœ‰è¶…è¿‡&lt;code class=&quot;highlighter-rouge&quot;&gt;storageRegionSize&lt;/code&gt;ï¼Œé‚£ä¹ˆåˆ™æŠŠå®ƒå‰©ä½™çš„éƒ½å¯ä»¥å€Ÿç»™shuffleä½¿ç”¨ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  private def getMaxMemory(conf: SparkConf): Long = {
    val systemMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)
    val reservedMemory = conf.getLong(&quot;spark.testing.reservedMemory&quot;,
      if (conf.contains(&quot;spark.testing&quot;)) 0 else RESERVED_SYSTEM_MEMORY_BYTES)
    val minSystemMemory = (reservedMemory * 1.5).ceil.toLong
    if (systemMemory &amp;lt; minSystemMemory) {
      throw new IllegalArgumentException(s&quot;System memory $systemMemory must &quot; +
        s&quot;be at least $minSystemMemory. Please increase heap size using the --driver-memory &quot; +
        s&quot;option or spark.driver.memory in Spark configuration.&quot;)
    }
    // SPARK-12759 Check executor memory to fail fast if memory is insufficient
    if (conf.contains(&quot;spark.executor.memory&quot;)) {
      val executorMemory = conf.getSizeAsBytes(&quot;spark.executor.memory&quot;)
      if (executorMemory &amp;lt; minSystemMemory) {
        throw new IllegalArgumentException(s&quot;Executor memory $executorMemory must be at least &quot; +
          s&quot;$minSystemMemory. Please increase executor memory using the &quot; +
          s&quot;--executor-memory option or spark.executor.memory in Spark configuration.&quot;)
      }
    }
    val usableMemory = systemMemory - reservedMemory
    val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.6)
    (usableMemory * memoryFraction).toLong
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;è¿™ä¸ªæ˜¯ç»Ÿä¸€å†…å­˜ç®¡ç†çš„è·å¾—æœ€å¤§å†…å­˜çš„å‡½æ•°ï¼Œå› ä¸ºshuffleå’Œstorageæ˜¯ç»Ÿä¸€ç®¡ç†çš„ï¼Œæ‰€ä»¥åªæœ‰ä¸€ä¸ªè·å¾—ç»Ÿä¸€æœ€å¤§å†…å­˜çš„å‡½æ•°ã€‚&lt;code class=&quot;highlighter-rouge&quot;&gt;usableMemory = systemMemory - reservedMemory&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;æœ€å¤§å†…å­˜=&lt;code class=&quot;highlighter-rouge&quot;&gt;usableMemory * memoryFraction&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ç»Ÿä¸€å†…å­˜ç®¡ç†çš„ä½¿ç”¨&quot;&gt;ç»Ÿä¸€å†…å­˜ç®¡ç†çš„ä½¿ç”¨&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;UnifiedMemoryManager&lt;/code&gt;æ˜¯åœ¨ä¸€ä¸ªé™æ€ç±»é‡Œé¢çš„&lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt;æ–¹æ³•è°ƒç”¨çš„ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = {
  val maxMemory = getMaxMemory(conf)
  new UnifiedMemoryManager(
    conf,
    maxHeapMemory = maxMemory,
    onHeapStorageRegionSize =
      (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,
    numCores = numCores)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ç„¶åé€šè¿‡ find Uages æ‰¾åˆ°æ˜¯åœ¨ &lt;code class=&quot;highlighter-rouge&quot;&gt;sparkEnv&lt;/code&gt;é‡Œé¢è°ƒç”¨ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;val memoryManager: MemoryManager =
  if (useLegacyMemoryManager) {
    new StaticMemoryManager(conf, numUsableCores)
  } else {
    UnifiedMemoryManager(conf, numUsableCores)
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;æ˜¯é€šè¿‡åˆ¤æ–­å‚æ•°ï¼Œåˆ¤æ–­æ˜¯ä½¿ç”¨ç»Ÿä¸€å†…å­˜ç®¡ç†è¿˜æ˜¯éå†…å­˜ç®¡ç†ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åé€šè¿‡æŸ¥çœ‹usages å‘ç°æ˜¯åœ¨ &lt;code class=&quot;highlighter-rouge&quot;&gt;CoarseGrainedExecutorBackEnd&lt;/code&gt; å’Œ &lt;code class=&quot;highlighter-rouge&quot;&gt;MesosExecutorBackEnd&lt;/code&gt;é‡Œé¢è°ƒç”¨çš„ï¼Œæ‰€ä»¥æ˜¯æ¯ä¸ªexecutoréƒ½æœ‰ä¸€ä¸ªç»Ÿä¸€å†…å­˜ç®¡ç†çš„å®ä¾‹(â€¦å¾ˆæ˜¾ç„¶ï¼Œé€»è¾‘ä¹Ÿæ˜¯è¿™æ ·)ã€‚&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/spark/2016/12/19/spark%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86</link>
                <guid>http://bbwff.github.io/spark/2016/12/19/sparkç»Ÿä¸€å†…å­˜ç®¡ç†</guid>
                <pubDate>2016-12-19T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>ç†è§£åŠ¨æ€è§„åˆ’</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æœ€è¿‘åœ¨çœ‹åŠ¨æ€è§„åˆ’ï¼ŒæŒºéš¾ç†è§£çš„ï¼Œæ…¢æ…¢çœ‹ï¼ŒæŠŠå¿ƒå¾—è®°å½•ä¸‹ã€‚&lt;/p&gt;

&lt;p&gt;åˆ†æ²»æ³•æ˜¯é€šè¿‡ç»„åˆå­é—®é¢˜çš„è§£æ¥è§£å†³æ•´ä¸ªé—®é¢˜ï¼Œä½†å®ƒçš„å­é—®é¢˜æ˜¯ç‹¬ç«‹çš„ã€‚è€ŒåŠ¨æ€è§„åˆ’ä¹Ÿæ˜¯æŠŠé—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œä½†å­é—®é¢˜ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œä¹Ÿå°±æ˜¯å„å­é—®é¢˜åŒ…å«å…¬å…±çš„å­å­é—®é¢˜ï¼Œå¦‚æœä½¿ç”¨åˆ†æ²»æ³•å°±ä¼šé‡å¤çš„æ±‚è§£å­å­é—®é¢˜ã€‚åŠ¨æ€è§„åˆ’ç®—æ³•å¯¹æ¯ä¸ªå­å­é—®é¢˜åªæ±‚è§£ä¸€æ¬¡ï¼Œå°†å…¶ç»“æœä¿å­˜åœ¨ä¸€å¼ è¡¨ä¸­ï¼Œä»è€Œé¿å…æ¯æ¬¡é‡åˆ°å„å­é—®é¢˜æ—¶é‡æ–°è®¡ç®—ã€‚åŠ¨æ€è§„åˆ’é€šå¸¸ç”¨äº&lt;code class=&quot;highlighter-rouge&quot;&gt;æœ€ä¼˜åŒ–é—®é¢˜&lt;/code&gt;,æ­¤ç±»é—®é¢˜æœ‰&lt;/p&gt;

&lt;h2 id=&quot;å‚è€ƒèµ„æ–™&quot;&gt;å‚è€ƒèµ„æ–™&lt;/h2&gt;

&lt;p&gt;http://blog.csdn.net/pi9nc/article/details/8142876&lt;/p&gt;

</description>
                <link>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/14/%E7%90%86%E8%A7%A3%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92</link>
                <guid>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/14/ç†è§£åŠ¨æ€è§„åˆ’</guid>
                <pubDate>2016-12-14T00:29:16+08:00</pubDate>
        </item>

        <item>
                <title>åŒºé—´å†…1å‡ºç°æ¬¡æ•°</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æ±‚å‡º1~13çš„æ•´æ•°ä¸­1å‡ºç°çš„æ¬¡æ•°,å¹¶ç®—å‡º100~1300çš„æ•´æ•°ä¸­1å‡ºç°çš„æ¬¡æ•°ï¼Ÿä¸ºæ­¤ä»–ç‰¹åˆ«æ•°äº†ä¸€ä¸‹1~13ä¸­åŒ…å«1çš„æ•°å­—æœ‰1ã€10ã€11ã€12ã€13å› æ­¤å…±å‡ºç°6æ¬¡,ä½†æ˜¯åŒºé—´å¤§ä¸€ç‚¹æ€ä¹ˆæ±‚å‘¢ï¼Ÿ&lt;/p&gt;

&lt;h1 id=&quot;é¢˜ç›®æè¿°&quot;&gt;é¢˜ç›®æè¿°&lt;/h1&gt;

&lt;p&gt;æ±‚å‡º1~13çš„æ•´æ•°ä¸­1å‡ºç°çš„æ¬¡æ•°,å¹¶ç®—å‡º100~1300çš„æ•´æ•°ä¸­1å‡ºç°çš„æ¬¡æ•°ï¼Ÿä¸ºæ­¤ä»–ç‰¹åˆ«æ•°äº†ä¸€ä¸‹1~13ä¸­åŒ…å«1çš„æ•°å­—æœ‰1ã€10ã€11ã€12ã€13å› æ­¤å…±å‡ºç°6æ¬¡,ä½†æ˜¯å¯¹äºåé¢é—®é¢˜ä»–å°±æ²¡è¾™äº†ã€‚ACMerå¸Œæœ›ä½ ä»¬å¸®å¸®ä»–,å¹¶æŠŠé—®é¢˜æ›´åŠ æ™®éåŒ–,å¯ä»¥å¾ˆå¿«çš„æ±‚å‡ºä»»æ„éè´Ÿæ•´æ•°åŒºé—´ä¸­1å‡ºç°çš„æ¬¡æ•°ã€‚
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;è§£é¢˜æ€è·¯&quot;&gt;è§£é¢˜æ€è·¯&lt;/h1&gt;

&lt;p&gt;åˆšå¼€å§‹æ²¡ç†è§£æ¸…æ¥šé¢˜ç›®çš„æ„æ€ï¼Œç†è§£é¢˜ç›®å¾ˆå…³é”®ã€‚&lt;/p&gt;

&lt;p&gt;é¢˜ç›®æè¿°é‡Œé¢è¯´1-13 åŒºé—´é‡Œé¢åŒ…å«1 çš„æœ‰ 1ï¼Œ10 ï¼Œ11ï¼Œ12ï¼Œ13è¿™äº”ä¸ªæ•°å­—ï¼Œä½†æ˜¯1å‡ºç°äº†6æ¬¡ï¼Œå› ä¸º11ä¸­æœ‰ä¸¤ä¸ª1ã€‚çœ‹æ¥æ˜¯æˆ‘ç†è§£é”™äº†ï¼Œæˆ‘ä¹‹å‰ä»¥ä¸ºæ˜¯æ±‚è¿™ä¸ªåŒºé—´é‡Œé¢åŒ…å«1çš„æ•°å­—çš„ä¸ªæ•°ï¼Œé‚£è¿™æ ·çš„è¯ï¼Œ1-13é‡Œé¢åªæœ‰äº”ä¸ªäº†ã€‚&lt;/p&gt;

&lt;p&gt;æ‰€ä»¥æˆ‘åˆšå¼€å§‹çœ‹åˆ°ç½‘ä¸Šè®²è§£æ—¶ï¼Œçœ‹åˆ°å®ƒæ˜¯åˆ†åˆ«æ±‚ä¸ªä½ï¼Œåä½ï¼Œç™¾ä½ç­‰ä¸Šé¢ä¸º1çš„å¯èƒ½æ¬¡æ•°ç›¸åŠ ä¹‹å’Œï¼Œå½“æ—¶å°±æƒ³ä¼šä¸ä¼šæœ‰é‡å¤çš„ï¼Œç°åœ¨æƒ³æƒ³ï¼Œè¿™æ ·åˆšå¥½ä¸é‡å¤ï¼Œæ¯”å¦‚åœ¨1-13é‡Œé¢ï¼Œä¸ªä½ä¸º1 å’Œåä½ä¸º1 éƒ½æœ‰11ï¼Œä½†æ˜¯è¿™æ ·ç›¸åŠ å¹¶æ²¡é‡å¤çš„ã€‚&lt;/p&gt;

&lt;p&gt;é‚£ä¹ˆç°åœ¨çš„é—®é¢˜å°±æ˜¯æ±‚ä¸€ä¸ªåŒºé—´ï¼Œä¸Šé¢æ¯ä¸ªä½ä¸Šé¢ä¸º1å¯èƒ½å‡ºç°çš„æ¬¡æ•°ã€‚&lt;/p&gt;

&lt;p&gt;ä¸¾ä¸ªä¾‹å­å§ï¼Œæˆ‘ä»¬çœ‹åä½ä¸Šé¢ä¸º1çš„ä¸ªæ•°ã€‚&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;1118è¿™ä¸ªæ•°å­—ï¼Œå› ä¸ºåä½å°±æ˜¯1ï¼Œæ‰€ä»¥ï¼Œé«˜ä½æ˜¯ä»»æ„æ•°å­—ï¼Œè¿˜æœ‰åœ°ä½æ˜¯ä»»æ„æ•°å­—ä»–éƒ½æœ‰1ï¼Œé«˜ä½æ˜¯11ï¼Œåœ°ä½æ˜¯8ï¼Œ å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªåŒºé—´ï¼Œåœ¨0-1110è¿™ä¸ªåŒºé—´ï¼Œå‰é¢çš„é«˜ä½å¯ä»¥æ˜¯0-10 ä»»æ„æ•°å­—å’Œåé¢åœ°ä½çš„0-9ä»»æ„æ•°å­—ï¼Œæˆ–è€…å‰é¢é«˜ä½æ˜¯11 ç„¶ååé¢ä½ä½ä¸º0 ï¼Œè€Œåœ¨1111-1118è¿™ä¸ªåŒºé—´ï¼Œè¿˜æœ‰8ä¸ªè®©åä½ä¸º1çš„ï¼Œæ‰€ä»¥æ¬¡æ•°å°±æ˜¯11*10+1+8.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;1128è¿™ä¸ªæ•°å­—ï¼Œåä½æ˜¯2ï¼Œæ‰€ä»¥è¿™ä¸ªåŒºé—´å¦‚æœè®©åä½ä¸º1ï¼Œåªèƒ½ç®—åˆ°0-1119è¿™ä¸ªåŒºé—´é‡Œé¢åä½ä¸º1çš„ä¸ªæ•°ã€‚ä¹Ÿå°±æ˜¯é«˜ä½æ˜¯0-11éšæ„ï¼Œä½ä½æ—¶0-9éšæ„ã€‚æ‰€ä»¥è¿™æ ·æƒ…å†µå‡ºç°æ¬¡æ•°å°±æ˜¯ï¼ˆ11+1ï¼‰*10=120.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;1108è¿™ä¸ªæ•°å­—ï¼Œåä½æ˜¯0ï¼Œæ‰€ä»¥å¦‚æœåä½ä¸º1ï¼Œåªèƒ½ç®—åˆ°0-1019è¿™ä¸ªåœ°æ–¹ã€‚æ¬¡æ•°åº”è¯¥æ˜¯11*10=110.&lt;/p&gt;

    &lt;p&gt;æ‰€ä»¥åº”è¯¥å¯ä»¥çœ‹å‡ºæ¥äº†ï¼Œå½“å‰ä½ä¸º1ï¼Œåˆ™ï¼Œé«˜ä½*å½“å‰ä½æƒé‡+ä½ä½+1ã€‚&lt;/p&gt;

    &lt;p&gt;å½“å‰ä½å¤§äº1ï¼Œåˆ™ï¼ˆé«˜ä½+1ï¼‰*å½“å‰ä½æƒé‡ã€‚&lt;/p&gt;

    &lt;p&gt;å½“å‰ä½å°äº1ï¼Œåˆ™é«˜ä½*å½“å‰ä½æƒé‡ã€‚&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ä»£ç å¦‚ä¸‹ï¼š&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JAVA&quot;&gt;public int NumberOf1Between1AndN_Solution(int n) {

    int i=1;
    int total=0;

    int hight;
    do{
         hight=n/(int)Math.pow(10,i);
        int temp=n%(int)Math.pow(10,i);
        int curr=temp/(int)Math.pow(10,i-1);
        int low=temp%(int)Math.pow(10,i-1);
        if (curr==1){
            total+=hight*(int)Math.pow(10,i-1)+low+1;

        }
        else if(curr&amp;lt;1){
            total+=hight*(int)Math.pow(10,i-1);
        }
        else if(curr&amp;gt;1){
            total+=(hight+1)*(int)Math.pow(10,i-1);

        }
        i++;
    }
    while (hight!=0);
    return total;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;æ‰©å±•&quot;&gt;æ‰©å±•&lt;/h1&gt;

&lt;p&gt;ä»¥ä¸Šæ€è·¯å¯ä»¥æ‰©å±•åˆ°åŒºé—´å†…xå‡ºç°çš„æ¬¡æ•°ã€‚&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/13/%E5%8C%BA%E9%97%B4%E5%86%851%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0</link>
                <guid>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/13/åŒºé—´å†…1å‡ºç°æ¬¡æ•°</guid>
                <pubDate>2016-12-13T04:55:43+08:00</pubDate>
        </item>

        <item>
                <title>å­—ç¬¦ä¸²å…¨æ’åˆ—</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æœ€è¿‘åœ¨åšä¸€äº›ç‰›å®¢ç½‘ä¸Šçš„é¢˜ï¼Œé‡åˆ°ä¸€é“é¢˜ï¼Œæ˜¯å¯¹ä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‰å­—å…¸åºè¾“å‡ºå®ƒçš„å…¨æ’åˆ—ã€‚&lt;/p&gt;

&lt;h2 id=&quot;é¢˜ç›®æè¿°&quot;&gt;é¢˜ç›®æè¿°&lt;/h2&gt;

&lt;p&gt;è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²,æŒ‰å­—å…¸åºæ‰“å°å‡ºè¯¥å­—ç¬¦ä¸²ä¸­å­—ç¬¦çš„æ‰€æœ‰æ’åˆ—ã€‚ä¾‹å¦‚è¾“å…¥å­—ç¬¦ä¸²abc,åˆ™æ‰“å°å‡ºç”±å­—ç¬¦a,b,cæ‰€èƒ½æ’åˆ—å‡ºæ¥çš„æ‰€æœ‰å­—ç¬¦ä¸²abc,acb,bac,bca,cabå’Œcbaã€‚Â 
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id=&quot;è§£é¢˜è¿‡ç¨‹&quot;&gt;è§£é¢˜è¿‡ç¨‹&lt;/h2&gt;
&lt;p&gt;ä¸€å¼€å§‹å—æ•°å­¦çš„æ€æƒ³å½±å“ä¸¥é‡ï¼Œæ€»æ˜¯è§‰å¾—å…¨æ’åˆ—æ˜¯å…ˆä»nä¸ªå­—ç¬¦é‡Œé¢éšæœºå–å‡ºä¸€ä¸ªï¼Œç„¶åå†ä»n-1é‡Œé¢éšæœºå–å‡ºä¸€ä¸ªå­—ç¬¦ï¼Œç›´åˆ°å–å®Œä¸ºæ­¢ã€‚ä½†æ˜¯è¿™æ ·ç›´æ¥åˆ†æ”¯æ¥å†™ï¼Œå¾ˆéš¾å†™å‡ºæ¥çš„ã€‚ 
å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹è½¬ä¸ºé€’å½’çš„è¿‡ç¨‹ã€‚åƒæ•°å­¦å½’çº³æ³•ä¸€æ ·ï¼Œä¸€æ­¥ä¸€æ­¥åŒ–ç¹ä¸ºç®€ã€‚
æ•°å­¦å½’çº³æ³•çš„æ€æƒ³å¦‚ä¸‹ï¼š
ä¸€èˆ¬åœ°ï¼Œè¯æ˜ä¸€ä¸ªä¸è‡ªç„¶æ•°næœ‰å…³çš„å‘½é¢˜P(nï¼‰ï¼Œæœ‰å¦‚ä¸‹æ­¥éª¤ï¼š
ï¼ˆ1ï¼‰è¯æ˜å½“nå–ç¬¬ä¸€ä¸ªå€¼n0æ—¶å‘½é¢˜æˆç«‹ã€‚n0å¯¹äºä¸€èˆ¬æ•°åˆ—å–å€¼ä¸º0æˆ–1ï¼Œä½†ä¹Ÿæœ‰ç‰¹æ®Šæƒ…å†µï¼›
ï¼ˆ2ï¼‰å‡è®¾å½“n=kï¼ˆkâ‰¥n0ï¼Œkä¸ºè‡ªç„¶æ•°ï¼‰æ—¶å‘½é¢˜æˆç«‹ï¼Œè¯æ˜å½“n=k+1æ—¶å‘½é¢˜ä¹Ÿæˆç«‹ã€‚
ç»¼åˆï¼ˆ1ï¼‰ï¼ˆ2ï¼‰ï¼Œå¯¹ä¸€åˆ‡è‡ªç„¶æ•°nï¼ˆâ‰¥n0ï¼‰ï¼Œå‘½é¢˜P(nï¼‰éƒ½æˆç«‹ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åè¿ç”¨åˆ°æ­¤å¤„ï¼Œ
åˆå§‹æ¡ä»¶ï¼šå½“åªæœ‰ä¸€ä¸ªå­—ç¬¦çš„æ—¶å€™ï¼Œä»–çš„å…¨æ’åˆ—å°±æ˜¯ä»–æœ¬èº«ã€‚
1ï¼‰å½“æœ‰ä¸¤ä¸ªå­—ç¬¦çš„æ—¶å€™ï¼Œä»–çš„å…¨æ’åˆ—å°±æ˜¯æ‰€æœ‰å­—ç¬¦å’Œç¬¬ä¸€ä¸ªå­—ç¬¦äº¤æ¢ä¹‹åçš„ç¬¬ä¸€ä¸ªå­—ç¬¦åŠ åé¢ä¸€ä¸ªå­—ç¬¦çš„å…¨æ’åˆ—ã€‚
2ï¼‰å½“æœ‰K+1å­—ç¬¦çš„æ—¶å€™ï¼Œä»–çš„å…¨æ’åˆ—å°±æ˜¯æ‰€æœ‰å­—ç¬¦åˆ†åˆ«ä¸ç¬¬ä¸€ä¸ªå­—ç¬¦äº¤æ¢ä¹‹åçš„ç¬¬ä¸€ä¸ªå­—ç¬¦åŠ åé¢kä¸ªå­—ç¬¦çš„å…¨æ’åˆ—ã€‚
ç»¼åˆ (1) (2)ï¼Œå¯¹ä¸€åˆ‡è‡ªç„¶æ•°n(n&amp;gt;0),è¿™ä¸ªå‘½é¢˜éƒ½æˆç«‹ã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åæ•°å­¦å½’çº³æ³•ï¼Œä¸€èˆ¬æ˜¯ç”±åˆå§‹æ¡ä»¶åˆ°ä¸€èˆ¬æ¡ä»¶çš„å½’çº³ï¼Œè·Ÿé€’å½’çš„è¿‡ç¨‹åˆšå¥½ç›¸åï¼Œä½†åªè¦æˆ‘ä»¬å½’çº³å‡ºäº†è¿™ä¸ªè§„å¾‹ï¼Œå†™é€’å½’å°±ç®€å•äº†ã€‚&lt;/p&gt;

&lt;p&gt;è®¾ä¸€ç»„æ•°p = {r1, r2, r3, â€¦ ,rn}, å…¨æ’åˆ—ä¸ºperm(p)ï¼Œpn = p - {rn}ã€‚
å› æ­¤perm(p) = r1perm(p1), r2perm(p2), r3perm(p3), â€¦ , rnperm(pn)ã€‚å½“n = 1æ—¶perm(p} = r1ã€‚
å…¶å®å°±æ˜¯è·Ÿå½’çº³ä¸€ä¸ªåçš„è¿‡ç¨‹ã€‚ä»£ç å¦‚ä¸‹&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JAVA&quot;&gt;import java.util.*;

/**
 * Created by bbw on 16/12/11.
 */
public class Permutation {
    TreeSet&amp;lt;String&amp;gt; treeSet=new TreeSet&amp;lt;String&amp;gt;();
    public  void  swap(char[] str,int a,int b){
        char c=str[a];
        str[a]=str[b];
        str[b]=c;
    }
 public void perm(char[] str, int a,int b){
        if (a==b){
            String t=&quot;&quot;;
            for (int i=0;i&amp;lt;=b;i++){
                t=t+str[i];

            }
            treeSet.add(t);
        }
        else {
            for (int i=a;i&amp;lt;=b;i++){
                swap(str,i,a);
                perm(str,a+1,b);
                swap(str,i,a);
            }

        }
    }
    public ArrayList&amp;lt;String&amp;gt; Permutation(String str) {
        perm(str.toCharArray(),0,str.length()-1);
        ArrayList&amp;lt;String&amp;gt; r=new ArrayList&amp;lt;String&amp;gt;();
        for (String s:treeSet)
            r.add(s);
        return r;

    }
}
&lt;/code&gt;&lt;/pre&gt;

</description>
                <link>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/12/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%85%A8%E6%8E%92%E5%88%97-JAVA</link>
                <guid>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/12/12/å­—ç¬¦ä¸²å…¨æ’åˆ—-JAVA</guid>
                <pubDate>2016-12-12T17:39:00+08:00</pubDate>
        </item>

        <item>
                <title>ç®—æ³•å¯¼è®ºå­¦ä¹ ç¬”è®°</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æœ€è¿‘åœ¨çœ‹ç®—æ³•å¯¼è®ºï¼Œæƒ³è®°å½•ä¸€ä¸ªè¯»ä¹¦ç¬”è®°ä¹‹ç±»çš„ä¸œè¥¿è‡ªå·±çœ‹ã€‚å°±æ˜¯ä¼šè®°è½½çš„å¾ˆä¹±ï¼Œå†™çš„å¾ˆéšæ„ï¼Œè‡ªå·±çœ‹ï¼Œåæ­£è¿™ä¸ªåšå®¢ä¹Ÿä¸ä¼šå…¬å¼€ï¼Œhhhhã€‚&lt;/p&gt;

&lt;h2 id=&quot;ç¬¬äºŒç« -ç®—æ³•å…¥é—¨&quot;&gt;ç¬¬äºŒç« -ç®—æ³•å…¥é—¨&lt;/h2&gt;

&lt;p&gt;ç®—æ³•è¿™ç§ä¸œè¥¿ï¼Œæœ‰äº›æ€æƒ³æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œæœ€å¥½æ˜¯æŠŠç®—æ³•å†™æˆä»£ç å®ç°ã€‚&lt;/p&gt;

&lt;p&gt;ç¬¬äºŒç« åœ¨è®²ç®—æ³•è®¾è®¡è¿™é‡Œï¼Œè®²äº†ä¸€ä¸ªåˆ†æ²»æ³•ï¼Œå°±æ˜¯æŠŠå¤§åŒ–å°ï¼Œä¹Ÿå°±æ˜¯é€’å½’é‚£ç§ï¼Œæœ€ååœ¨åˆå¹¶ï¼Œåœ¨æ’åºé‡Œé¢å°±æ˜¯é€’å½’å½’å¹¶æ’åºï¼Œä»£ç é‡Œé¢æœ‰ä¸€ä¸ªå°ç»†èŠ‚ï¼Œå°±æ˜¯å“¨å…µã€‚åœ¨å½’å¹¶æ’åºé‡Œé¢å“¨å…µå°±æ˜¯ä¼šæ”¾åœ¨è¦å½’å¹¶çš„æ•°ç»„çš„æœ€åä½ç½®ï¼Œä¸€èˆ¬æ˜¯ä¸€ä¸ªæ— ç©·å¤§ï¼Œå¦‚æœæ˜¯å‡åºï¼Œå°±è®¾ç½®æ­£æ— ç©·ï¼Œé™åºå°±è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼Œè¿™æ ·çš„è¯ï¼Œå°±ä¸ç”¨åˆ¤æ–­å½’å¹¶æ—¶å€™ä¸¤ä¸ªæ•°ç»„åˆ°åº•æ˜¯ä¸æ˜¯éç©ºäº†ã€‚ä»£ç æ¯”è¾ƒç®€æ´ï¼Œç®—æ˜¯ä»£ç å®ç°çš„å°æŠ€å·§äº†ã€‚&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;ä¸‹é¢æ˜¯æˆ‘å†™çš„åˆ†æ²»æ³•æ’åºçš„ä»£ç ï¼Œæ˜¯é€’å½’å½’å¹¶æ’åºã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;package Introduction2Algorithms.chapter2;
import java.io.Console;
/**
 * Created by bbw on 2016/11/28.
 */
//é‡‡ç”¨é€’å½’ä¹Ÿå°±æ˜¯åˆ†æ²»æ³•
public class mergeSort {
 static    void merge(int [] a,int p,int q, int r){
        int l1=q-p+1;
        int l2=r-q;
        int[]a1=new int[l1+1];
        int[]a2=new int[l2+1];
        for (int i=0;i&amp;lt;l1;i++){
            a1[i]=a[p+i];
        }
        //æ­¤å¤„æ˜¯ä¸€ä¸ªå“¨å…µï¼Œæ— ç©·å¤§çš„
        a1[l1]=Integer.MAX_VALUE;
        for (int i=0;i&amp;lt;l2;i++){
            a2[i]=a[q+1+i];
        }
        //æ­¤å¤„æ˜¯ä¸€ä¸ªå“¨å…µï¼Œæ— ç©·å¤§çš„
        a2[l2]=Integer.MAX_VALUE;

        int i1=0;
        int i2=0;

//æ­¤å¤„å‡ºç°è¿‡bugï¼Œå°±æ˜¯æˆ‘æŠŠiä»0å¼€å§‹äº†ï¼Œåº”è¯¥æ˜¯ä»på¼€å§‹
        for (int i=p;i&amp;lt;=r;i++){
            if (a1[i1]&amp;lt;a2[i2]) {
                a[i] = a1[i1];
                i1++;
            }
            else {
                a[i] = a2[i2];
                i2++;
            }
        }
    }

    static void mergeSort(int[]a,int p,int q){

        if (p&amp;lt;q){
            int t=(p+q)/2;
            mergeSort(a,p,t);
            mergeSort(a,t+1,q);
            merge(a,p,t,q);
        }
    }

    public static void main(String[] args)
    {
        int[] x = { 6, 2, 4, 1, 5, 9 };
        mergeSort(x,0,x.length-1);
        for (int i:x){
            System.out.println(i);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;è¿™ä¸ªä»£ç æŒºç®€å•çš„ï¼Œä½†æ˜¯æˆ‘å±…ç„¶ä¸­é—´å‡ºäº†bugï¼Œå°±æ˜¯æ•°ç»„çš„è¾¹ç•Œæ²¡æå¥½ï¼Œå¯¼è‡´æ•°æ®æ··ä¹±ã€‚è¿™é‡Œé¢çš„å“¨å…µæ¯”è¾ƒæ–¹ä¾¿ï¼Œä½†å°±åªæ˜¯æµªè´¹äº†ä¸€ä¸ªç©ºé—´ï¼Œä»£ä»·ä¸å¤§ï¼Œä»£ç æ¯”è¾ƒç®€æ´ã€‚&lt;/p&gt;

&lt;p&gt;å†è¯´ä¸‹æ€ä¹ˆç®—æ—¶é—´å¤æ‚åº¦ï¼Œå› ä¸ºæ¯ä¸ªmergeSortéƒ½æ˜¯æ‹†æˆä¸¤ä¸ªä¸€åŠæ•°é‡çº§çš„mergeSortåŠ ä¸€ä¸ªmergeã€‚mergeçš„æ—¶é—´å¤æ‚åº¦æ˜¯Cn.&lt;/p&gt;

&lt;p&gt;æ‰€ä»¥T(n)=2T(n/2)+cnã€‚&lt;/p&gt;

&lt;p&gt;ç„¶åä¸€èˆ¬è¿™ç§æ—¶é—´å¤æ‚åº¦å°±æ˜¯æŒ‰ç…§é€’å½’æ‹†å¼€ï¼Œå¯ä»¥æ‹†æˆlogNå±‚ã€‚ç„¶åæ¯ä¸€å±‚éƒ½æœ‰ä¸€ä¸ªcnï¼Œæ‹†logNæ¬¡å°±æœ‰ CnLogNï¼Œç„¶å  é‚£ä¸ª logN.T(1)å°±è¢«èˆå»ï¼Œæ—¶é—´å¤æ‚åº¦å°±æ˜¯O(nlogN)ã€‚å…·ä½“ç±»ä¼¼é€’å½’æ—¶é—´å¤æ‚åº¦è®¡ç®—æ–¹æ³•çœ‹&lt;a href=&quot;http://blog.csdn.net/xiaoxian8023/article/details/8134260&quot;&gt;è¿™ä¸ªç½‘é¡µ&lt;/a&gt;ã€‚&lt;/p&gt;

&lt;p&gt;ä¸‹é¢æ˜¯ä¸€é“åˆ©ç”¨å½’å¹¶æ’åºæ±‚é€†åºå¯¹çš„é¢˜ç›®ã€‚&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;é¢˜ç›®æè¿°&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;åœ¨æ•°ç»„ä¸­çš„ä¸¤ä¸ªæ•°å­—ï¼Œå¦‚æœå‰é¢ä¸€ä¸ªæ•°å­—å¤§äºåé¢çš„æ•°å­—ï¼Œåˆ™è¿™ä¸¤ä¸ªæ•°å­—ç»„æˆä¸€ä¸ªé€†åºå¯¹ã€‚è¾“å…¥ä¸€ä¸ªæ•°ç»„,æ±‚å‡ºè¿™ä¸ªæ•°ç»„ä¸­çš„é€†åºå¯¹çš„æ€»æ•°Pã€‚å¹¶å°†På¯¹1000000007å–æ¨¡çš„ç»“æœè¾“å‡ºã€‚ å³è¾“å‡ºP%1000000007Â &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;è¾“å…¥æè¿°:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;é¢˜ç›®ä¿è¯è¾“å…¥çš„æ•°ç»„ä¸­æ²¡æœ‰çš„ç›¸åŒçš„æ•°å­—&lt;/p&gt;

&lt;p&gt;æ•°æ®èŒƒå›´ï¼š&lt;/p&gt;

&lt;p&gt;â€‹	å¯¹äº%50çš„æ•°æ®,size&amp;lt;=10^4&lt;/p&gt;

&lt;p&gt;â€‹	å¯¹äº%75çš„æ•°æ®,size&amp;lt;=10^5&lt;/p&gt;

&lt;p&gt;â€‹	å¯¹äº%100çš„æ•°æ®,size&amp;lt;=2*10^5&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;è¾“å…¥ä¾‹å­:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1,2,3,4,5,6,7,0&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;è¾“å‡ºä¾‹å­:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;7&lt;/p&gt;

&lt;p&gt;ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œä¸è¿‡&lt;code class=&quot;highlighter-rouge&quot;&gt;count&lt;/code&gt;éœ€è¦å£°æ˜ä¸ºlongç±»å‹ï¼Œå› ä¸ºä½¿ç”¨intä¼šæº¢å‡ºã€‚javaé‡Œé¢çš„intç±»å‹ä¸º32ä½ï¼ŒèŒƒå›´å¤§æ¦‚ä¸º4.29*pow(10,9)ï¼Œä¹Ÿå°±æ˜¯æ­£è´ŸäºŒåå¤šäº¿ã€‚è€Œé¢˜ç›®ä¸­å–æ¨¡å°±æ˜¯å–åäº¿ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“å°±ä¼šè¶Šç•Œï¼Œæ‰€ä»¥ä»¥åè¿™ç§é¢˜ç›®ï¼Œä¸€å®šè¦ä¼šä½¿ç”¨longç±»å‹ï¼Œè€Œä¸æ˜¯ä¸€å‘³çš„intã€‚
&lt;code class=&quot;highlighter-rouge&quot;&gt;ä»£ç çš„é‡ç‚¹&lt;/code&gt;æ˜¯æ¯æ¬¡å‰é¢æ¯”åé¢å¤§ï¼Œcount å°±åŠ å‰é¢å‰©ä½™çš„ä¸ªæ•°ã€‚&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public class Solution {

     public long count=0;
    public     void merge(int [] a,int p,int q, int r){
        int l1=q-p+1;
        int l2=r-q;
        int[]a1=new int[l1+1];
        int[]a2=new int[l2+1];
        for (int i=0;i&amp;lt;l1;i++){
            a1[i]=a[p+i];
        }
        a1[l1]=Integer.MAX_VALUE;
        for (int i=0;i&amp;lt;l2;i++){
            a2[i]=a[q+1+i];
        }
        a2[l2]=Integer.MAX_VALUE;
    
        int i1=0;
        int i2=0;
       //æ­¤å¤„è¾¹ç•Œæé”™äº†,æˆ‘å».
        for (int i=p;i&amp;lt;=r;i++){
            if (a1[i1]&amp;lt;a2[i2]) {
                a[i] = a1[i1++];
            }
            else {
                a[i] = a2[i2++];
                count+=l1-i1;
    
            } 
   }
}
    public void mergeSort(int[]a,int p,int q){
    
        if (p&amp;lt;q){
            int t=(p+q)/2;
            mergeSort(a,p,t);
            mergeSort(a,t+1,q);
            merge(a,p,t,q);
    
        }
    }
    public int InversePairs(int [] array) {
        mergeSort(array,0,array.length-1);
        return (int)(count%1000000007);
    
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</description>
                <link>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/11/28/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0</link>
                <guid>http://bbwff.github.io/%E7%AE%97%E6%B3%95/2016/11/28/ç®—æ³•å¯¼è®ºå­¦ä¹ ç¬”è®°</guid>
                <pubDate>2016-11-28T17:29:00+08:00</pubDate>
        </item>

        <item>
                <title>Jvm ç›¸å…³</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æƒ³çœ‹çœ‹jvmç›¸å…³çš„ï¼Œäº†è§£ä¸‹gcä»¥åŠå†…å­˜çš„åˆ†é…ã€‚&lt;/p&gt;

&lt;h1 id=&quot;jvm&quot;&gt;Jvm&lt;/h1&gt;

&lt;p&gt;jvmå†…æœ‰æ–¹æ³•åŒºï¼Œè¿è¡Œæ—¶å¸¸é‡æ± æ˜¯æ–¹æ³•åŒºçš„ä¸€éƒ¨åˆ†ã€‚Stringçš„å®ä¾‹åŒ–å¯¹è±¡åœ¨jvmä¸­å°±æ˜¯å¸¸é‡ï¼Œä»‹ç»ä¸‹Stringç±»çš„intern()æ–¹æ³•ã€‚&lt;/p&gt;

&lt;p&gt;å½“ä¸€ä¸ªStringå®ä¾‹strè°ƒç”¨intern()æ–¹æ³•æ—¶ï¼ŒjavaæŸ¥æ‰¾å¸¸é‡æ± ä¸­æ˜¯å¦æœ‰ç›¸åŒunicodeçš„å­—ç¬¦ä¸²å¸¸é‡ï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¿”å›å…¶å¼•ç”¨ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™åœ¨å¸¸é‡æ± ä¸­å¢åŠ ä¸€ä¸ªunicodeç­‰äºstrçš„å­—ç¬¦ä¸²å¹¶è¿”å›å®ƒçš„å¼•ç”¨ã€‚è¿™æ ·å°±ä¼šèŠ‚çœæ–¹æ³•åŒºå¸¸é‡æ± çš„ç©ºé—´ã€‚&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/java/2016/11/17/Jvm-%E7%9B%B8%E5%85%B3</link>
                <guid>http://bbwff.github.io/java/2016/11/17/Jvm-ç›¸å…³</guid>
                <pubDate>2016-11-17T01:15:30+08:00</pubDate>
        </item>

        <item>
                <title>java unsafeç±»çš„ä½¿ç”¨</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æœ€è¿‘åœ¨å†™å †å¤–æ“ä½œçš„ä»£ç ï¼Œéœ€è¦ç”¨åˆ°unsafe ç±»ï¼Œè®°å½•ä¸‹ã€‚&lt;/p&gt;

&lt;h1 id=&quot;unsafe-ç®€ä»‹&quot;&gt;unsafe ç®€ä»‹&lt;/h1&gt;

&lt;p&gt;unsafeç±»ä½äº sun.miscåŒ…,ä¹‹æ‰€ä»¥å«unsafeæ˜¯å› ä¸ºä»–æ“ä½œå †å¤–å†…å­˜ï¼Œå³ä¸å—JVMæ§åˆ¶çš„å†…å­˜ã€‚ç”±äºæœ€è¿‘è¦åšç‚¹æŠŠæ•°æ®å­˜å‚¨åœ¨å †å¤–çš„å·¥ä½œï¼Œæ‰€ä»¥äº†è§£äº†ä¸‹unsafeã€‚&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;ä¸‹é¢æ˜¯å…³äºunsafeåšæµ‹è¯•çš„ä»£ç ã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JAVA&quot;&gt;import sun.misc.Unsafe;

import java.lang.reflect.Field;

/**
 * Created by bbw on 2016/11/11.
 */
class cat{
    public Integer name;
    public Integer age;
    public cat(Integer name,Integer age){
        this.name=name;
        this.age=age;
    }

}
public class testUnSafe {
    private static int apple = 10;
    private int orange = 10;
    private int banana=10;
    public   cat ki=new cat(233,3);

 //è¿™æ˜¯è·å¾—å¯¹è±¡é‡Œé¢å¯¹è±¡fieldçš„æ–¹æ³•ï¼Œæ ¹æ®è¿™ä¸ªå¯¹è±¡åœ¨ç±»é‡Œé¢çš„åç§»é‡æ¥è·å¾—
    public Object getObject(long offset) throws SecurityException, NoSuchFieldException, IllegalArgumentException,
            IllegalAccessException{
        return getUnsafeInstance().getObject(this,offset);
    }


    public static void main(String[] args) throws Exception {
        Unsafe unsafe = getUnsafeInstance();
        testUnSafe tus=new testUnSafe();

        Field appleField = testUnSafe.class.getDeclaredField(&quot;apple&quot;);
        // è·å¾—fieldçš„åç§»é‡
        System.out.println(&quot;Location of Apple: &quot; + unsafe.staticFieldOffset(appleField));

        Field orangeField = testUnSafe.class.getDeclaredField(&quot;orange&quot;);
        System.out.println(&quot;Location of Orange: &quot; + unsafe.objectFieldOffset(orangeField));



//è¿™æ˜¯field æ˜¯ä¸€ä¸ªcatç±»çš„å®ä¾‹åŒ–å¯¹è±¡ï¼Œæ ¹æ®ä»–çš„ä¾¿å®œåœ°å€è·å¾—å¯¹è±¡ï¼Œç„¶åå¼ºåˆ¶ç±»å‹è½¬åŒ–
        Field catField = testUnSafe.class.getDeclaredField(&quot;ki&quot;);
        System.out.println(&quot;Location of cat: &quot; + unsafe.objectFieldOffset(catField));
        long offset=unsafe.objectFieldOffset(catField);
        Object rki=tus.getObject(offset);
        cat rrki=(cat)rki;
        System.out.println(rrki.name);

        // follow is addressTest
        cat ncat=new cat(333,444);
        cat ncat2=new cat(555,666);
        cat[] ca={ncat,ncat2};
        long catArrayOffset=unsafe.arrayBaseOffset(cat[].class);
        System.out.println(catArrayOffset+&quot; &quot;+unsafe.arrayIndexScale(cat[].class));
        //cat rncat=((cat[])(tus.getObject(catArrayOffset)))[0];
       // System.out.println(rncat.name+rncat.age);
        Field bananaField = testUnSafe.class.getDeclaredField(&quot;banana&quot;);
        System.out.println(&quot;Location of banana: &quot; + unsafe.objectFieldOffset(bananaField));
    }
    
    //è·å¾—unsafe çš„æ–¹æ³•ï¼Œæ˜¯å•ä¾‹æ¨¡å¼
    private static Unsafe getUnsafeInstance() throws SecurityException, NoSuchFieldException, IllegalArgumentException,
            IllegalAccessException {
        Field theUnsafeInstance = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);
        theUnsafeInstance.setAccessible(true);
        return (Unsafe) theUnsafeInstance.get(Unsafe.class);
    }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;unsafe æ˜¯å•ä¾‹æ¨¡å¼ï¼Œæ‰€ä»¥å…¨å±€å°±åªæœ‰ä¸€ä¸ªunsafeï¼Œå¿…é¡»ç”¨å®ƒæä¾›çš„æ–¹æ³•æ¥è·å–ã€‚
ç„¶åé‡Œé¢æœ‰è·å¾—é‡Œé¢å­—æ®µ å’Œé™æ€å­—æ®µåç§»åœ°å€çš„æ–¹æ³•ï¼Œåç§»åœ°å€æ˜¯ç›¸å¯¹äºåœ¨è¿™ä¸ªå¯¹è±¡é‡Œé¢çš„åç§»åœ°å€ï¼Œå¯ä»¥æ ¹æ®åç§»åœ°å€è·å¾—è¿™ä¸ªfieldã€‚
ä¾‹å¦‚ï¼Œæˆ‘åœ¨è¿™ä¸ªç±»é‡Œé¢å£°æ˜çš„ cat ç±» field ï¼Œå°±å¯ä»¥æ ¹æ®å®ƒåœ¨å¯¹è±¡é‡Œé¢åç§»åœ°å€æ¥å–å¾—è¿™ä¸ªç±»ã€‚&lt;/p&gt;

&lt;p&gt;è‡³äºå¦‚ä½•è·å¾—æ–¹æ³•é‡Œé¢å˜é‡çš„å†…å­˜åœ°å€ä»¥åŠå¦‚ä½•é€šè¿‡è¿™ä¸ªè·å¾—çš„å†…å­˜åœ°å€æ¥å–å¾—è¿™ä¸ªå˜é‡å¯¹è±¡ï¼Œæˆ‘è¿˜ä¸æ˜¯å¾ˆæ˜ç™½ï¼ŒåªçŸ¥é“unsafe.arrayBaseOffset æ¥è·å¾—å¯¹è±¡æ•°æ®çš„åç§»åœ°å€ã€‚&lt;/p&gt;

&lt;p&gt;ä¸‹é¢æ˜¯æˆ‘å†™çš„ä¸€ä¸ªé™æ€ç±»ï¼Œå¯ä»¥ç”¨æ¥å®ç°unsafeçš„æ”¾ç½®å˜é‡ï¼Œå¹¶ä¸”å¯ä»¥æŠŠè¿™å—å †å¤–å†…å­˜é‡Œé¢å­˜çš„æ•°æ®è½¬åŒ–ä¸ºè¿­ä»£å™¨ã€‚
æˆ‘æ˜¯è¿™æ ·å­˜æ•°æ®çš„ï¼Œé¦–å…ˆæ˜¯ç”³è¯·ä¸€å—å †å¤–å†…å­˜ï¼Œç„¶åå‰å››ä¸ªå­—èŠ‚å­˜å‚¨è¿™å—å†…å­˜çš„å¤§å°ï¼Œç„¶åç´§æ¥ç€å››ä¸ªå­—èŠ‚å­˜å‚¨å·²ç»ä½¿ç”¨çš„å¤§å°ã€‚ç„¶åå­˜å‚¨æ•°æ®çš„ç±»å‹æ˜¯ä»å¤–éƒ¨ä¼ è¿›å»çš„ï¼Œ0ä»£è¡¨int,1ä»£è¡¨longï¼Œ2ä»£è¡¨doubleã€‚
ç„¶åæ¯æ¬¡åœ¨å†™å…¥æ•°æ®çš„æ—¶å€™ï¼Œéƒ½ä¼šåˆ¤æ–­è¿™å—å†…å­˜çš„å¤§å°å¤Ÿä¸å¤Ÿå†™å…¥æ•°æ®ï¼Œå¦‚æœä¸å¤Ÿå°±ç”³è¯·ä¸€ä¸ªæ›´å¤§çš„å†…å­˜ï¼Œç„¶åæŠŠåŸæ¥çš„æ•°æ®æ‹·è´åˆ°æ–°çš„å†…å­˜é‡Œé¢ï¼Œé‡æ–°å¯¹è¿™å—å†…å­˜çš„å‰å…«ä¸ªå­—èŠ‚èµ‹å€¼ï¼Œå³å†…å­˜çš„å¤§å°å’Œä½¿ç”¨æƒ…å†µã€‚
ç„¶åè¿™ä¸ªç±»çš„é™æ€å‚æ•°åœ¨æ¯æ¬¡ä¼ å…¥å†…å­˜çš„èµ·å§‹åœ°å€åä¼šé¦–å…ˆè¯»å–è¿™å—å†…å­˜çš„å‰å…«ä¸ªå­—èŠ‚ï¼Œè·å¾—å†…å­˜å¤§å°ä»¥åŠä½¿ç”¨æƒ…å†µã€‚&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JAVA&quot;&gt;
package org.apache.spark.unsafe;
import java.util.Iterator;
/**
 * Created by bbw on 2016/11/14.
 */
public  final class UnsafeBuffer&amp;lt;T&amp;gt; {



    public  static int  MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;

    public static   int  hugeCapacity(int minCapacity) {
        if (minCapacity &amp;lt; 0) throw new OutOfMemoryError();
        if ((minCapacity &amp;gt; MAX_ARRAY_SIZE))
            return Integer.MAX_VALUE;
        else
            return MAX_ARRAY_SIZE;
        }


    public static long  copyBuf2New ( long baseAddress,int vType,int  minCapacity) {
        // read the size and count of this buf(the format size,count)
        int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);



        long address = PlatformDependent.UNSAFE.allocateMemory(minCapacity);
        // write the size and count

        PlatformDependent.UNSAFE.putInt(null,address,minCapacity);
        PlatformDependent.UNSAFE.putInt(null,address+4,sizeCount);


        int   i= 8;
        switch (vType) {
            case 0 :
        while (i &amp;lt; sizeCount) {
        PlatformDependent.UNSAFE.putInt(null, address + i, PlatformDependent.UNSAFE.getInt(null, baseAddress + i));
        i = i + 4;
        }
            case 1 :
        while (i &amp;lt; sizeCount) {
        PlatformDependent.UNSAFE.putLong(null, address + i, PlatformDependent.UNSAFE.getLong(null, baseAddress + i));
        i = i + 8;
        }
            case 2 :
        while (i &amp;lt; sizeCount) {
        PlatformDependent.UNSAFE.putDouble(null, address + i, PlatformDependent.UNSAFE.getDouble(null, baseAddress + i));
        i = i + 8;
        }
        default:
            assert (1==0);
        }
        return address;

        }


        public static long putInt(long baseAddress, int vType,int value){
            int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
            int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);

           long address= ensureCapacity(baseAddress,vType,sizeCount+4);

            PlatformDependent.UNSAFE.putInt(null,address+sizeCount,value);

            PlatformDependent.UNSAFE.putInt(null,address+4,sizeCount+4);

            return address;


        }
    public static long putLong(long baseAddress, int vType,long value){
        int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);

        long address= ensureCapacity(baseAddress,vType,sizeCount+8);

        PlatformDependent.UNSAFE.putLong(null,address+sizeCount,value);

        PlatformDependent.UNSAFE.putInt(null,address+4,sizeCount+8);

        return address;


    }
    public static long putDouble(long baseAddress, int vType,double value){
        int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);

        long address= ensureCapacity(baseAddress,vType,sizeCount+8);

        PlatformDependent.UNSAFE.putDouble(null,address+sizeCount,value);

        PlatformDependent.UNSAFE.putInt(null,address+4,sizeCount+8);

        return address;


    }


public  static long grow (long  baseAddress,int vType,int minCapacity) {

    int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
    int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);
        int  oldCapacity=size;
        int  newCapacity = oldCapacity &amp;lt;&amp;lt; 1;
        if (newCapacity - minCapacity &amp;lt; 0) newCapacity = minCapacity;
        if (newCapacity - MAX_ARRAY_SIZE &amp;gt; 0) newCapacity = hugeCapacity(minCapacity);
        //buf = Arrays.copyOf(buf, newCapacity)
        //é‡æ–°åˆ†é…ç©ºé—´
        // baseAddress=PlatformDependent.UNSAFE.allocateMemory(newCapacity)
        long  temp=copyBuf2New(baseAddress,vType,minCapacity);
        PlatformDependent.UNSAFE.freeMemory(baseAddress);

    return temp;
}

    public static  long   ensureCapacity (long baseAddress,int vType,int minCapacity) {

    int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
    int sizeCount=PlatformDependent.UNSAFE.getInt(null,baseAddress+4);


        if (minCapacity - size &amp;gt; 0)
           return grow(baseAddress,vType,minCapacity);
    else return baseAddress;
}




    public static   Iterator&amp;lt;Integer&amp;gt; intIterator( long baseAddress) {
        // int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        final int sizeCount = PlatformDependent.UNSAFE.getInt(null, baseAddress + 4);
        final long address = baseAddress;
        return new Iterator&amp;lt;Integer&amp;gt;() {
            int offset = 8;

            @Override
            public boolean hasNext() {
                if (offset &amp;lt; sizeCount)
                    return true;
                else {
                    PlatformDependent.UNSAFE.freeMemory(address);
                    return false;
                }
            }

            @Override
            public Integer next() {
                offset += 4;
                return PlatformDependent.UNSAFE.getInt(null, address + offset - 4);
            }
        };
    }

    public static   Iterator&amp;lt;Long&amp;gt; longIterator( long baseAddress) {
        // int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        final int sizeCount = PlatformDependent.UNSAFE.getInt(null, baseAddress + 4);
        final long address = baseAddress;
        return new Iterator&amp;lt;Long&amp;gt;() {
            int offset = 8;

            @Override
            public boolean hasNext() {
                if (offset &amp;lt; sizeCount)
                    return true;
                else {
                    PlatformDependent.UNSAFE.freeMemory(address);
                    return false;
                }
            }

            @Override
            public Long next() {
                offset += 8;
                return PlatformDependent.UNSAFE.getLong(null, address + offset - 8);
            }
        };
    }

    public static   Iterator&amp;lt;Double&amp;gt; doubleIterator( long baseAddress) {
        // int size=PlatformDependent.UNSAFE.getInt(null,baseAddress);
        final int sizeCount = PlatformDependent.UNSAFE.getInt(null, baseAddress + 4);
        final long address = baseAddress;
        return new Iterator&amp;lt;Double&amp;gt;() {
            int offset = 8;

            @Override
            public boolean hasNext() {
                if (offset &amp;lt; sizeCount)
                    return true;
                else {
                    PlatformDependent.UNSAFE.freeMemory(address);
                    return false;
                }
            }

            @Override
            public Double next() {
                offset += 8;
                return PlatformDependent.UNSAFE.getDouble(null, address + offset - 8);
            }
        };
    }


    public static  long createBuff(int size){
        long address=PlatformDependent.UNSAFE.allocateMemory(size);
        PlatformDependent.UNSAFE.putInt(null,address,size);
        PlatformDependent.UNSAFE.putInt(null,address+4,8);
        return address;
    }
}

&lt;/code&gt;&lt;/pre&gt;
</description>
                <link>http://bbwff.github.io/java/2016/11/13/java-unsafe%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8</link>
                <guid>http://bbwff.github.io/java/2016/11/13/java-unsafeç±»çš„ä½¿ç”¨</guid>
                <pubDate>2016-11-13T08:24:19+08:00</pubDate>
        </item>

        <item>
                <title>è®¡ç®—æœºä¼šè®®</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ä»è€æ¿ä¸»é¡µæåˆ°çš„ä¼šè®®åˆ—è¡¨ï¼Œç”¨äºæŸ¥æ‰¾ä¼šè®®è®ºæ–‡ã€‚&lt;/p&gt;

&lt;h1 id=&quot;area-system-technology&quot;&gt;AREA: System Technology&lt;/h1&gt;

&lt;h2 id=&quot;rank-1&quot;&gt;Rank 1:&lt;/h2&gt;

&lt;p&gt;SIGCOMM: ACM Conf on Comm Architectures, Protocols &amp;amp; Apps
INFOCOM: Annual Joint Conf IEEE Comp &amp;amp; Comm Soc
SPAA: Symp on Parallel Algms and Architecture
PODC: ACM Symp on Principles of Distributed Computing
PPoPP: Principles and Practice of Parallel Programming
RTSS: Real Time Systems Symp
SOSP: ACM SIGOPS Symp on OS Principles
SOSDI: Usenix Symp on OS Design and Implementation 
CCS: ACM Conf on Comp and Communications Security
IEEE Symposium on Security and Privacy
MOBICOM: ACM Intl Conf on Mobile Computing and Networking
USENIX Conf on Internet Tech and Sys
ICNP: Intl Conf on Network Protocols
PACT: Intl Conf on Parallel Arch and Compil Tech
RTAS: IEEE Real-Time and Embedded Technology and Applications Symposium
ICDCS: IEEE Intl Conf on Distributed Comp Systems&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;rank-2&quot;&gt;Rank 2:&lt;/h2&gt;

&lt;p&gt;CC: Compiler Construction
IPDPS: Intl Parallel and Dist Processing Symp
IC3N: Intl Conf on Comp Comm and Networks
ICPP: Intl Conf on Parallel Processing
SRDS: Symp on Reliable Distributed Systems
MPPOI: Massively Par Proc Using Opt Interconns
ASAP: Intl Conf on Apps for Specific Array Processors
Euro-Par: European Conf. on Parallel Computing
Fast Software Encryption
Usenix Security Symposium
European Symposium on Research in Computer Security
WCW: Web Caching Workshop
LCN: IEEE Annual Conference on Local Computer Networks
IPCCC: IEEE Intl Phoenix Conf on Comp &amp;amp; Communications
CCC: Cluster Computing Conference
ICC: Intl Conf on Comm
WCNC: IEEE Wireless Communications and Networking Conference
CSFW: IEEE Computer Security Foundations Workshop&lt;/p&gt;

&lt;h2 id=&quot;rank-3&quot;&gt;Rank 3:&lt;/h2&gt;

&lt;p&gt;MPCS: Intl. Conf. on Massively Parallel Computing Systems
GLOBECOM: Global Comm
ICCC: Intl Conf on Comp Communication
NOMS: IEEE Network Operations and Management Symp
CONPAR: Intl Conf on Vector and Parallel Processing
VAPP: Vector and Parallel Processing
ICPADS: Intl Conf. on Parallel and Distributed Systems
Public Key Cryptosystems
Annual Workshop on Selected Areas in Cryptography
Australasia Conference on Information Security and Privacy
Int. Conf on Inofrm and Comm. Security
Financial Cryptography
Workshop on Information Hiding
Smart Card Research and Advanced Application Conference
ICON: Intl Conf on Networks
NCC: Nat Conf Comm
IN: IEEE Intell Network Workshop
Softcomm: Conf on Software in Tcomms and Comp Networks
INET: Internet Society Conf
Workshop on Security and Privacy in E-commerce&lt;/p&gt;

&lt;h2 id=&quot;un-ranked&quot;&gt;Un-ranked:&lt;/h2&gt;

&lt;p&gt;PARCO: Parallel Computing
SE: Intl Conf on Systems Engineering (**)
PDSECA: workshop on Parallel and Distributed Scientific and Engineering Computing with Applications
CACS: Computer Audit, Control and Security Conference
SREIS: Symposium on Requirements Engineering for Information Security
SAFECOMP: International Conference on Computer Safety, Reliability and Security
IREJVM: Workshop on Intermediate Representation Engineering for the Java Virtual Machine
EC: ACM Conference on Electronic Commerce
EWSPT: European Workshop on Software Process Technology
HotOS: Workshop on Hot Topics in Operating Systems
HPTS: High Performance Transaction Systems
Hybrid Systems
ICEIS: International Conference on Enterprise Information Systems
IOPADS: I/O in Parallel and Distributed Systems
IRREGULAR: Workshop on Parallel Algorithms for Irregularly Structured Problems
KiVS: Kommunikation in Verteilten Systemen
LCR: Languages, Compilers, and Run-Time Systems for Scalable Computers
MCS: Multiple Classifier Systems
MSS: Symposium on Mass Storage Systems
NGITS: Next Generation Information Technologies and Systems
OOIS: Object Oriented Information Systems
SCM: System Configuration Management
Security Protocols Workshop
SIGOPS European Workshop
SPDP: Symposium on Parallel and Distributed Processing
TreDS: Trends in Distributed Systems
USENIX Technical Conference
VISUAL: Visual Information and Information Systems
FoDS: Foundations of Distributed Systems: Design and Verification of Protocols conference
RV: Post-CAV Workshop on Runtime Verification
ICAIS: International ICSC-NAISO Congress on Autonomous Intelligent Systems
ITiCSE: Conference on Integrating Technology into Computer Science Education
CSCS: CyberSystems and Computer Science Conference
AUIC: Australasian User Interface Conference
ITI: Meeting of Researchers in Computer Science, Information Systems Research &amp;amp; Statistics
European Conference on Parallel Processing
RODLICS: Wses International Conference on Robotics, Distance Learning &amp;amp; Intelligent Communication Systems
International Conference On Multimedia, Internet &amp;amp; Video Technologies
PaCT: Parallel Computing Technologies workshop
PPAM: International Conference on Parallel Processing and Applied Mathematics
International Conference On Information Networks, Systems And Technologies
AmiRE: Conference on Autonomous Minirobots for Research and Edutainment
DSN: The International Conference on Dependable Systems and Networks
IHW: Information Hiding Workshop
GTVMT: International Workshop on Graph Transformation and Visual Modeling Techniques&lt;/p&gt;

&lt;h1 id=&quot;area-databases&quot;&gt;AREA: Databases&lt;/h1&gt;

&lt;h2 id=&quot;rank-1-1&quot;&gt;Rank 1:&lt;/h2&gt;

&lt;p&gt;SIGMOD: ACM SIGMOD Conf on Management of Data
PODS: ACM SIGMOD Conf on Principles of DB Systems
VLDB: Very Large Data Bases
ICDE: Intl Conf on Data Engineering
CIKM: Intl. Conf on Information and Knowledge Management
ICDT: Intl Conf on Database Theory&lt;/p&gt;

&lt;h2 id=&quot;rank-2-1&quot;&gt;Rank 2:&lt;/h2&gt;

&lt;p&gt;SSD: Intl Symp on Large Spatial Databases
DEXA: Database and Expert System Applications
FODO: Intl Conf on Foundation on Data Organization
EDBT: Extending DB Technology
DOOD: Deductive and Object-Oriented Databases
DASFAA: Database Systems for Advanced Applications
SSDBM: Intl Conf on Scientific and Statistical DB Mgmt
CoopIS - Conference on Cooperative Information Systems
ER - Intl Conf on Conceptual Modeling (ER)&lt;/p&gt;
&lt;h2 id=&quot;rank-3-1&quot;&gt;Rank 3:&lt;/h2&gt;

&lt;p&gt;COMAD: Intl Conf on Management of Data
BNCOD: British National Conference on Databases
ADC: Australasian Database Conference
ADBIS: Symposium on Advances in DB and Information Systems
DaWaK - Data Warehousing and Knowledge Discovery
RIDE Workshop
IFIP-DS: IFIP-DS Conference
IFIP-DBSEC - IFIP Workshop on Database Security
NGDB: Intl Symp on Next Generation DB Systems and Apps
ADTI: Intl Symp on Advanced DB Technologies and Integration
FEWFDB: Far East Workshop on Future DB Systems
MDM - Int. Conf. on Mobile Data Access/Management (MDA/MDM)
VDB - Visual Database Systems
IDEAS - International Database Engineering and Application Symposium&lt;/p&gt;

&lt;h2 id=&quot;others&quot;&gt;Others:&lt;/h2&gt;

&lt;p&gt;ARTDB - Active and Real-Time Database Systems
CODAS: Intl Symp on Cooperative DB Systems for Adv Apps
DBPL - Workshop on Database Programming Languages
EFIS/EFDBS - Engineering Federated Information (Database) Systems
KRDB - Knowledge Representation Meets Databases
NDB - National Database Conference (China) 
NLDB - Applications of Natural Language to Data Bases
FQAS - Flexible Query-Answering Systems
IDC(W) - International Database Conference (HK CS)
RTDB - Workshop on Real-Time Databases
SBBD: Brazilian Symposium on Databases
WebDB - International Workshop on the Web and Databases
WAIM: Interational Conference on Web Age Information Management
DASWIS - Data Semantics in Web Information Systems
DMDW - Design and Management of Data Warehouses
DOLAP - International Workshop on Data Warehousing and OLAP
DMKD - Workshop on Research Issues in Data Mining and Knowledge Discovery
KDEX - Knowledge and Data Engineering Exchange Workshop
NRDM - Workshop on Network-Related Data Management
MobiDE - Workshop on Data Engineering for Wireless and Mobile Access
MDDS - Mobility in Databases and Distributed Systems
MEWS - Mining for Enhanced Web Search
TAKMA - Theory and Applications of Knowledge MAnagement
WIDM: International Workshop on Web Information and Data Management
W2GIS - International Workshop on Web and Wireless Geographical Information Systems
CDB - Constraint Databases and Applications
DTVE - Workshop on Database Technology for Virtual Enterprises
IWDOM - International Workshop on Distributed Object Management 
OODBS - Workshop on Object-Oriented Database Systems
PDIS: Parallel and Distributed Information Systems&lt;/p&gt;

&lt;h1 id=&quot;other-links&quot;&gt;Other Links&lt;/h1&gt;
&lt;h2 id=&quot;infomation-center&quot;&gt;Infomation center&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://grid.hust.edu.cn/xhshi/TopConferences.htm&quot;&gt;List of Important Conferences (CGCL)&lt;/a&gt;
&lt;a href=&quot;http://citeseer.ist.psu.edu/statistics.html&quot;&gt;Publications Impacts Citeseer Statistics&lt;/a&gt;
&lt;a href=&quot;http://www.cs.ucsb.edu/~almeroth/conf/stats/&quot;&gt;Networking Conferences Statistics&lt;/a&gt;
&lt;a href=&quot;http://www.ntu.edu.sg/home/assourav/crank.htm&quot;&gt;Computer Science Conference Rankings&lt;/a&gt;
&lt;a href=&quot;http://www.ntu.edu.sg/home/assourav/jrank.htm&quot;&gt;Computer Science Journal Rankings&lt;/a&gt;
&lt;a href=&quot;http://www.ee.unsw.edu.au/~timm/netconf/#icdcs&quot;&gt;Networking conference dates&lt;/a&gt;
&lt;a href=&quot;http://www.usenix.org/&quot;&gt;USENIX Group&lt;/a&gt;
&lt;a href=&quot;http://tab.computer.org/tcsc/&quot;&gt;IEEE Service Computing Community&lt;/a&gt;
&lt;a href=&quot;http://www.gridcomputing.com/&quot;&gt;Grid Computing Information Centre&lt;/a&gt;
&lt;a href=&quot;http://www.grids-center.org/&quot;&gt;Grids Center&lt;/a&gt;
&lt;a href=&quot;http://computer.org/parascope/&quot;&gt;ParaScope&lt;/a&gt;
&lt;a href=&quot;http://www.computer.org/portal/site/dsonline/menuitem.0e7741ff4cba82ff96d34f108bcd45f3/index.jsp?&amp;amp;pName=dsonline_grid_test&amp;amp;&quot;&gt;IEEE DS Online on Grid computing&lt;/a&gt;
&lt;a href=&quot;http://www.usenix.org/events/index.html&quot;&gt;USENIX Conference Calendar&lt;/a&gt;
&lt;a href=&quot;http://www.acm.org/conferences&quot;&gt;ACM Conference Canlendar&lt;/a&gt;
&lt;a href=&quot;http://www.cs.wisc.edu/~arch/www/&quot;&gt;WWW Computer Architecture Page&lt;/a&gt;
&lt;a href=&quot;http://www.nesc.ac.uk/projects/&quot;&gt;Projects in UK e-Science&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://duda.imag.fr/Comer/research.html&quot;&gt;Essays about Computer Science by Douglas E. Comer&lt;/a&gt;
&lt;a href=&quot;http://www.cs.indiana.edu/how.2b/how.2b.html&quot;&gt;How to Be a Good Graduate Student by Marie desJardins&lt;/a&gt;
&lt;a href=&quot;https://www.usenix.org/legacy/event/samples/submit/advice.html&quot;&gt;How to write a good system paper&lt;/a&gt;
&lt;a href=&quot;http://www.cs.utexas.edu/users/EWD/transcriptions/EWD06xx/EWD637.html&quot;&gt;Dijkstraâ€™s Rules for Successful Scientific Research&lt;/a&gt;
&lt;a href=&quot;http://www.cs.jhu.edu/~mdredze/publications/HowtoBeaSuccessfulPhDStudent.pdf&quot;&gt;How to Be a Successful PhD Student (in Computer Science (in NLP/ML))&lt;/a&gt;&lt;/p&gt;

</description>
                <link>http://bbwff.github.io/%E7%A7%91%E7%A0%94%E7%9B%B8%E5%85%B3/2016/08/30/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BC%9A%E8%AE%AE</link>
                <guid>http://bbwff.github.io/%E7%A7%91%E7%A0%94%E7%9B%B8%E5%85%B3/2016/08/30/è®¡ç®—æœºä¼šè®®</guid>
                <pubDate>2016-08-30T18:01:29+08:00</pubDate>
        </item>

        <item>
                <title>ganglia å®‰è£…</title>
                <description>
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;æœ€è¿‘å¸®å¤§è èå®‰è£…gangliaï¼Œè®°å½•ä¸‹ï¼Œæ–¹ä¾¿ä»¥åå®‰è£…ã€‚&lt;/p&gt;

&lt;h1 id=&quot;cluster-server-and-clients&quot;&gt;Cluster Server and Clients&lt;/h1&gt;

&lt;p&gt;I configured our nodes with the following hostnames using these steps. Our server is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3.buhpc.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The clients are:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1.buhpc.com
2.buhpc.com
4.buhpc.com
5.buhpc.com
6.buhpc.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;!--more--&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;On the server, inside the shared folder of our cluster, we will first download the latest version of ganglia. For our cluster, /nfs is the folder with our network file system.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /nfs
wget http://downloads.sourceforge.net/project/ganglia/ganglia%20monitoring%20core/3.7.2/ganglia-3.7.2.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;On the server, we will install dependencies and libconfuse.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install freetype-devel rpm-build php httpd libpng-devel libart_lgpl-devel python-devel pcre-devel autoconf automake libtool expat-devel rrdtool-devel apr-devel gcc-c++ make pkgconfig -y
yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/l/libconfuse-2.7-7.el7.x86_64.rpm -y
yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/l/libconfuse-devel-2.7-7.el7.x86_64.rpm -y

#å»ºç«‹rrdæ•°æ®åº“
mkdir -p /var/lib/ganglia/rrds/
chown nobody:nobody -R /var/lib/ganglia/rrds/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now, we will build the rpms from ganglia-3.7.2 on the server.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rpmbuild -tb ganglia-3.7.2.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;After running rpmbuild, /root/rpmbuild/RPMS/x86_64 contains the generated rpms:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /root/rpmbuild/RPMS/x86_64/
yum install *ganglia*.rpm -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We will remove gmetad because we do not need it on the clients. Send the rest of the rpms to all the clientsâ€™ /tmp folder:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /root/rpmbuild/RPMS/x86_64/
rm -rf ganglia-gmetad*.rpm
scp *.rpm root@1.buhpc.com:/tmp
scp *.rpm root@2.buhpc.com:/tmp
scp *.rpm root@4.buhpc.com:/tmp
scp *.rpm root@5.buhpc.com:/tmp
scp *.rpm root@6.buhpc.com:/tmp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;SSH onto every client and install the rpms that we will need:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh root@#.buhpc.com
yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/l/libconfuse-2.7-7.el7.x86_64.rpm -y
yum install https://dl.fedoraproject.org/pub/epel/7/x86_64/l/libconfuse-devel-2.7-7.el7.x86_64.rpm -y
yum install /tmp/*ganglia*.rpm - y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Back on the server, we will adjust the gmetad configuration file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd /etc/ganglia
vim gmetad.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;buhpc will be the name of  our cluster. Find the following line and add the name of your cluster and ip address. I am using the subdomain instead of the ip address.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data_source &quot;buhpc&quot; 1 3.buhpc.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now, we edit the serverâ€™s gmond configuration file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/ganglia/gmond.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Make sure that these sections have the following and comment any extra lines you see that are within each section.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cluster {
  name = &quot;buhpc&quot;
  owner = &quot;unspecified&quot;
  latlong = &quot;unspecified&quot;
  url = &quot;unspecified&quot;
}

udp_send_channel {
  host = 1.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 2.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 3.buhpc.com
  port = 8649
  ttl = 1
}
udp_send_channel {
  host = 4.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 5.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 6.buhpc.com
  port = 8649
  ttl = 1
}

udp_recv_channel {
  port = 8649
  retry_bind = true
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Now, SSH into each of the clients and do the following individually. On every client:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/ganglia/gmond.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We will change the clientsâ€™ gmond.conf in the same way as the serverâ€™s.  Make sure that these sections have the following lines and comment any extra lines you see that are within each section.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cluster {
  name = &quot;buhpc&quot;
  owner = &quot;unspecified&quot;
  latlong = &quot;unspecified&quot;
  url = &quot;unspecified&quot;
}

udp_send_channel {
  host = 1.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 2.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 3.buhpc.com
  port = 8649
  ttl = 1
}
udp_send_channel {
  host = 4.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 5.buhpc.com
  port = 8649
  ttl = 1
}

udp_send_channel {
  host = 6.buhpc.com
  port = 8649
  ttl = 1
}

udp_recv_channel {
  port = 8649
  retry_bind = true
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;We will start gmond on the clients for monitoring.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chkconfig gmond on
systemctl start gmond
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;ç„¶åï¼Œå®‰è£…ganglia-web 3.7.1&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget http://superb-sea2.dl.sourceforge.net/project/ganglia/ganglia-web/3.7.1/ganglia-web-3.7.1.tar.gz
tar zxvf  ganglia-web-3.7.1.tar.gz
cd  ganglia-web-3.7.1
vim Makefile
      # Location where gweb should be installed to (excluding conf, dwoo dirs).
      GDESTDIR = /var/www/html/ganglia

      # Gweb statedir (where conf dir and Dwoo templates dir are stored)
      GWEB_STATEDIR = /var/lib/ganglia-web

      # Gmetad rootdir (parent location of rrd folder)
      GMETAD_ROOTDIR = /var/lib/ganglia

      # User by which your webserver is running
      APACHE_USER =  apache

 make install
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next, we will want to disable SELinux. Change SELINUX inside /etc/sysconfig/selinux from enforcing to disabled. Then, restart the server node.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/sysconfig/selinux
SELINUX=disabled
#å¦‚æœ SELINUXæœ¬å°±æ˜¯disableï¼Œä¸å¿…reboot
reboot
Now, on the server, weâ€™ll open the correct ports on the firewall.

#å¦‚æœ firewall æ²¡æœ‰æ‰“å¼€ï¼Œsystemctl service firewalld
firewall-cmd --permanent --zone=public --add-service=http
firewall-cmd --permanent --zone=public --add-port=8649/udp
firewall-cmd --permanent --zone=public --add-port=8649/tcp
firewall-cmd --permanent --zone=public --add-port=8651/tcp
firewall-cmd --permanent --zone=public --add-port=8652/tcp
firewall-cmd --reload
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;On the server, we will now start httpd, gmetad, and gmond.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chkconfig httpd
chkconfig gmetad on
chkconfig gmond on
systemctl start httpd
systemctl start gmetad
systemctl start gmond
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Visit http://3.buhpc.com/ganglia to see Gangliaâ€™s monitoring. You should see something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.slothparadise.com/wp-content/uploads/2016/03/ganglia-home-page.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

</description>
                <link>http://bbwff.github.io/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/2016/08/17/ganglia-%E5%AE%89%E8%A3%85</link>
                <guid>http://bbwff.github.io/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/2016/08/17/ganglia-å®‰è£…</guid>
                <pubDate>2016-08-17T05:15:03+08:00</pubDate>
        </item>

        <item>
                <title>ç”Ÿæ´»æ—¥è®°</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ä¸äººæ¸£åµæ¶&quot; id=&quot;markdown-toc-ä¸äººæ¸£åµæ¶&quot;&gt;ä¸äººæ¸£åµæ¶&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#å­¤ç‹¬&quot; id=&quot;markdown-toc-å­¤ç‹¬&quot;&gt;å­¤ç‹¬&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;è®°å½•ä¸€äº›ç”Ÿæ´»ä¸­çš„çäº‹ï¼Œæˆ–æ˜¯åæ§½ï¼Œæˆ–æ˜¯é¸¡æ±¤ï¼ŒæŒç»­æ›´æ–°&lt;/p&gt;

&lt;h2 id=&quot;ä¸äººæ¸£åµæ¶&quot;&gt;ä¸äººæ¸£åµæ¶&lt;/h2&gt;

&lt;p&gt;â€‹     &lt;strong&gt;2016-11-19&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å› ä¸ºä¸€ä¸ªäººè€Œè®¨åŒå¦ä¸€ä¸ªäººï¼Œå¹¶ä¸”åœ¨è´Ÿé¢æƒ…ç»ªä¸‹äº†è§£äº†è¿™ä¸ªäººæ›´æ·±å±‚æ¬¡çš„ä¸€é¢è€Œæ›´è®¨åŒè¿™ä¸ªäººã€‚å› ä¸ºè¿™ä¸ªäººç»™æˆ‘çš„å°è±¡å¾ˆå·®ï¼Œæ‰€ä»¥åœ¨æ²¡æœ‰èµ°å‡ºçš„è´Ÿé¢æƒ…ç»ªä¸‹ï¼Œå°±æƒ³å»æ­å‘è¿™ä¸ªäººï¼Œå°±å»æ‰¾è¿™ä¸ªäººäº‰åµï¼Œè‡ªå·±ä¹Ÿå˜æˆäº†ä¸€ä¸ªå’Œçƒ‚äººäº‰åµçš„äººï¼Œä¹Ÿå˜å¾—è®©äººè®¨åŒã€‚&lt;/p&gt;

&lt;p&gt;è¿™ä¸ªä¸–ç•Œä¸Šæœ‰å¾ˆå¤šäººï¼Œæœ‰å¾ˆå¤šå¥½äººï¼Œä¹Ÿæœ‰å¾ˆå¤šçƒ‚äººã€‚ç”Ÿæ´»æ˜¯ä¸€ä¸ªäººçš„ï¼Œä¸è¦æé‚£ä¹ˆå¤æ‚ï¼Œç®€å•çš„ç”Ÿæ´»ï¼Œè®©è‡ªå·±ä¿æŒä¸€ä¸ªç§¯æçš„å¿ƒæ€ï¼Œå¯¹çƒ‚äººä»˜ä¹‹ä¸€ç¬‘ï¼Œå¤©ç†å¾ªç¯ï¼Œæœ‰å› å°±æœ‰æœï¼Œåšäº†ä»€ä¹ˆè‡ªç„¶ä¼šå¾—åˆ°ä»€ä¹ˆæ ·çš„æœã€‚äººç”Ÿè¿˜æœ‰å¾ˆé•¿ï¼Œä½†æ—¶é—´ç»ˆå½’æœ‰é™ï¼ŒæŠŠæ—¶é—´ç•™ç»™é‚£äº›æœ‰æ„ä¹‰çš„äº‹ï¼Œç”Ÿæ´»ä¼šå¾ˆç¾å¥½ã€‚&lt;/p&gt;

&lt;p&gt;å¥½å¥½åœ°æ´»ç€å§ã€‚&lt;/p&gt;

&lt;h2 id=&quot;å­¤ç‹¬&quot;&gt;å­¤ç‹¬&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;2017-01-02&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;å¤§å¤šæ•°æ—¶é—´éƒ½æ˜¯å­¤ç‹¬çš„ï¼Œä¸ç®¡æ˜¯äººå¤šè¿˜æ˜¯äººå°‘ã€‚æœ‰çš„äº‹æƒ…ä¸æƒ³ä¸å‘¨å›´çš„äººè¯´ï¼›ä¸çƒ­è¡·äºä¸»åŠ¨è”ç³»å¥½æœ‹å‹ï¼Œè™½ç„¶ä»–ä»¬åœ¨æˆ‘å¿ƒç›®ä¸­éƒ½æ˜¯å¥½æœ‹å‹ï¼Œä½†æ˜¯ä¹Ÿä¸æ„¿æ„å»æ‰“æ‰°åˆ«ï¼›å–œæ¬¢å¬æ¯”è¾ƒä¼¤æ„Ÿçš„æ­Œï¼›ä¸çƒ­è¡·äºè¿‡èŠ‚ï¼Œæˆ–è®¸è¿™äº›èŠ‚æ—¥åœ¨æˆ‘å¿ƒä¸­çš„åœ°ä½ä¸é«˜å§ï¼Œç°åœ¨æ„Ÿè§‰é™¤äº†æ˜¥èŠ‚ã€ä¸­ç§‹ã€è¿˜æœ‰å…ƒå®µèŠ‚è¿™äº›ä¼ ç»ŸèŠ‚æ—¥ï¼Œå…¶ä»–éƒ½ä¸æˆ‘æ— å…³ï¼Œä»€ä¹ˆå†¬è‡³æˆ‘éƒ½ä¸åƒæ°´é¥ºï¼ˆæˆ‘å†¬è‡³å‰ä¸€å¤©åƒï¼‰ï¼Œä»€ä¹ˆå¹³å®‰å¤œæˆ‘éƒ½ä¸åƒè‹¹æœï¼ˆæˆ‘ç¬¬äºŒå¤©æˆ–è€…å‰å¤©åƒï¼‰ï¼›å¦‚æœä¸æ˜¯åˆ«äººå«æˆ‘å»çœ‹ç”µå½±ï¼Œæˆ‘è¿˜æ˜¯æ¯”è¾ƒå–œæ¬¢ä¸€ä¸ªäººå»çœ‹ç”µå½±ï¼›ä¸€ä¸ªäººå‡ºå»é€›è¡—ä¹°è¡£æœï¼›å¯æ˜¯æˆ‘å¹¶ä¸å–œæ¬¢ä¸€ä¸ªäººå»è·‘æ­¥ï¼Œæˆ–è®¸æ˜¯æˆ‘è®¤ä¸ºç©ºæ°”æ±¡æŸ“å¤ªä¸¥é‡äº†ï¼Ÿå¦‚æœæœ‰äººè¦é™ªæˆ‘ä¸€èµ·å»å¸é›¾éœ¾ï¼Œé‚£æˆ‘ä¼šå’Œä»–ä¸€èµ·è·‘çš„ï¼Œhhhhâ€¦&lt;/p&gt;

&lt;p&gt;ç¥æ—©æ—¥æ¯•ä¸š!&lt;/p&gt;

&lt;embed src=&quot;//music.163.com/style/swf/widget.swf?sid=27646199&amp;amp;type=2&amp;amp;auto=1&amp;amp;width=320&amp;amp;height=66&quot; width=&quot;340&quot; height=&quot;86&quot; allownetworking=&quot;all&quot; /&gt;

</description>
                <link>http://bbwff.github.io/%E6%97%A5%E8%AE%B0/2016/07/22/%E7%94%9F%E6%B4%BB%E6%97%A5%E8%AE%B0</link>
                <guid>http://bbwff.github.io/%E6%97%A5%E8%AE%B0/2016/07/22/ç”Ÿæ´»æ—¥è®°</guid>
                <pubDate>2016-07-22T00:00:00+08:00</pubDate>
        </item>

        <item>
                <title>Hello World</title>
                <description>
&lt;p&gt;ç›®å½•&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;æ¨¡æ¿&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Black&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;list&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mark&lt;/code&gt;&lt;/p&gt;
</description>
                <link>http://bbwff.github.io/test/1992/05/31/hello-world</link>
                <guid>http://bbwff.github.io/test/1992/05/31/hello-world</guid>
                <pubDate>1992-05-31T00:00:00+08:00</pubDate>
        </item>


</channel>
</rss>
