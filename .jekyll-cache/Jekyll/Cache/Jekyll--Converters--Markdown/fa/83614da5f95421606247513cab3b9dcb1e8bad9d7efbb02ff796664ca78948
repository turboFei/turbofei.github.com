I">_<h1 id="investigation-of-dynamic-allocation-in-spark">Investigation of Dynamic Allocation in Spark</h1>

<p>Dynamic executor allocation is an important mechanism in Spark for better resource scheduling. Today I will give a deep investigation of how this mechanism works, what is the pros and cons of this mechanism and future works we could do on this area.</p>

<h2 id="why-dynamic-allocation-matters">Why Dynamic Allocation Matters</h2>

<p>As we all know Spark is a distributed computation engine, it has several deploy modes like Standalone, Mesos and Yarn, which means Spark can run on these cluster managers. Spark core itself does not care about resource scheduling and management, it will ask the cluster manager to get resources it wanted. This is the legacy design of modern computation engine, like MRv2, separate job scheduling and resource management.</p>

<p>But the difference between Spark and MRv2 is:</p>

<blockquote>
  <p>Each Spark task is a thread which resides in a process called “<strong>executor</strong>”, normally executor is a long running process launched at the start of Spark application, and be killed after the application is finished.</p>
</blockquote>

<blockquote>
  <p>While in MRv2, each task resides in a process, its lifetime is task based, which means it will be killed when the task is finished.</p>
</blockquote>

<p>Thinking of each process as a resource unit, in Spark it will hold the resources until the end of application, while in MRv2, each resource unit will be released at run-time.</p>

<p>If your workload is the only application running on the cluster, this resource pre-acquisition will get better performance, since it doesn’t need to acquire the resources in the run-time. But in a real, in-production environment, normally you’re not the monopolizer of the cluster, this pre-acquisiton will introduce some resource scheduling problems:</p>

<ol>
  <li>Under utilise of cluster resources. For example, if you start a spark-shell application  with some resources, but do not submit even one job for a long time, at this situation these resources are under-utilized, though occupied by Spark, it is not a intended behaviour.</li>
  <li>Starvation of other applications. If your application occupies lots of resources without releasing, other application will be queued for resource acquisition.</li>
  <li>Lack of elastic resource scaling ability. It is hard to estimate the amount of reasonable resources beforehand. For example, if you’re running a iterative workload in which data will be inflated gradually through iteration, a pre-esitimated resource amount is not a good choice.</li>
</ol>

<p>So according to problems mentioned above, instead of pre-acquisiton of resources, is it possible to acquire and release the resources in the runtime according to the load of current application?</p>

<p>The <strong>Dynamic Executor Allocation</strong> is introduced to handle such issues, it is firstly brought in Spark 1.2, through the evolution and refinement of codes, now this functionality is more mature and robust.</p>

<h2 id="to-enable-dynamic-allocation-mechanism">To Enable Dynamic Allocation Mechanism</h2>

<p>To enable this function, you have to configure <strong>spark.dynamicAllocation.enabled</strong> to <strong>true</strong>, also enable <strong>spark.shuffle.service.enabled</strong>, because now executors will by dynamically added or removed, so external shuffle service should be enabled for shuffle data transmission.</p>

<p>Similar to the configuration of MRv2 external shuffle service, user should add *spark-<version>-yarn-shuffle.jar* to the classpath of NodeManager, besides below configurations should be added to yarn-site.xml:</version></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
  &lt;value&gt;spark_shuffle&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;
  &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;
&lt;/property&gt;
</code></pre></div></div>

<blockquote>
  <p>Note: Spark on Yarn configuration <code class="language-plaintext highlighter-rouge">--num-executors</code> is not worked when dynamic allocation is enabled, you don’t need to specify this.</p>
</blockquote>

<p>Besides, there has several configurations for you to fine-grained control the behavior of dynamic allocation, below is the configurations pasted from Spark official document.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Property Name</th>
      <th style="text-align: center">Default</th>
      <th style="text-align: left">Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.executorIdleTimeout</td>
      <td style="text-align: center">60s</td>
      <td style="text-align: left">If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this description.`</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.cachedExecutorIdleTimeout</td>
      <td style="text-align: center">2 * executorIdleTimeout</td>
      <td style="text-align: left">If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this description.</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.initialExecutors</td>
      <td style="text-align: center">spark.dynamicAllocation.minExecutors</td>
      <td style="text-align: left">Initial number of executors to run if dynamic allocation is enabled.</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.maxExecutors</td>
      <td style="text-align: center">Integer.MAX_VALUE</td>
      <td style="text-align: left">Upper bound for the number of executors if dynamic allocation is enabled.</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.minExecutors</td>
      <td style="text-align: center">0</td>
      <td style="text-align: left">Lower bound for the number of executors if dynamic allocation is enabled.</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.schedulerBacklogTimeout</td>
      <td style="text-align: center">1s</td>
      <td style="text-align: left">If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description.</td>
    </tr>
    <tr>
      <td style="text-align: left">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</td>
      <td style="text-align: center">schedulerBacklogTimeout</td>
      <td style="text-align: left">Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests. For more detail, see this description.</td>
    </tr>
  </tbody>
</table>

<h2 id="inside-of-dynamic-allocation-mechanism">Inside of Dynamic Allocation Mechanism</h2>

<p>Compared to statically request all the resources at the start of application, dynamic allocation mechanism could request and remove the resources dynamically at run-time. So what’s the policy to request and remove the resources?</p>

<p>First we will discover the request policy of dynamic allocation mechanism.</p>

<h3 id="request-policy">Request Policy</h3>

<p>To request resources from cluster manager, basically we have to figure out two problems:</p>

<ol>
  <li>How to measure the resources currently Spark requires?</li>
  <li>When to request the resources from cluster manager?</li>
</ol>

<h4 id="resource-in-spark">Resource in Spark</h4>

<p>In Spark a resource unit is <strong>executor</strong>, executor is combined with a bunch of CPU cores and memory. Each executor is a unique resource unit requested to cluster manager. For example, if we set <code class="language-plaintext highlighter-rouge">spark.executor.cores</code> to 10 and <code class="language-plaintext highlighter-rouge">spark.executor.memory</code> to <code class="language-plaintext highlighter-rouge">10g</code>, which means this resource unit is a combination of 10 cores and 10g memory to request to cluster manager.</p>

<p>While in the Spark execution layer, the smallest running unit is <strong>Task</strong>, so how to represent tasks as resource unit and be measured and controlled in Spark? Spark defines each task occupies <code class="language-plaintext highlighter-rouge">spark.task.cups</code> number of CPU cores, so we could simply calculate the number of tasks which could simultaneously run a executor:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private val tasksPerExecutor =
  conf.getInt("spark.executor.cores", 1) / conf.getInt("spark.task.cpus", 1)
</code></pre></div></div>

<p>So now we simply connect the resource unit (executor) with execution unit (task), we could calculate the mount of resources we wanted through tasks.</p>

<p>For example if we have 100 tasks pending to run and each task occupies 1 core, also each executor has 10 cores, ideally we will need 10 executors to run all these tasks simultaneously. We treat this 10 executors as a current request to send to cluster manager.</p>

<p>So as a conclusion:</p>

<blockquote>
  <p>Spark internally bridge the resource unit (executor) and execution unit (task), and calculate the number of required resources (executors) through tasks.</p>
</blockquote>

<h4 id="how-to-calculate-the-desired-resources-executors">How to calculate the desired resources (executors)</h4>

<p>We’ve already mentioned about the resource unit in Spark and how to calculate the resources through tasks. Now we have another question, how a get a <strong>desired</strong> resource number?</p>

<p>Spark <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code> has a sophisticated algorithm to calculate the desired executor number.</p>

<ol>
  <li>
    <p>Spark calculate the maximum number of executors it requires through pending and running tasks:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> private def maxNumExecutorsNeeded(): Int = {
 	val numRunningOrPendingTasks = listener.totalPendingTasks + listener.totalRunningTasks
 	(numRunningOrPendingTasks + tasksPerExecutor - 1) / tasksPerExecutor
 }
</code></pre></div>    </div>
  </li>
  <li>
    <p>If current executor number is more than the expected number:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> // The target number exceeds the number we actually need, so stop adding new
 // executors and inform the cluster manager to cancel the extra pending requests
 val oldNumExecutorsTarget = numExecutorsTarget
 numExecutorsTarget = math.max(maxNeeded, minNumExecutors)
 numExecutorsToAdd = 1

 // If the new target has not changed, avoid sending a message to the cluster manager
 if (numExecutorsTarget &lt; oldNumExecutorsTarget) {
   client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)
   logDebug(s"Lowering target number of executors to $numExecutorsTarget (previously " +
     s"$oldNumExecutorsTarget) because not all requested executors are actually needed")
 }
 numExecutorsTarget - oldNumExecutorsTarget
</code></pre></div>    </div>

    <p>If the current executor number is more than the desired number, Spark will notify the cluster manager to cancel pending requests, since they are unneeded. For those already allocated executors, they will be ramped down to a reasonable number later through timeout mechanism.</p>
  </li>
  <li>
    <p>If current executor number cannot satisfy the desired number:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> val oldNumExecutorsTarget = numExecutorsTarget
 // There's no point in wasting time ramping up to the number of executors we already have, so
 // make sure our target is at least as much as our current allocation:
 numExecutorsTarget = math.max(numExecutorsTarget, executorIds.size)
 // Boost our target with the number to add for this round:
 numExecutorsTarget += numExecutorsToAdd
 // Ensure that our target doesn't exceed what we need at the present moment:
 numExecutorsTarget = math.min(numExecutorsTarget, maxNumExecutorsNeeded)
 // Ensure that our target fits within configured bounds:
 numExecutorsTarget = math.max(math.min(numExecutorsTarget, maxNumExecutors), minNumExecutors)

 val delta = numExecutorsTarget - oldNumExecutorsTarget

 // If our target has not changed, do not send a message
 // to the cluster manager and reset our exponential growth
 if (delta == 0) {
   numExecutorsToAdd = 1
   return 0
 }

 val addRequestAcknowledged = testing ||
   client.requestTotalExecutors(numExecutorsTarget, localityAwareTasks, hostToLocalTaskCount)
 if (addRequestAcknowledged) {
   val executorsString = "executor" + { if (delta &gt; 1) "s" else "" }
   logInfo(s"Requesting $delta new $executorsString because tasks are backlogged" +
     s" (new desired total will be $numExecutorsTarget)")
   numExecutorsToAdd = if (delta == numExecutorsToAdd) {
     numExecutorsToAdd * 2
   } else {
     1
   }
   delta
 } else {
   logWarning(
     s"Unable to reach the cluster manager to request $numExecutorsTarget total executors!")
   0
 }
</code></pre></div>    </div>

    <p>There are two things should be noted:</p>

    <ul>
      <li>The actual request is triggered when there have been pending tasks for <code class="language-plaintext highlighter-rouge">spark.dynamicAllocation.schedulerBacklogTimeout</code> seconds, and then triggered again every <code class="language-plaintext highlighter-rouge">spark.dynamicAllocation.sustainedSchedulerBacklogTimeout</code> seconds thereafter if the queue of pending tasks persists, which means resource allocation request will not be issued immediately, still need to wait the backlog time.</li>
      <li>The number of executors requested in each round increases exponentially from the previous round. For instance, an application will add 1 executor in the first round, and then 2, 4, 8 and so on in the subsequent rounds.</li>
    </ul>
  </li>
</ol>

<h4 id="when-to-request-resources-executors">When to request resources (executors)</h4>

<p>Inside the <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code>, a timer based scheduling thread will be start in each 100ms to calculate the number of resources and request to cluster manager, the code snippet is shown below:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val scheduleTask = new Runnable() {
  override def run(): Unit = {
    try {
      schedule()
    } catch {
      case ct: ControlThrowable =&gt;
        throw ct
      case t: Throwable =&gt;
        logWarning(s"Uncaught exception in thread ${Thread.currentThread().getName}", t)
    }
  }
}
executor.scheduleAtFixedRate(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)
</code></pre></div></div>

<p>and:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def schedule(): Unit = synchronized {
  val now = clock.getTimeMillis

  updateAndSyncNumExecutorsTarget(now)

  removeTimes.retain { case (executorId, expireTime) =&gt;
    val expired = now &gt;= expireTime
    if (expired) {
      initializing = false
      removeExecutor(executorId)
    }
    !expired
  }
}
</code></pre></div></div>

<p>Till now, we simply introduce the mechanism of how and when to calculate and request the desired resources. We didn’t introduce the details of how to count the number of pending tasks, running tasks and completed tasks, this is achieved by <code class="language-plaintext highlighter-rouge">SparkListener</code>, you could check the code of <code class="language-plaintext highlighter-rouge">ExecutorAllocationListener</code> inside the <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code>.</p>

<h2 id="how-yarn-supports-dynamic-allocation">How YARN supports Dynamic Allocation</h2>

<p>We’ve already introduced about how to calculate the desired resources (executor numbers), now we have to issue these resource requests to the cluster manager to allocate/deallocate the resources. Here we will introduce how YARN support resource allocation and deallocation.</p>

<p>In the YARN side, resource unit is <strong>container</strong>, container is a resource unit combined by a bunch of CPU cores, memory. Here in Spark we assume each executor is equal to a container, so the allocation of executors now becomes allocation of containers in YARN side.</p>

<h3 id="communicate-with-application-master">Communicate with Application Master</h3>

<p>When desired executor number is calculated by <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code>, we need to communicate with ApplicationMaster about the number of executors we required, then let ApplicationMaster to communicate with YARN resource manager to request the containers. Here I will simply show the overall process:</p>

<ol>
  <li>Call <code class="language-plaintext highlighter-rouge">SparkContext#requestTotalExecutors</code> to update the number of executors.</li>
  <li><code class="language-plaintext highlighter-rouge">SparkContext#requestTotalExecutors</code> will call <code class="language-plaintext highlighter-rouge">CoarseGrainedSchedulerBackend#requestTotalExecutors</code> to update the executors.</li>
  <li><code class="language-plaintext highlighter-rouge">CoarseGrainedSchedulerBackend#requestTotalExecutors</code> will call <code class="language-plaintext highlighter-rouge">YarnSchedulerBackend#doRequestTotalExecutors</code> to send the request to AM.</li>
  <li>AM will get the current total number of executors required and pass it to <code class="language-plaintext highlighter-rouge">YarnAllocator#requestTotalExecutorsWithPreferredLocalities</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">YarnAllocator</code> will further allocate or deallocate the containers based on the current number of running and pending containers.</li>
</ol>

<h3 id="container-allocation">Container Allocation</h3>

<p>When current container number is less than the desired number, we need to increase the number of container by requesting to ResourceManager, Spark on Yarn did this in <code class="language-plaintext highlighter-rouge">YarnAllocator#updateResourceRequests</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val numPendingAllocate = getNumPendingAllocate
val missing = targetNumExecutors - numPendingAllocate - numExecutorsRunning

if (missing &gt; 0) {
  logInfo(s"Will request $missing executor containers, each with ${resource.getVirtualCores} " +
    s"cores and ${resource.getMemory} MB memory including $memoryOverhead MB overhead")

  val containerLocalityPreferences = containerPlacementStrategy.localityOfRequestedContainers(
    missing, numLocalityAwareTasks, hostToLocalTaskCounts, allocatedHostToContainersMap)

  for (locality &lt;- containerLocalityPreferences) {
    val request = createContainerRequest(resource, locality.nodes, locality.racks)
    amClient.addContainerRequest(request)
    val nodes = request.getNodes
    val hostStr = if (nodes == null || nodes.isEmpty) "Any" else nodes.last
    logInfo(s"Container request (host: $hostStr, capability: $resource)")
  }
}
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">missing</code> is the actual container number we wanted, if <code class="language-plaintext highlighter-rouge">missing &gt; 0</code> which means current number of containers is not enough to satisfy the need, more containers need to be allocated. we will calculate the locality of containers and send the container requests to resource manager through:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>amClient.addContainerRequest(request)
</code></pre></div></div>

<p>The details of container locality calculation algorithm can be found here in <code class="language-plaintext highlighter-rouge">LocalityPreferredContainerPlacementStrategy</code>.</p>

<h3 id="container-deallocation">Container Deallocation</h3>

<p>When current container number is more than the desired number, we need to decrease the number of containers by cancelling the requests, here in Spark on Yarn, we did this by cancelling the pending container requests and killing the running containers explicitly.</p>

<p><strong>Cancel the pending container requests</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val numPendingAllocate = getNumPendingAllocate
val missing = targetNumExecutors - numPendingAllocate - numExecutorsRunning

if (missing &gt; 0) {
	...
} else if (missing &lt; 0) {
  val numToCancel = math.min(numPendingAllocate, -missing)
  logInfo(s"Canceling requests for $numToCancel executor containers")

  val matchingRequests = amClient.getMatchingRequests(RM_REQUEST_PRIORITY, ANY_HOST, resource)
  if (!matchingRequests.isEmpty) {
    matchingRequests.head.take(numToCancel).foreach(amClient.removeContainerRequest)
  } else {
    logWarning("Expected to find pending requests, but found none.")
  }
}
</code></pre></div></div>

<p>** Kill the running containers **</p>

<p>When current container is idle for a while, which means there’s no tasks running on the executors for a period of time, <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code> will explicitly ramp down the number of containers by killing them.</p>

<p>The code in <code class="language-plaintext highlighter-rouge">ExecutorAllocationManager</code> can be seen in:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def schedule(): Unit = synchronized {
  val now = clock.getTimeMillis

  updateAndSyncNumExecutorsTarget(now)

  removeTimes.retain { case (executorId, expireTime) =&gt;
    val expired = now &gt;= expireTime
    if (expired) {
      initializing = false
      removeExecutor(executorId)
    }
    !expired
  }
}
</code></pre></div></div>

<p>This killing request will be transmitted to Spark application master, when application master receives this message, it will explicitly kill the executors by calling <code class="language-plaintext highlighter-rouge">killExecutor</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/**
 * Request that the ResourceManager release the container running the specified executor.
 */
def killExecutor(executorId: String): Unit = synchronized {
  if (executorIdToContainer.contains(executorId)) {
    val container = executorIdToContainer.remove(executorId).get
    containerIdToExecutorId.remove(container.getId)
    internalReleaseContainer(container)
    numExecutorsRunning -= 1
  } else {
    logWarning(s"Attempted to kill unknown executor $executorId!")
  }
}
</code></pre></div></div>

<p>Internally it will kill the containers by AMRMClient:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def internalReleaseContainer(container: Container): Unit = {
  releasedContainers.add(container.getId())
  amClient.releaseAssignedContainer(container.getId())
}
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>Until now we simply investigate the mechanism of dynamic executor allocation. Currently dynamic executor allocation is no only for Yarn, but also can run in Standalone and Mesos mode. more features will be added into this mechanism to make it more robust. For better resource utilization and multi-tenancy sharing, dynamic executor allocation is an important feature for everyone to try.</p>
:ET