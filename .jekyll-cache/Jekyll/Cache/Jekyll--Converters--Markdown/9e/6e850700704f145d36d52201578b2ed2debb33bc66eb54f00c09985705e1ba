I"¯M<p>éšç€big dataçš„å‘å±•ï¼Œäººä»¬å¯¹å¤§æ•°æ®çš„å¤„ç†è¦æ±‚ä¹Ÿè¶Šæ¥è¶Šé«˜ï¼Œä¼ ç»Ÿçš„MapReduceç­‰æ‰¹å¤„ç†æ¡†æ¶åœ¨æŸäº›ç‰¹å®šé¢†åŸŸ(å¦‚å®æ—¶ç”¨æˆ·æ¨èï¼Œç”¨æˆ·è¡Œä¸ºåˆ†æ)å·²ç»æ— æ³•æ»¡è¶³äººä»¬å¯¹å®æ—¶æ€§çš„éœ€æ±‚ã€‚å› æ­¤è¯ç”Ÿäº†ä¸€æ‰¹å¦‚<a href="http://incubator.apache.org/s4/"><strong>S4</strong></a>ï¼Œ<a href="http://storm-project.net/"><strong>Storm</strong></a>è¿™æ ·çš„æµå¼çš„ã€å®æ—¶çš„è®¡ç®—æ¡†æ¶ã€‚æœ¬æ–‡ä»‹ç»çš„<a href="http://spark-project.org/docs/latest/streaming-programming-guide.html"><strong>Spark Streaming</strong></a>ä¹Ÿæ­£æ˜¯ä¸€ä¸ªè¿™æ ·çš„æµå¼è®¡ç®—æ¡†æ¶ã€‚</p>

<h1 id="what-is-spark-streaming">What is Spark Streaming</h1>

<p>ä½œä¸ºUC Berkeleyäº‘è®¡ç®—software stackçš„ä¸€éƒ¨åˆ†ï¼ŒSpark Streamingæ˜¯å»ºç«‹åœ¨Sparkä¸Šçš„åº”ç”¨æ¡†æ¶ï¼Œåˆ©ç”¨Sparkçš„åº•å±‚æ¡†æ¶ä½œä¸ºå…¶æ‰§è¡ŒåŸºç¡€ï¼Œå¹¶åœ¨å…¶ä¸Šæ„å»ºäº†<code class="language-plaintext highlighter-rouge">DStream</code>çš„è¡Œä¸ºæŠ½è±¡ã€‚åˆ©ç”¨<code class="language-plaintext highlighter-rouge">DStream</code>æ‰€æä¾›çš„apiï¼Œç”¨æˆ·å¯ä»¥åœ¨æ•°æ®æµä¸Šå®æ—¶è¿›è¡Œcountï¼Œjoinï¼Œaggregateç­‰æ“ä½œã€‚</p>

<blockquote>
  <p>A Spark Streaming application is very similar to a Spark application; it consists of a driver program that runs the userâ€™s main function and continuous executes various parallel operations on input streams of data. The main abstraction Spark Streaming provides is a discretized stream (DStream), which is a continuous sequence of RDDs (distributed collections of elements) representing a continuous stream of data. DStreams can be created from live incoming data (such as data from a socket, Kafka, etc.) or can be generated by transformong existing DStreams using parallel operators like map, reduce, and window.</p>
</blockquote>

<h1 id="how-to-use-spark-streaming">How to Use Spark Streaming</h1>

<p>ä½œä¸ºæ„å»ºäºSparkä¹‹ä¸Šçš„åº”ç”¨æ¡†æ¶ï¼ŒSpark Streamingæ‰¿è¢­äº†Sparkçš„ç¼–ç¨‹é£æ ¼ï¼Œå¯¹äºäº†è§£Sparkçš„ç”¨æˆ·æ¥è¯´èƒ½å¤Ÿå¿«é€Ÿåœ°ä¸Šæ‰‹ã€‚æ¥ä¸‹æ¥ä»¥word countä¸ºä¾‹æ¥ä»‹ç»Spark Streamingçš„ä½¿ç”¨æ–¹å¼:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import spark.streaming.{Seconds, StreamingContext}
import spark.streaming.StreamingContext._
...

// Create the context and set up a network input stream to receive from a host:port
val ssc = new StreamingContext(args(0), "NetworkWordCount", Seconds(1))
val lines = ssc.socketTextStream(args(1), args(2).toInt)

// Split the lines into words, count them, and print some of the counts on the master
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)
wordCounts.print()

// Start the computation
ssc.start()
</code></pre></div></div>

<ol>
  <li>
    <p>åˆ›å»º<code class="language-plaintext highlighter-rouge">StreamingContext</code>å¯¹è±¡</p>

    <p>åŒSparkåˆå§‹éœ€è¦åˆ›å»º<code class="language-plaintext highlighter-rouge">SparkContext</code>å¯¹è±¡ä¸€æ ·ï¼Œä½¿ç”¨Spark Streamingå°±éœ€è¦åˆ›å»º<code class="language-plaintext highlighter-rouge">StreamingContext</code>å¯¹è±¡ã€‚åˆ›å»º<code class="language-plaintext highlighter-rouge">StreamingContext</code>å¯¹è±¡æ‰€éœ€çš„å‚æ•°ä¸<code class="language-plaintext highlighter-rouge">SparkContext</code>åŸºæœ¬ä¸€è‡´ï¼ŒåŒ…æ‹¬æŒ‡æ˜<code class="language-plaintext highlighter-rouge">master</code>ï¼Œè®¾å®šåç§°(å¦‚<code class="language-plaintext highlighter-rouge">NetworkWordCount</code>)ã€‚éœ€è¦æ³¨æ„çš„æ˜¯å‚æ•°<code class="language-plaintext highlighter-rouge">Seconds(1)</code>ï¼ŒSpark Streamingéœ€è¦æŒ‡å®šå¤„ç†æ•°æ®çš„æ—¶é—´é—´éš”ï¼Œå¦‚ä¸Šä¾‹æ‰€ç¤ºçš„<code class="language-plaintext highlighter-rouge">1s</code>ï¼Œé‚£ä¹ˆSpark Streamingä¼šä»¥<code class="language-plaintext highlighter-rouge">1s</code>ä¸ºæ—¶é—´çª—å£è¿›è¡Œæ•°æ®å¤„ç†ã€‚æ­¤å‚æ•°éœ€è¦æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚å’Œé›†ç¾¤çš„å¤„ç†èƒ½åŠ›è¿›è¡Œé€‚å½“çš„è®¾ç½®ã€‚</p>
  </li>
  <li>
    <p>åˆ›å»º<code class="language-plaintext highlighter-rouge">InputDStream</code></p>

    <p>å¦‚åŒStormçš„<code class="language-plaintext highlighter-rouge">Spout</code>ï¼ŒSpark Streamingéœ€è¦æŒ‡æ˜æ•°æ®æºã€‚å¦‚ä¸Šä¾‹æ‰€ç¤ºçš„<code class="language-plaintext highlighter-rouge">socketTextStream</code>ï¼ŒSpark Streamingä»¥socketè¿æ¥ä½œä¸ºæ•°æ®æºè¯»å–æ•°æ®ã€‚å½“ç„¶Spark Streamingæ”¯æŒå¤šç§ä¸åŒçš„æ•°æ®æºï¼ŒåŒ…æ‹¬<code class="language-plaintext highlighter-rouge">kafkaStream</code>ï¼Œ<code class="language-plaintext highlighter-rouge">flumeStream</code>ï¼Œ<code class="language-plaintext highlighter-rouge">fileStream</code>ï¼Œ	<code class="language-plaintext highlighter-rouge">networkStream</code>ç­‰ã€‚</p>
  </li>
  <li>
    <p>æ“ä½œ<code class="language-plaintext highlighter-rouge">DStream</code></p>

    <p>å¯¹äºä»æ•°æ®æºå¾—åˆ°çš„<code class="language-plaintext highlighter-rouge">DStream</code>ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œå„ç§æ“ä½œï¼Œå¦‚ä¸Šä¾‹æ‰€ç¤ºçš„æ“ä½œå°±æ˜¯ä¸€ä¸ªå…¸å‹çš„word countæ‰§è¡Œæµç¨‹ï¼šå¯¹äºå½“å‰æ—¶é—´çª—å£å†…ä»æ•°æ®æºå¾—åˆ°çš„æ•°æ®é¦–å…ˆè¿›è¡Œåˆ†å‰²ï¼Œç„¶ååˆ©ç”¨MapReduceç®—æ³•æ˜ å°„å’Œè®¡ç®—ï¼Œå½“ç„¶æœ€åè¿˜æœ‰<code class="language-plaintext highlighter-rouge">print()</code>è¾“å‡ºç»“æœã€‚</p>
  </li>
  <li>
    <p>å¯åŠ¨Spark Streaming</p>

    <p>ä¹‹å‰æ‰€ä½œçš„æ‰€æœ‰æ­¥éª¤åªæ˜¯åˆ›å»ºäº†æ‰§è¡Œæµç¨‹ï¼Œç¨‹åºæ²¡æœ‰çœŸæ­£è¿æ¥ä¸Šæ•°æ®æºï¼Œä¹Ÿæ²¡æœ‰å¯¹æ•°æ®è¿›è¡Œä»»ä½•æ“ä½œï¼Œåªæ˜¯è®¾å®šå¥½äº†æ‰€æœ‰çš„æ‰§è¡Œè®¡åˆ’ï¼Œå½“<code class="language-plaintext highlighter-rouge">ssc.start()</code>å¯åŠ¨åç¨‹åºæ‰çœŸæ­£è¿›è¡Œæ‰€æœ‰é¢„æœŸçš„æ“ä½œã€‚</p>
  </li>
</ol>

<p>è‡³æ­¤å¯¹äºSpark Streamingçš„å¦‚ä½•ä½¿ç”¨æœ‰äº†ä¸€ä¸ªå¤§æ¦‚çš„å°è±¡ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥æ¢ç©¶ä¸€ä¸‹Spark StreamingèƒŒåçš„ä»£ç ã€‚</p>

<hr />
<h1 id="spark-streaming-æºç åˆ†æ">Spark Streaming æºç åˆ†æ#</h1>

<h2 id="streamingcontext"><code class="language-plaintext highlighter-rouge">StreamingContext</code></h2>

<p>Spark Streamingä½¿ç”¨<code class="language-plaintext highlighter-rouge">StreamingContext</code>æä¾›å¯¹å¤–æ¥å£ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨<code class="language-plaintext highlighter-rouge">StreamingContext</code>æä¾›çš„apiæ¥æ„å»ºè‡ªå·±çš„Spark Streamingåº”ç”¨ç¨‹åºã€‚</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>å†…éƒ¨ç»´æŠ¤<code class="language-plaintext highlighter-rouge">SparkContext</code>å®ä¾‹ï¼Œé€šè¿‡<code class="language-plaintext highlighter-rouge">SparkContext</code>è¿›è¡Œ<code class="language-plaintext highlighter-rouge">RDD</code>çš„æ“ä½œã€‚</li>
  <li>åœ¨å®ä¾‹åŒ–<code class="language-plaintext highlighter-rouge">StreamingContext</code>æ—¶éœ€è¦æŒ‡å®š<code class="language-plaintext highlighter-rouge">batchDuration</code>,ç”¨æ¥æŒ‡ç¤ºSpark Streaming recurring jobçš„é‡å¤æ—¶é—´ã€‚</li>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>æä¾›äº†å¤šç§ä¸åŒçš„æ¥å£ï¼Œå¯ä»¥ä»å¤šç§æ•°æ®æºåˆ›å»º<code class="language-plaintext highlighter-rouge">DStream</code>ã€‚</li>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>æä¾›äº†èµ·åœstreaming jobçš„apiã€‚</li>
</ul>

<h2 id="dstream"><code class="language-plaintext highlighter-rouge">DStream</code></h2>

<p>Spark Streamingæ˜¯å»ºç«‹åœ¨SparkåŸºç¡€ä¸Šçš„ï¼Œå®ƒå°è£…äº†Sparkçš„<code class="language-plaintext highlighter-rouge">RDD</code>å¹¶åœ¨å…¶ä¸ŠæŠ½è±¡äº†æµå¼çš„æ•°æ®è¡¨ç°å½¢å¼<code class="language-plaintext highlighter-rouge">DStream</code>ï¼š</p>

<blockquote>
  <p>A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous sequence of RDDs (of the same type) representing a continuous stream of data. DStreams can either be created from live data (such as, data from HDFS, Kafka or Flume) or it can be generated by transformation existing DStreams using operations such as <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">window</code> and <code class="language-plaintext highlighter-rouge">reduceByKeyAndWindow</code>. While a Spark Streaming program is running, each DStream periodically generates a RDD, either from live data or by transforming the RDD generated by a parent DStream.</p>
</blockquote>

<p><img src="/img/2013-04-02-spark-streaming-introduction/dstream_hierarchy.png" alt="DStream Class Hierarchy" width="640" /></p>

<p><code class="language-plaintext highlighter-rouge">DStream</code>å†…éƒ¨ä¸»è¦ç»“æ„å¦‚ä¸‹æ‰€ç¤º:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>abstract class DStream[T: ClassManifest] (
    @transient protected[streaming] var ssc: StreamingContext
  		) extends Serializable with Logging {
</code></pre></div></div>

  	  initLogging()

  	  // =======================================================================
  	  // Methods that should be implemented by subclasses of DStream
  	  // =======================================================================

  	  /** Time interval after which the DStream generates a RDD */
  	  def slideDuration: Duration

  	  /** List of parent DStreams on which this DStream depends on */
  	  def dependencies: List[DStream[_]]

  	  /** Method that generates a RDD for the given time */
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  /** DStreamçš„æ ¸å¿ƒå‡½æ•°ï¼Œæ¯ä¸€ä¸ªç»§æ‰¿äºæ­¤çš„å­ç±»éƒ½éœ€è¦å®ç°æ­¤compute()å‡½æ•°ã€‚è€Œæ ¹æ®ä¸åŒçš„
      DStreamï¼Œ compute()å‡½æ•°éƒ½éœ€è¦å®ç°å…¶ç‰¹å®šåŠŸèƒ½ï¼Œè€Œè®¡ç®—çš„ç»“æœåˆ™æ˜¯è¿”å›è®¡ç®—å¥½çš„RDD*/
  	  def compute (validTime: Time): Option[RDD[T]]

  // =======================================================================
  	  // Methods and fields available on all DStreams
  	  // =======================================================================
</code></pre></div></div>

  	  // RDDs generated, marked as protected[streaming] so that testsuites can access it
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  /** æ¯ä¸€ä¸ªDStreamå†…éƒ¨ç»´æŠ¤çš„RDD HashMapï¼ŒDStreamæœ¬è´¨ä¸Šå°è£…äº†ä¸€ç»„ä»¥Timeä¸ºkeyçš„RDDï¼Œè€Œå¯¹äº
      DStreamçš„å„ç§æ“ä½œåœ¨å†…éƒ¨æ˜ å°„ä¸ºå¯¹RDDçš„æ“ä½œ */
  	  @transient
  	  protected[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()
</code></pre></div></div>

  	  // Time zero for the DStream
  	  protected[streaming] var zeroTime: Time = null

  	  // Duration for which the DStream will remember each RDD created
  	  protected[streaming] var rememberDuration: Duration = null

  	  // Storage level of the RDDs in the stream
  	  protected[streaming] var storageLevel: StorageLevel = StorageLevel.NONE

  	  // Checkpoint details
  	  protected[streaming] val mustCheckpoint = false
  	  protected[streaming] var checkpointDuration: Duration = null
  	  protected[streaming] val checkpointData = new DStreamCheckpointData(this)

  	  // Reference to whole DStream graph
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  /** æ‰€æœ‰çš„DStreaméƒ½æ³¨å†Œåˆ°DStreamGraphä¸­ï¼Œè°ƒç”¨DStreamGraphæ¥æ‰§è¡Œæ‰€æœ‰çš„DStreamå’Œæ‰€æœ‰çš„dependencies */
  	  protected[streaming] var graph: DStreamGraph = null
</code></pre></div></div>

  	  protected[streaming] def isInitialized = (zeroTime != null)

  	  // Duration for which the DStream requires its parent DStream to remember each RDD created
  	  protected[streaming] def parentRememberDuration = rememberDuration

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ...
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">DStream</code>åœ¨å†…éƒ¨ç»´æŠ¤äº†ä¸€ç»„æ—¶é—´åºåˆ—çš„<code class="language-plaintext highlighter-rouge">RDD</code>ï¼Œå¯¹äº<code class="language-plaintext highlighter-rouge">DStream</code>çš„transformationå’Œoutputåœ¨å†…éƒ¨éƒ½è½¬åŒ–ä¸ºå¯¹äº<code class="language-plaintext highlighter-rouge">RDD</code>çš„transformationå’Œoutputã€‚</p>

<p>ä¸‹é¢æ¥çœ‹ä¸€ä¸‹å¯¹äº<code class="language-plaintext highlighter-rouge">DStream</code>çš„è®¡ç®—æ˜¯å¦‚ä½•æ˜ å°„åˆ°å¯¹äº<code class="language-plaintext highlighter-rouge">RDD</code>çš„è®¡ç®—ä¸Šå»çš„ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>protected[streaming] def getOrCompute(time: Time): Option[RDD[T]] = {
  // If this DStream was not initialized (i.e., zeroTime not set), then do it
  // If RDD was already generated, then retrieve it from HashMap
  generatedRDDs.get(time) match {

    // If an RDD was already generated and is being reused, then
    // probably all RDDs in this DStream will be reused and hence should be cached
    case Some(oldRDD) =&gt; Some(oldRDD)

    // if RDD was not generated, and if the time is valid
    // (based on sliding time of this DStream), then generate the RDD
    case None =&gt; {
      if (isTimeValid(time)) {
        /** å¯¹äºæ¯ä¸€æ¬¡çš„è®¡ç®—ï¼ŒDStreamä¼šè°ƒç”¨å­ç±»æ‰€å®ç°çš„compute()å‡½æ•°æ¥è®¡ç®—äº§ç”Ÿæ–°çš„RDD */
        compute(time) match {
          case Some(newRDD) =&gt;
            if (storageLevel != StorageLevel.NONE) {
              newRDD.persist(storageLevel)
              logInfo("Persisting RDD " + newRDD.id + " for time " + time + " to " + storageLevel + " at time " + time)
            }
            if (checkpointDuration != null &amp;&amp; (time - zeroTime).isMultipleOf (checkpointDuration)) {
              newRDD.checkpoint()
              logInfo("Marking RDD " + newRDD.id + " for time " + time + " for checkpointing at time " + time)
            }
			/** æ–°äº§ç”Ÿçš„RDDä¼šæ”¾å…¥Hash Mapä¸­ */
            generatedRDDs.put(time, newRDD)
            Some(newRDD)
          case None =&gt;
            None
        }
      } else {
        None
      }
    }
  }
}
</code></pre></div></div>

<p>é€šè¿‡æ¯æ¬¡æäº¤çš„jobï¼Œè°ƒç”¨<code class="language-plaintext highlighter-rouge">getOrCompute()</code>æ¥è®¡ç®—:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>protected[streaming] def generateJob(time: Time): Option[Job] = {
 getOrCompute(time) match {
    case Some(rdd) =&gt; {
      val jobFunc = () =&gt; {
        val emptyFunc = { (iterator: Iterator[T]) =&gt; {} }
        context.sparkContext.runJob(rdd, emptyFunc)
      }
      Some(new Job(time, jobFunc))
    }
    case None =&gt; None
  }
}
</code></pre></div></div>

<h2 id="job--scheduler"><code class="language-plaintext highlighter-rouge">Job</code> &amp; <code class="language-plaintext highlighter-rouge">Scheduler</code></h2>

<p>ä»<code class="language-plaintext highlighter-rouge">DStream</code>å¯çŸ¥ï¼Œåœ¨è°ƒç”¨<code class="language-plaintext highlighter-rouge">generateJob()</code>æ—¶ï¼Œ<code class="language-plaintext highlighter-rouge">DStream</code>ä¼šé€šè¿‡<code class="language-plaintext highlighter-rouge">getOrCompute()</code>å‡½æ•°æ¥è®¡ç®—æˆ–æ˜¯è½¬æ¢<code class="language-plaintext highlighter-rouge">DStream</code>ï¼Œé‚£ä¹ˆSpark Streamingä¼šåœ¨ä½•æ—¶è°ƒç”¨<code class="language-plaintext highlighter-rouge">generateJob()</code>å‘¢?</p>

<p>åœ¨å®ä¾‹åŒ–<code class="language-plaintext highlighter-rouge">StreamingContext</code>æ—¶ï¼Œ<code class="language-plaintext highlighter-rouge">StreamingContext</code>ä¼šè¦æ±‚ç”¨æˆ·è®¾ç½®<code class="language-plaintext highlighter-rouge">batchDuration</code>ï¼Œè€Œ<code class="language-plaintext highlighter-rouge">batchDuration</code>åˆ™æŒ‡æ˜äº†recurring jobçš„é‡å¤æ—¶é—´ï¼Œåœ¨æ¯ä¸ª<code class="language-plaintext highlighter-rouge">batchDuration</code>åˆ°æ¥æ—¶éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªæ–°çš„jobæ¥è®¡ç®—<code class="language-plaintext highlighter-rouge">DStream</code>ï¼Œä»<code class="language-plaintext highlighter-rouge">Scheduler</code>çš„ä»£ç é‡Œå¯ä»¥çœ‹åˆ°ï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val clockClass = System.getProperty("spark.streaming.clock", "spark.streaming.util.SystemClock")
val clock = Class.forName(clockClass).newInstance().asInstanceOf[Clock]

/** Spark streamingåœ¨Schedulerå†…éƒ¨åˆ›å»ºäº†recurring timerï¼Œrecurring timerçš„è¶…æ—¶æ—¶é—´
    åˆ™æ˜¯ç”¨æˆ·è®¾ç½®çš„batchDurationï¼Œåœ¨è¶…æ—¶åè°ƒç”¨Schedulerçš„generateJob */
val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds,
longTime =&gt; generateJobs(new Time(longTime)))
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">generateJobs()</code>çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼Œ<code class="language-plaintext highlighter-rouge">Scheduler</code>çš„<code class="language-plaintext highlighter-rouge">generateJobs()</code>ä¼šè°ƒç”¨<code class="language-plaintext highlighter-rouge">DStreamGraph</code>çš„<code class="language-plaintext highlighter-rouge">generateJobs</code>ï¼Œå¹¶å¯¹äºæ¯ä¸€ä¸ªjobä½¿ç”¨<code class="language-plaintext highlighter-rouge">JobManager</code>æ¥run jobã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generateJobs(time: Time) {
  SparkEnv.set(ssc.env)
  logInfo("\n-----------------------------------------------------\n")
  graph.generateJobs(time).foreach(jobManager.runJob)
  latestTime = time
  doCheckpoint(time)
}
</code></pre></div></div>

<p>åœ¨<code class="language-plaintext highlighter-rouge">DStreamGraph</code>ä¸­ï¼Œ<code class="language-plaintext highlighter-rouge">generateJobs()</code>å¦‚ä¸‹æ‰€ç¤ºï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generateJobs(time: Time): Seq[Job] = {
  this.synchronized {
    logInfo("Generating jobs for time " + time)
    val jobs = outputStreams.flatMap(outputStream =&gt; outputStream.generateJob(time))
    logInfo("Generated " + jobs.length + " jobs for time " + time)
    jobs
  }
}
</code></pre></div></div>

<p>å¯¹äºæ¯ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">outputStream</code>è°ƒç”¨<code class="language-plaintext highlighter-rouge">generateJob()</code>æ¥è½¬æ¢æˆ–è®¡ç®—<code class="language-plaintext highlighter-rouge">DStream</code>ï¼Œoutputçš„è®¡ç®—ä¼šä¾èµ–äºdependecyçš„è®¡ç®—ï¼Œå› æ­¤æœ€åä¼šå¯¹æ‰€æœ‰dependencyéƒ½è¿›è¡Œè®¡ç®—ï¼Œå¾—å‡ºæœ€åçš„<code class="language-plaintext highlighter-rouge">outputStream</code>ã€‚</p>

<p>è€Œæ‰€æœ‰çš„è¿™äº›æ“ä½œï¼Œéƒ½åœ¨è°ƒç”¨<code class="language-plaintext highlighter-rouge">StreamingContext</code>çš„å¯åŠ¨å‡½æ•°åè¿›è¡Œæ‰§è¡Œã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def start() {
  if (checkpointDir != null &amp;&amp; checkpointDuration == null &amp;&amp; graph != null) {
    checkpointDuration = graph.batchDuration
  }

  validate()

  /** StreamingContextæ³¨å†Œå’Œå¯åŠ¨æ‰€æœ‰çš„input stream */
  val networkInputStreams = graph.getInputStreams().filter(s =&gt; s match {
      case n: NetworkInputDStream[_] =&gt; true
      case _ =&gt; false
    }).map(_.asInstanceOf[NetworkInputDStream[_]]).toArray

  if (networkInputStreams.length &gt; 0) {
    // Start the network input tracker (must start before receivers)
    networkInputTracker = new NetworkInputTracker(this, networkInputStreams)
    networkInputTracker.start()
  }

  Thread.sleep(1000)

  // å¯åŠ¨schedulerè¿›è¡Œstreamingçš„æ“ä½œ
  scheduler = new Scheduler(this)
  scheduler.start()
}
</code></pre></div></div>

<hr />

<p>è‡³æ­¤ï¼Œå¯¹äºSpark Streamingçš„ä½¿ç”¨å’Œå†…éƒ¨ç»“æ„åº”è¯¥æœ‰äº†ä¸€ä¸ªåŸºæœ¬çš„äº†è§£ï¼Œä»¥ä¸€å‰¯Spark Streamingå¯åŠ¨åçš„æµç¨‹å›¾æ¥ç»“æŸè¿™ç¯‡æ–‡ç« ã€‚</p>

<p><img src="/img/2013-04-02-spark-streaming-introduction/flowchart.png" alt="DStream Class Hierarchy" width="640" /></p>

<h2 id="reference">Reference</h2>

<p><a href="http://spark-project.org/docs/latest/streaming-programming-guide.html"><strong>Spark Streaming Documentation</strong></a></p>
:ET