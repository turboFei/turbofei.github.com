I"RK<p>随着big data的发展，人们对大数据的处理要求也越来越高，传统的MapReduce等批处理框架在某些特定领域(如实时用户推荐，用户行为分析)已经无法满足人们对实时性的需求。因此诞生了一批如<a href="http://incubator.apache.org/s4/"><strong>S4</strong></a>，<a href="http://storm-project.net/"><strong>Storm</strong></a>这样的流式的、实时的计算框架。本文介绍的<a href="http://spark-project.org/docs/latest/streaming-programming-guide.html"><strong>Spark Streaming</strong></a>也正是一个这样的流式计算框架。</p>

<h1 id="what-is-spark-streaming">What is Spark Streaming</h1>

<p>作为UC Berkeley云计算software stack的一部分，Spark Streaming是建立在Spark上的应用框架，利用Spark的底层框架作为其执行基础，并在其上构建了<code class="language-plaintext highlighter-rouge">DStream</code>的行为抽象。利用<code class="language-plaintext highlighter-rouge">DStream</code>所提供的api，用户可以在数据流上实时进行count，join，aggregate等操作。</p>

<blockquote>
  <p>A Spark Streaming application is very similar to a Spark application; it consists of a driver program that runs the user’s main function and continuous executes various parallel operations on input streams of data. The main abstraction Spark Streaming provides is a discretized stream (DStream), which is a continuous sequence of RDDs (distributed collections of elements) representing a continuous stream of data. DStreams can be created from live incoming data (such as data from a socket, Kafka, etc.) or can be generated by transformong existing DStreams using parallel operators like map, reduce, and window.</p>
</blockquote>

<h1 id="how-to-use-spark-streaming">How to Use Spark Streaming</h1>

<p>作为构建于Spark之上的应用框架，Spark Streaming承袭了Spark的编程风格，对于了解Spark的用户来说能够快速地上手。接下来以word count为例来介绍Spark Streaming的使用方式:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import spark.streaming.{Seconds, StreamingContext}
import spark.streaming.StreamingContext._
...

// Create the context and set up a network input stream to receive from a host:port
val ssc = new StreamingContext(args(0), "NetworkWordCount", Seconds(1))
val lines = ssc.socketTextStream(args(1), args(2).toInt)

// Split the lines into words, count them, and print some of the counts on the master
val words = lines.flatMap(_.split(" "))
val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)
wordCounts.print()

// Start the computation
ssc.start()
</code></pre></div></div>

<ol>
  <li>
    <p>创建<code class="language-plaintext highlighter-rouge">StreamingContext</code>对象</p>

    <p>同Spark初始需要创建<code class="language-plaintext highlighter-rouge">SparkContext</code>对象一样，使用Spark Streaming就需要创建<code class="language-plaintext highlighter-rouge">StreamingContext</code>对象。创建<code class="language-plaintext highlighter-rouge">StreamingContext</code>对象所需的参数与<code class="language-plaintext highlighter-rouge">SparkContext</code>基本一致，包括指明<code class="language-plaintext highlighter-rouge">master</code>，设定名称(如<code class="language-plaintext highlighter-rouge">NetworkWordCount</code>)。需要注意的是参数<code class="language-plaintext highlighter-rouge">Seconds(1)</code>，Spark Streaming需要指定处理数据的时间间隔，如上例所示的<code class="language-plaintext highlighter-rouge">1s</code>，那么Spark Streaming会以<code class="language-plaintext highlighter-rouge">1s</code>为时间窗口进行数据处理。此参数需要根据用户的需求和集群的处理能力进行适当的设置。</p>
  </li>
  <li>
    <p>创建<code class="language-plaintext highlighter-rouge">InputDStream</code></p>

    <p>如同Storm的<code class="language-plaintext highlighter-rouge">Spout</code>，Spark Streaming需要指明数据源。如上例所示的<code class="language-plaintext highlighter-rouge">socketTextStream</code>，Spark Streaming以socket连接作为数据源读取数据。当然Spark Streaming支持多种不同的数据源，包括<code class="language-plaintext highlighter-rouge">kafkaStream</code>，<code class="language-plaintext highlighter-rouge">flumeStream</code>，<code class="language-plaintext highlighter-rouge">fileStream</code>，	<code class="language-plaintext highlighter-rouge">networkStream</code>等。</p>
  </li>
  <li>
    <p>操作<code class="language-plaintext highlighter-rouge">DStream</code></p>

    <p>对于从数据源得到的<code class="language-plaintext highlighter-rouge">DStream</code>，用户可以在其基础上进行各种操作，如上例所示的操作就是一个典型的word count执行流程：对于当前时间窗口内从数据源得到的数据首先进行分割，然后利用MapReduce算法映射和计算，当然最后还有<code class="language-plaintext highlighter-rouge">print()</code>输出结果。</p>
  </li>
  <li>
    <p>启动Spark Streaming</p>

    <p>之前所作的所有步骤只是创建了执行流程，程序没有真正连接上数据源，也没有对数据进行任何操作，只是设定好了所有的执行计划，当<code class="language-plaintext highlighter-rouge">ssc.start()</code>启动后程序才真正进行所有预期的操作。</p>
  </li>
</ol>

<p>至此对于Spark Streaming的如何使用有了一个大概的印象，接下来我们来探究一下Spark Streaming背后的代码。</p>

<hr />
<h1 id="spark-streaming-源码分析">Spark Streaming 源码分析#</h1>

<h2 id="streamingcontext"><code class="language-plaintext highlighter-rouge">StreamingContext</code></h2>

<p>Spark Streaming使用<code class="language-plaintext highlighter-rouge">StreamingContext</code>提供对外接口，用户可以使用<code class="language-plaintext highlighter-rouge">StreamingContext</code>提供的api来构建自己的Spark Streaming应用程序。</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>内部维护<code class="language-plaintext highlighter-rouge">SparkContext</code>实例，通过<code class="language-plaintext highlighter-rouge">SparkContext</code>进行<code class="language-plaintext highlighter-rouge">RDD</code>的操作。</li>
  <li>在实例化<code class="language-plaintext highlighter-rouge">StreamingContext</code>时需要指定<code class="language-plaintext highlighter-rouge">batchDuration</code>,用来指示Spark Streaming recurring job的重复时间。</li>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>提供了多种不同的接口，可以从多种数据源创建<code class="language-plaintext highlighter-rouge">DStream</code>。</li>
  <li><code class="language-plaintext highlighter-rouge">StreamingContext</code>提供了起停streaming job的api。</li>
</ul>

<h2 id="dstream"><code class="language-plaintext highlighter-rouge">DStream</code></h2>

<p>Spark Streaming是建立在Spark基础上的，它封装了Spark的<code class="language-plaintext highlighter-rouge">RDD</code>并在其上抽象了流式的数据表现形式<code class="language-plaintext highlighter-rouge">DStream</code>：</p>

<blockquote>
  <p>A Discretized Stream (DStream), the basic abstraction in Spark Streaming, is a continuous sequence of RDDs (of the same type) representing a continuous stream of data. DStreams can either be created from live data (such as, data from HDFS, Kafka or Flume) or it can be generated by transformation existing DStreams using operations such as <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">window</code> and <code class="language-plaintext highlighter-rouge">reduceByKeyAndWindow</code>. While a Spark Streaming program is running, each DStream periodically generates a RDD, either from live data or by transforming the RDD generated by a parent DStream.</p>
</blockquote>

<p><img src="/img/2013-04-02-spark-streaming-introduction/dstream_hierarchy.png" alt="DStream Class Hierarchy" width="640" /></p>

<p><code class="language-plaintext highlighter-rouge">DStream</code>内部主要结构如下所示:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>abstract class DStream[T: ClassManifest] (
    @transient protected[streaming] var ssc: StreamingContext
	) extends Serializable with Logging {

  initLogging()

  // =======================================================================
  // Methods that should be implemented by subclasses of DStream
  // =======================================================================

  /** Time interval after which the DStream generates a RDD */
  def slideDuration: Duration

  /** List of parent DStreams on which this DStream depends on */
  def dependencies: List[DStream[_]]

  /** Method that generates a RDD for the given time */
  /** DStream的核心函数，每一个继承于此的子类都需要实现此compute()函数。而根据不同的
      DStream， compute()函数都需要实现其特定功能，而计算的结果则是返回计算好的RDD*/
  def compute (validTime: Time): Option[RDD[T]]

  // =======================================================================
  // Methods and fields available on all DStreams
  // =======================================================================

  // RDDs generated, marked as protected[streaming] so that testsuites can access it
  /** 每一个DStream内部维护的RDD HashMap，DStream本质上封装了一组以Time为key的RDD，而对于
      DStream的各种操作在内部映射为对RDD的操作 */
  @transient
  protected[streaming] var generatedRDDs = new HashMap[Time, RDD[T]] ()

  // Time zero for the DStream
  protected[streaming] var zeroTime: Time = null

  // Duration for which the DStream will remember each RDD created
  protected[streaming] var rememberDuration: Duration = null

  // Storage level of the RDDs in the stream
  protected[streaming] var storageLevel: StorageLevel = StorageLevel.NONE

  // Checkpoint details
  protected[streaming] val mustCheckpoint = false
  protected[streaming] var checkpointDuration: Duration = null
  protected[streaming] val checkpointData = new DStreamCheckpointData(this)

  // Reference to whole DStream graph
  /** 所有的DStream都注册到DStreamGraph中，调用DStreamGraph来执行所有的DStream和所有的dependencies */
  protected[streaming] var graph: DStreamGraph = null

  protected[streaming] def isInitialized = (zeroTime != null)

  // Duration for which the DStream requires its parent DStream to remember each RDD created
  protected[streaming] def parentRememberDuration = rememberDuration

  ...
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">DStream</code>在内部维护了一组时间序列的<code class="language-plaintext highlighter-rouge">RDD</code>，对于<code class="language-plaintext highlighter-rouge">DStream</code>的transformation和output在内部都转化为对于<code class="language-plaintext highlighter-rouge">RDD</code>的transformation和output。</p>

<p>下面来看一下对于<code class="language-plaintext highlighter-rouge">DStream</code>的计算是如何映射到对于<code class="language-plaintext highlighter-rouge">RDD</code>的计算上去的。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>protected[streaming] def getOrCompute(time: Time): Option[RDD[T]] = {
  // If this DStream was not initialized (i.e., zeroTime not set), then do it
  // If RDD was already generated, then retrieve it from HashMap
  generatedRDDs.get(time) match {

    // If an RDD was already generated and is being reused, then
    // probably all RDDs in this DStream will be reused and hence should be cached
    case Some(oldRDD) =&gt; Some(oldRDD)

    // if RDD was not generated, and if the time is valid
    // (based on sliding time of this DStream), then generate the RDD
    case None =&gt; {
      if (isTimeValid(time)) {
        /** 对于每一次的计算，DStream会调用子类所实现的compute()函数来计算产生新的RDD */
        compute(time) match {
          case Some(newRDD) =&gt;
            if (storageLevel != StorageLevel.NONE) {
              newRDD.persist(storageLevel)
              logInfo("Persisting RDD " + newRDD.id + " for time " + time + " to " + storageLevel + " at time " + time)
            }
            if (checkpointDuration != null &amp;&amp; (time - zeroTime).isMultipleOf (checkpointDuration)) {
              newRDD.checkpoint()
              logInfo("Marking RDD " + newRDD.id + " for time " + time + " for checkpointing at time " + time)
            }
			/** 新产生的RDD会放入Hash Map中 */
            generatedRDDs.put(time, newRDD)
            Some(newRDD)
          case None =&gt;
            None
        }
      } else {
        None
      }
    }
  }
}
</code></pre></div></div>

<p>通过每次提交的job，调用<code class="language-plaintext highlighter-rouge">getOrCompute()</code>来计算:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>protected[streaming] def generateJob(time: Time): Option[Job] = {
 getOrCompute(time) match {
    case Some(rdd) =&gt; {
      val jobFunc = () =&gt; {
        val emptyFunc = { (iterator: Iterator[T]) =&gt; {} }
        context.sparkContext.runJob(rdd, emptyFunc)
      }
      Some(new Job(time, jobFunc))
    }
    case None =&gt; None
  }
}
</code></pre></div></div>

<h2 id="job--scheduler"><code class="language-plaintext highlighter-rouge">Job</code> &amp; <code class="language-plaintext highlighter-rouge">Scheduler</code></h2>

<p>从<code class="language-plaintext highlighter-rouge">DStream</code>可知，在调用<code class="language-plaintext highlighter-rouge">generateJob()</code>时，<code class="language-plaintext highlighter-rouge">DStream</code>会通过<code class="language-plaintext highlighter-rouge">getOrCompute()</code>函数来计算或是转换<code class="language-plaintext highlighter-rouge">DStream</code>，那么Spark Streaming会在何时调用<code class="language-plaintext highlighter-rouge">generateJob()</code>呢?</p>

<p>在实例化<code class="language-plaintext highlighter-rouge">StreamingContext</code>时，<code class="language-plaintext highlighter-rouge">StreamingContext</code>会要求用户设置<code class="language-plaintext highlighter-rouge">batchDuration</code>，而<code class="language-plaintext highlighter-rouge">batchDuration</code>则指明了recurring job的重复时间，在每个<code class="language-plaintext highlighter-rouge">batchDuration</code>到来时都会产生一个新的job来计算<code class="language-plaintext highlighter-rouge">DStream</code>，从<code class="language-plaintext highlighter-rouge">Scheduler</code>的代码里可以看到：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val clockClass = System.getProperty("spark.streaming.clock", "spark.streaming.util.SystemClock")
val clock = Class.forName(clockClass).newInstance().asInstanceOf[Clock]

/** Spark streaming在Scheduler内部创建了recurring timer，recurring timer的超时时间
    则是用户设置的batchDuration，在超时后调用Scheduler的generateJob */
val timer = new RecurringTimer(clock, ssc.graph.batchDuration.milliseconds,
longTime =&gt; generateJobs(new Time(longTime)))
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">generateJobs()</code>的代码如下所示，<code class="language-plaintext highlighter-rouge">Scheduler</code>的<code class="language-plaintext highlighter-rouge">generateJobs()</code>会调用<code class="language-plaintext highlighter-rouge">DStreamGraph</code>的<code class="language-plaintext highlighter-rouge">generateJobs</code>，并对于每一个job使用<code class="language-plaintext highlighter-rouge">JobManager</code>来run job。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generateJobs(time: Time) {
  SparkEnv.set(ssc.env)
  logInfo("\n-----------------------------------------------------\n")
  graph.generateJobs(time).foreach(jobManager.runJob)
  latestTime = time
  doCheckpoint(time)
}
</code></pre></div></div>

<p>在<code class="language-plaintext highlighter-rouge">DStreamGraph</code>中，<code class="language-plaintext highlighter-rouge">generateJobs()</code>如下所示：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generateJobs(time: Time): Seq[Job] = {
  this.synchronized {
    logInfo("Generating jobs for time " + time)
    val jobs = outputStreams.flatMap(outputStream =&gt; outputStream.generateJob(time))
    logInfo("Generated " + jobs.length + " jobs for time " + time)
    jobs
  }
}
</code></pre></div></div>

<p>对于每一个<code class="language-plaintext highlighter-rouge">outputStream</code>调用<code class="language-plaintext highlighter-rouge">generateJob()</code>来转换或计算<code class="language-plaintext highlighter-rouge">DStream</code>，output的计算会依赖于dependecy的计算，因此最后会对所有dependency都进行计算，得出最后的<code class="language-plaintext highlighter-rouge">outputStream</code>。</p>

<p>而所有的这些操作，都在调用<code class="language-plaintext highlighter-rouge">StreamingContext</code>的启动函数后进行执行。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def start() {
  if (checkpointDir != null &amp;&amp; checkpointDuration == null &amp;&amp; graph != null) {
    checkpointDuration = graph.batchDuration
  }

  validate()

  /** StreamingContext注册和启动所有的input stream */
  val networkInputStreams = graph.getInputStreams().filter(s =&gt; s match {
      case n: NetworkInputDStream[_] =&gt; true
      case _ =&gt; false
    }).map(_.asInstanceOf[NetworkInputDStream[_]]).toArray

  if (networkInputStreams.length &gt; 0) {
    // Start the network input tracker (must start before receivers)
    networkInputTracker = new NetworkInputTracker(this, networkInputStreams)
    networkInputTracker.start()
  }

  Thread.sleep(1000)

  // 启动scheduler进行streaming的操作
  scheduler = new Scheduler(this)
  scheduler.start()
}
</code></pre></div></div>

<hr />

<p>至此，对于Spark Streaming的使用和内部结构应该有了一个基本的了解，以一副Spark Streaming启动后的流程图来结束这篇文章。</p>

<p><img src="/img/2013-04-02-spark-streaming-introduction/flowchart.png" alt="DStream Class Hierarchy" width="640" /></p>

<h2 id="reference">Reference</h2>

<p><a href="http://spark-project.org/docs/latest/streaming-programming-guide.html"><strong>Spark Streaming Documentation</strong></a></p>
:ET