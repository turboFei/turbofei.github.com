I"æ<p>æœ¬æ–‡è½¬è‡ª<a href="https://github.com/jerryshao/jerryshao.github.com">Jerryshao Blog</a></p>

<h3 id="background">Background</h3>

<p>Spark Streaming has many commons compared with Spark, it abstracts DStream which based on RDD, its transformations and outputs are similar to Spark. But due to its periodically running property, some problems which may not be serious in Spark will become a big deal in Spark Streaming. This article introduce a dependency chain problem which will delay streaming job gradually and make the job crash.</p>

<h2 id="why-streaming-job-runs-gradually-slower">Why Streaming job runs gradually slower</h2>

<h3 id="problem-statement">Problem statement</h3>

<p>Spark Streaming jobâ€™s running time gradually slows while input data size is almost the same. You can see the jobâ€™s running time chart as below.</p>

<p><img src="/img/2013-05-22-streaming-troubleshooting/job_delay.png" alt="job_delay" width="480" /></p>

<p>my job is like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>newGenRdd = input.filer(...).map(...).join(...).countByValue()
oldRdd.zipPartition(..., newGenRdd)
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">newGenRdd</code> will be calculated in each batch duration, <code class="language-plaintext highlighter-rouge">oldRdd</code> is cached in Sharkâ€™s <code class="language-plaintext highlighter-rouge">memoryMetadataManager</code>. Then <code class="language-plaintext highlighter-rouge">oldRdd</code> will be zipped with <code class="language-plaintext highlighter-rouge">newGenRdd</code> to get a zipped RDD, this zipped RDD will be next roundâ€™s <code class="language-plaintext highlighter-rouge">oldRdd</code>. So in each batch duration, job runs as above code shows.</p>

<p>In my <code class="language-plaintext highlighter-rouge">zipPartition</code>, I will filter out some records which are older than a specific time, to make sure that the total record numbers in the RDD will be stable.</p>

<p>So in a common sense, while the input data size is almost the same, the jobâ€™s running time of each batch duration should be stable as expected. But as above chart shows, running time gradually increase as time passes by.</p>

<h3 id="phenomenon">Phenomenon</h3>

<ol>
  <li>
    <p>ClientDriver hostâ€™s network output grows gradually, and StandaloneExecutorBackend hostsâ€™ network input grows gradually. You can see the below network graph of the whole cluster, in which sr412 is ClientDriver and others are StandaloneExecutorBackend</p>

    <p><img src="/img/2013-05-22-streaming-troubleshooting/network.png" alt="network_report" width="640" /></p>

    <p>This graph shows that only ClientDriverâ€™s network output and StandaloneExecutorBackendsâ€™ network input increases, which indicates that in each batch duration ClientDriver transmit data to all slaves, there might exists several possibilities:</p>

    <ul>
      <li>data structure created in ClientDriver transmit to slaves for closure to use.</li>
      <li>some dependent static data structures transmit to slaves when static functions are called on slavesâ€™ closure.</li>
      <li>some control diagram transmit to slaves in Akka.</li>
    </ul>

    <p>Also the growth of network traffic should be noticed, some data structures that transmited to slaves might be cumulative.</p>
  </li>
  <li>
    <p>Serialized task size grows gradually. According to network traffic phenomenon, furtherly I dig out all the serialized task size in each jobâ€™s <code class="language-plaintext highlighter-rouge">zipPartition</code> stage, as the blow chart shows, task size gradually grows while the input data size of each batch duration is almost the same.</p>

    <p><img src="/img/2013-05-22-streaming-troubleshooting/task_size.png" alt="network_report" width="480" /></p>

    <p>Also this <code class="language-plaintext highlighter-rouge">zipPartition</code> stage running time is increased, as blow chart shows:</p>

    <p><img src="/img/2013-05-22-streaming-troubleshooting/stage_delay.png" alt="network_report" width="480" /></p>

    <p>After carefully examing my implementation in <code class="language-plaintext highlighter-rouge">zipPartition</code>, in which data structure is invariant in each batch duration job, so I think it might be the Spark framework introduced problem.</p>
  </li>
  <li>
    <p>I dig out all the dependencies of the <code class="language-plaintext highlighter-rouge">oldRdd</code> and <code class="language-plaintext highlighter-rouge">newGenRdd</code> recursively, I found that as job runs periodically, dependencies of <code class="language-plaintext highlighter-rouge">oldRdd</code> increase rapidly, while dependencies of the <code class="language-plaintext highlighter-rouge">newGenRdd</code> maintains the same, as below chart shows.</p>

    <p><img src="/img/2013-05-22-streaming-troubleshooting/dependency_number.png" alt="network_report" width="480" /></p>
  </li>
</ol>

<h3 id="reason">Reason</h3>

<p>According to the above phenomena, it is obviously that the growth of dependency chain makes job being gradually slower, by investigating Sparkâ€™s code, in each batch duration, <code class="language-plaintext highlighter-rouge">oldRdd</code> will add <code class="language-plaintext highlighter-rouge">newGenRdd</code>â€™s dependency chain to itself, after several rounds, <code class="language-plaintext highlighter-rouge">oldRdd</code>â€™s dependency chain becomes huge, serialization and deserialization which previously is trivial now becomes a time-consuming work. Taking below code as a example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var rdd = ...
for (i &lt;- 0 to 100)
  rdd = rdd.map(x =&gt; x)

rdd = rdd.cache
</code></pre></div></div>

<p>Here as you iteratively use <code class="language-plaintext highlighter-rouge">oldRdd</code> to do transformation, each iterationâ€™s dependency chain will be added to recently used rdd, lastly this <code class="language-plaintext highlighter-rouge">rdd</code> will have a long dependency chain including all the iterativeâ€™s dependency. Also thls transformation will run gradually slower.</p>

<p>So as included, the growth of dependencies makes serialization and deserialization of each task be a main burden when job runs. Also this reason can explain why task deserialization will meet stack overflow exception even job is not so complicated.</p>

<p>I also tested without <code class="language-plaintext highlighter-rouge">oldRdd</code> combined, each time <code class="language-plaintext highlighter-rouge">newGenRdd</code> will be put in Sharkâ€™s memoryMetadataManager but without zip with <code class="language-plaintext highlighter-rouge">oldRdd</code>, now the job running time becomes stable.</p>

<p>So I think for all the iterative job which will use previously calculated RDD will meet this problem. This problem will sometimes be hidden as GC problem or shuffle problem. For small iteratives this is not a big deal, but if you want to do some machine learning works that will iterate jobs for many times, this should be a problem.</p>

<p>This issue is also stated in Spark User Group:</p>

<p><a href="https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/-Cyfe3G6VwY/PFFnslzWn6AJ">Is there some way to break down the RDD dependency
chain?</a></p>

<p><a href="https://groups.google.com/forum/?fromgroups#!searchin/spark-users/dependency/spark-users/NkxcmmS-DbM/c9qvuShbHEUJ">Spark Memory
Question</a></p>

<p>One way to break down this RDD dependency chain is to write RDD to file and read it back to memory, this will clean all the dependencies of this RDD. Maybe a way to clean dependencies might also be a solution, but it is hard to implment.</p>
:ET