I"¦
<p>ç›®å½•</p>

<ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#word-count" id="markdown-toc-word-count">Word Count</a>    <ul>
      <li><a href="#ç†è®ºå‰–æ" id="markdown-toc-ç†è®ºå‰–æ">ç†è®ºå‰–æ</a></li>
      <li><a href="#æºç å‰–æ" id="markdown-toc-æºç å‰–æ">æºç å‰–æ</a></li>
      <li><a href="#æäº¤job" id="markdown-toc-æäº¤job">æäº¤job</a></li>
      <li><a href="#åˆ’åˆ†stage" id="markdown-toc-åˆ’åˆ†stage">åˆ’åˆ†stage</a></li>
      <li><a href="#æäº¤tasks" id="markdown-toc-æäº¤tasks">æäº¤tasks###</a></li>
      <li><a href="#æ‰§è¡Œtask" id="markdown-toc-æ‰§è¡Œtask">æ‰§è¡Œtask###</a>        <ul>
          <li><a href="#shufflemaptask" id="markdown-toc-shufflemaptask">ShuffleMapTask</a></li>
          <li><a href="#resulttask" id="markdown-toc-resulttask">ResultTask####</a></li>
          <li><a href="#rdd-è¿­ä»£é“¾" id="markdown-toc-rdd-è¿­ä»£é“¾">rdd è¿­ä»£é“¾</a></li>
          <li><a href="#æ£€æŸ¥ç‚¹" id="markdown-toc-æ£€æŸ¥ç‚¹">æ£€æŸ¥ç‚¹####</a></li>
        </ul>
      </li>
      <li><a href="#compute-é“¾" id="markdown-toc-compute-é“¾">compute é“¾</a></li>
    </ul>
  </li>
  <li><a href="#å‚è€ƒ" id="markdown-toc-å‚è€ƒ">å‚è€ƒ##</a></li>
</ul>

<h3 id="background">Background</h3>
<p>ä»æœ€ç®€å•çš„sparkåº”ç”¨WordCountå…¥æ‰‹ï¼Œåˆ†ærddé“¾ï¼Œåˆ†æjobå¦‚ä½•æäº¤ï¼Œtaskå¦‚ä½•æäº¤ï¼Œä»å…¨å±€äº†è§£sparkåº”ç”¨çš„æ‰§è¡Œæµç¨‹ã€‚</p>

<h2 id="word-count">Word Count</h2>

<p>word countæ˜¯spark æœ€åŸºæœ¬çš„å°ç¨‹åºï¼Œä¸»è¦åŠŸèƒ½å°±æ˜¯ç»Ÿè®¡ä¸€ä¸ªæ–‡ä»¶é‡Œé¢å„ä¸ªå•è¯å‡ºç°çš„ä¸ªæ•°ã€‚ä»£ç å¾ˆç®€æ´ï¼Œå¦‚ä¸‹ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import org.apache.spark.{SparkConf, SparkContext}

object SparkWC {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf()
    val sparkContext = new SparkContext(sparkConf)
    sparkContext.textFile(args(0))
          .flatMap(line =&gt; line.split(" "))
          .map(word =&gt; (word, 1))
          .reduceByKey(_ + _)
          .saveAsTextFile(args(1))
  }
}
</code></pre></div></div>

<h3 id="ç†è®ºå‰–æ">ç†è®ºå‰–æ</h3>
<p>é‡Œé¢çš„RDDé“¾ï¼Œç”¨ä»–ä»¬çš„æ“ä½œè¡¨ç¤ºï¼Œå°±æ˜¯textFile-&gt;flatMap-&gt;map-&gt;reduceBykey-&gt;saveAsTextFile.</p>

<p>sparké‡Œé¢æœ‰ä¸¤ç§æ“ä½œï¼Œ<code class="language-plaintext highlighter-rouge">action</code> å’Œ<code class="language-plaintext highlighter-rouge">transformation</code>ï¼Œå…¶ä¸­actionä¼šè§¦å‘æäº¤jobçš„æ“ä½œï¼Œtransformationä¸ä¼šè§¦å‘jobï¼Œåªæ˜¯è¿›è¡Œrddçš„è½¬æ¢ã€‚è€Œä¸åŒtransformationæ“ä½œçš„rddé“¾ä¸¤ç«¯çš„ä¾èµ–å…³ç³»ä¹Ÿä¸åŒï¼Œsparkä¸­çš„rddä¾èµ–æœ‰ä¸¤ç§ï¼Œåˆ†åˆ«æ˜¯<code class="language-plaintext highlighter-rouge">narrow dependency</code> å’Œ <code class="language-plaintext highlighter-rouge">wide dependency</code> ,è¿™ä¸¤ç§ä¾èµ–å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚</p>

<p><img src="/imgs/spark-basic/rdd_dependency.png" alt="" /></p>

<p>å·¦è¾¹å›¾æ˜¯çª„ä¾èµ–ï¼Œå³è¾¹å›¾æ˜¯å®½ä¾èµ–ï¼Œçª„ä¾èµ–é‡Œé¢çš„partitionçš„å¯¹åº”é¡ºåºæ˜¯ä¸å˜çš„ï¼Œæ¬¾ä¾èµ–ä¼šæ¶‰åŠshuffleæ“ä½œï¼Œä¼šé€ æˆpartitionæ··æ´—ï¼Œå› æ­¤å¾€å¾€ä»¥æ¬¾ä¾èµ–åˆ’åˆ†stageã€‚åœ¨ä¸Šé¢çš„æ“ä½œä¸­ï¼ŒsaveAsTextFileæ˜¯actionï¼ŒreduceByKeyæ˜¯å®½ä¾èµ–ï¼Œå› æ­¤è¿™ä¸ªåº”ç”¨æ€»å…±æœ‰1ä¸ªjobï¼Œä¸¤ä¸ªstageï¼Œç„¶ååœ¨ä¸åŒçš„stageä¸­ä¼šæ‰§è¡Œtasksã€‚</p>

<h3 id="æºç å‰–æ">æºç å‰–æ</h3>

<p>ä»rddé“¾å¼€å§‹åˆ†æã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair =&gt; pair._2.toString).setName(path)
  }
</code></pre></div></div>

<p>textFile è¿™ä¸ªç®—å­çš„è¿”å›ç»“æœæ˜¯ä¸€ä¸ªRDDï¼Œç„¶åRDDé“¾å°±å¼€å§‹äº†ï¼Œå¯ä»¥çœ‹å‡ºæ¥ä»–è°ƒç”¨äº†ä¸€äº›æ–°çš„å‡½æ•°ï¼Œæ¯”å¦‚hadoopFileå•¥çš„ï¼Œè¿™äº›æˆ‘ä»¬éƒ½ä¸ç®¡ï¼Œå› ä¸ºä»–ä»¬éƒ½æ²¡æœ‰è§¦å‘ commitJobï¼Œæ‰€ä»¥è¿™äº›ä¸­é—´è¿‡ç¨‹æˆ‘ä»¬å°±çœç•¥ï¼Œç›´åˆ°saveAsTextFileè¿™ä¸ªactionã€‚</p>

<h3 id="æäº¤job">æäº¤job</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def saveAsTextFile(path: String): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    //
    // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit
    // Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`
    // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an
    // Ordering for `NullWritable`. That's why the compiler will generate different anonymous
    // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.
    //
    // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate
    // same bytecodes for `saveAsTextFile`.
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =&gt;
      val text = new Text()
      iter.map { x =&gt;
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)
  }
  
  
  //æ¥ä¸‹æ¥è°ƒç”¨è¿™ä¸ª
  def saveAsHadoopFile[F &lt;: OutputFormat[K, V]](
      path: String)(implicit fm: ClassTag[F]): Unit = self.withScope {
    saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]])
  }
  
//çœç•¥ä¸€éƒ¨åˆ†è°ƒç”¨è¿‡ç¨‹
...
...

//æœ€åè°ƒç”¨è¿™ä¸ªå‡½æ•°
  def saveAsHadoopDataset(conf: JobConf): Unit = self.withScope {
    // Rename this as hadoopConf internally to avoid shadowing (see SPARK-2038).
    val hadoopConf = conf
    val outputFormatInstance = hadoopConf.getOutputFormat
    val keyClass = hadoopConf.getOutputKeyClass
    val valueClass = hadoopConf.getOutputValueClass
    if (outputFormatInstance == null) {
      throw new SparkException("Output format class not set")
    }
    if (keyClass == null) {
      throw new SparkException("Output key class not set")
    }
    if (valueClass == null) {
      throw new SparkException("Output value class not set")
    }
    SparkHadoopUtil.get.addCredentials(hadoopConf)

    logDebug("Saving as hadoop file of type (" + keyClass.getSimpleName + ", " +
      valueClass.getSimpleName + ")")

    if (isOutputSpecValidationEnabled) {
      // FileOutputFormat ignores the filesystem parameter
      val ignoredFs = FileSystem.get(hadoopConf)
      hadoopConf.getOutputFormat.checkOutputSpecs(ignoredFs, hadoopConf)
    }

    val writer = new SparkHadoopWriter(hadoopConf)
    writer.preSetup()

    val writeToFile = (context: TaskContext, iter: Iterator[(K, V)]) =&gt; {
      // Hadoop wants a 32-bit task attempt ID, so if ours is bigger than Int.MaxValue, roll it
      // around by taking a mod. We expect that no task will be attempted 2 billion times.
      val taskAttemptId = (context.taskAttemptId % Int.MaxValue).toInt

      val outputMetricsAndBytesWrittenCallback: Option[(OutputMetrics, () =&gt; Long)] =
        initHadoopOutputMetrics(context)

      writer.setup(context.stageId, context.partitionId, taskAttemptId)
      writer.open()
      var recordsWritten = 0L

      Utils.tryWithSafeFinallyAndFailureCallbacks {
        while (iter.hasNext) {
          val record = iter.next()
          writer.write(record._1.asInstanceOf[AnyRef], record._2.asInstanceOf[AnyRef])

          // Update bytes written metric every few records
          maybeUpdateOutputMetrics(outputMetricsAndBytesWrittenCallback, recordsWritten)
          recordsWritten += 1
        }
      }(finallyBlock = writer.close())
      writer.commit()
      outputMetricsAndBytesWrittenCallback.foreach { case (om, callback) =&gt;
        om.setBytesWritten(callback())
        om.setRecordsWritten(recordsWritten)
      }
    }

    self.context.runJob(self, writeToFile)
    writer.commitJob()
  }
</code></pre></div></div>
<p>ä¸Šé¢æ˜¯saveAstextFileçš„è°ƒç”¨è¿‡ç¨‹ï¼Œä¸­é—´çœç•¥äº†ä¸€ä¸ªå‡½æ•°ï¼Œçœ‹ä»£ç çš„æœ€åä¸¤è¡Œã€‚å¯ä»¥çœ‹å‡ºè°ƒç”¨äº†<code class="language-plaintext highlighter-rouge"> self.context.runJob()</code>å¯ä»¥çŸ¥é“è¿™é‡Œè§¦å‘äº†jobçš„æäº¤ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      resultHandler: (Int, U) =&gt; Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException("SparkContext has been shutdown")
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo("Starting job: " + callSite.shortForm)
    if (conf.getBoolean("spark.logLineage", false)) {
      logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint() //æ˜¯å¦cache rdd
  }
</code></pre></div></div>

<p>å¯ä»¥çœ‹å‡ºä¸Šé¢ä»£ç æœ‰ <code class="language-plaintext highlighter-rouge">dagScheduler.runJob</code>ï¼Œå¼€å§‹è¿›è¡Œè°ƒåº¦ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&gt; Unit,
      properties: Properties): Unit = {
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    // Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,
    // which causes concurrent SQL executions to fail if a fork-join pool is used. Note that
    // due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it's
    // safe to pass in null here. For more detail, see SPARK-13747.
    val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait]
    waiter.completionFuture.ready(Duration.Inf)(awaitPermission)
    waiter.completionFuture.value.get match {
      case scala.util.Success(_) =&gt;
        logInfo("Job %d finished: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
      case scala.util.Failure(exception) =&gt;
        logInfo("Job %d failed: %s, took %f s".format
          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))
        // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.
        val callerStackTrace = Thread.currentThread().getStackTrace.tail
        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)
        throw exception
    }
  }
</code></pre></div></div>

<p>åœ¨ dagScheduler.runJob()é‡Œé¢æœ‰ <code class="language-plaintext highlighter-rouge">submitJob</code>çš„æ“ä½œï¼Œæäº¤jobã€‚
çœ‹ä¸‹é¢<code class="language-plaintext highlighter-rouge">submitJob</code>çš„ä»£ç ã€‚</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&gt; Unit,
      properties: Properties): JobWaiter[U] = {
    // Check to make sure we are not launching a task on a partition that does not exist.
    val maxPartitions = rdd.partitions.length
    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach { p =&gt;
      throw new IllegalArgumentException(
        "Attempting to access a non-existent partition: " + p + ". " +
          "Total number of partitions: " + maxPartitions)
    }

    val jobId = nextJobId.getAndIncrement()
    if (partitions.size == 0) {
      // Return immediately if the job is running 0 tasks
      return new JobWaiter[U](this, jobId, 0, resultHandler)
    }

    assert(partitions.size &gt; 0)
    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]
    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)
    eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
    waiter
  }
</code></pre></div></div>
<p>ç„¶åeventProcessLoop.post(JobSubmitted â€¦ ç„¶åå°±æœ‰å¾ªç¯ç¨‹åºå¤„ç† è¿™ä¸ªpostã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
  case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;
    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
</code></pre></div></div>

<h3 id="åˆ’åˆ†stage">åˆ’åˆ†stage</h3>

<p>æäº¤å®Œjobä¹‹åï¼Œä¼šå¯¹stageè¿›è¡Œåˆ’åˆ†ã€‚</p>

<p><code class="language-plaintext highlighter-rouge">handleJobSubmitted</code>,å¦‚ä¸‹ä»£ç ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private[scheduler] def handleJobSubmitted(jobId: Int,
    finalRDD: RDD[_],
    func: (TaskContext, Iterator[_]) =&gt; _,
    partitions: Array[Int],
    callSite: CallSite,
    listener: JobListener,
    properties: Properties) {
  var finalStage: ResultStage = null
  try {
    // New stage creation may throw an exception if, for example, jobs are run on a
    // HadoopRDD whose underlying HDFS files have been deleted.
    finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)
  } catch {
    case e: Exception =&gt;
      logWarning("Creating new stage failed due to exception - job: " + jobId, e)
      listener.jobFailed(e)
      return
  }

  val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)
  clearCacheLocs()
  logInfo("Got job %s (%s) with %d output partitions".format(
    job.jobId, callSite.shortForm, partitions.length))
  logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
  logInfo("Parents of final stage: " + finalStage.parents)
  logInfo("Missing parents: " + getMissingParentStages(finalStage))

  val jobSubmissionTime = clock.getTimeMillis()
  jobIdToActiveJob(jobId) = job
  activeJobs += job
  finalStage.setActiveJob(job)
  val stageIds = jobIdToStageIds(jobId).toArray
  val stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))
  listenerBus.post(
    SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))
  submitStage(finalStage)

  submitWaitingStages()
}
</code></pre></div></div>

<p>è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œå…ˆæ˜¯æ‰¾åˆ°æœ€åä¸€ä¸ªstageï¼Œ finalStageï¼Œç„¶åå°±ç”ŸæˆstageIdè¿˜æœ‰stageçš„ä¸€äº›ä¿¡æ¯ï¼Œç„¶åpost å‡ºjobå¼€å§‹çš„æ¶ˆæ¯ï¼Œç„¶åæäº¤æœ€åä¸€ä¸ªstageï¼Œæœ€åä¸€è¡Œæ˜¯æäº¤ç­‰å¾…çš„stagesã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** Submits stage, but first recursively submits any missing parents. */
private def submitStage(stage: Stage) {
  val jobId = activeJobForStage(stage)
  if (jobId.isDefined) {
    logDebug("submitStage(" + stage + ")")
    if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) {
      val missing = getMissingParentStages(stage).sortBy(_.id)
      logDebug("missing: " + missing)
      if (missing.isEmpty) {
        logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
        submitMissingTasks(stage, jobId.get)
      } else {
        for (parent &lt;- missing) {
          submitStage(parent)
        }
        waitingStages += stage
      }
    }
  } else {
    abortStage(stage, "No active job for stage " + stage.id, None)
  }
}
</code></pre></div></div>

<p>è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œå°±æ˜¯é€’å½’æäº¤ä¹‹å‰éƒ½æ²¡æœ‰æäº¤çš„stageï¼Œå› ä¸ºä¹‹å‰æ˜¯æäº¤æœ€åä¸€ä¸ªstageå—ï¼Œä½†æ˜¯å‰é¢stageä¹Ÿæ²¡æ“ä½œï¼Œæ‰€ä»¥è¦ä¸æ–­åœ°æäº¤parentStageï¼Œç›´åˆ°jobçš„å¤´éƒ¨ã€‚å¦‚æœè¯´è¿™ä¸ªstageæ²¡æœ‰æœªå®Œæˆçš„parentStageï¼Œé‚£å°±ä»£è¡¨å®ƒå‰é¢éƒ½æ‰§è¡Œå®Œæ¯•ã€‚</p>

<h3 id="æäº¤tasks">æäº¤tasks###</h3>

<p>æ‰¾åˆ°æœ€å¼€å§‹è¿˜æ²¡å®Œæˆçš„stageï¼Œé‚£ä¹ˆæäº¤è¿™ä¸ªstageçš„Tasksã€‚è°ƒç”¨çš„å‡½æ•°æ˜¯<code class="language-plaintext highlighter-rouge">submitMissingTasks(stage,jobId.get)</code>.</p>

<p>ä¸‹é¢æ˜¯ è¿™ä¸ªå‡½æ•°çš„ä»£ç ï¼Œæœ‰ç‚¹é•¿ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def submitMissingTasks(stage: Stage, jobId: Int) {
  logDebug("submitMissingTasks(" + stage + ")")
  // Get our pending tasks and remember them in our pendingTasks entry
  stage.pendingPartitions.clear()

  // First figure out the indexes of partition ids to compute.
  val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()

  // Use the scheduling pool, job group, description, etc. from an ActiveJob associated
  // with this Stage
  val properties = jobIdToActiveJob(jobId).properties

  runningStages += stage
  // SparkListenerStageSubmitted should be posted before testing whether tasks are
  // serializable. If tasks are not serializable, a SparkListenerStageCompleted event
  // will be posted, which should always come after a corresponding SparkListenerStageSubmitted
  // event.
  stage match {
    case s: ShuffleMapStage =&gt;
      outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
    case s: ResultStage =&gt;
      outputCommitCoordinator.stageStart(
        stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
  }
  val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
    stage match {
      case s: ShuffleMapStage =&gt;
        partitionsToCompute.map { id =&gt; (id, getPreferredLocs(stage.rdd, id))}.toMap
      case s: ResultStage =&gt;
        val job = s.activeJob.get
        partitionsToCompute.map { id =&gt;
          val p = s.partitions(id)
          (id, getPreferredLocs(stage.rdd, p))
        }.toMap
    }
  } catch {
    case NonFatal(e) =&gt;
      stage.makeNewStageAttempt(partitionsToCompute.size)
      listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
      abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
      runningStages -= stage
      return
  }

  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)
  listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))

  // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.
  // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast
  // the serialized copy of the RDD and for each task we will deserialize it, which means each
  // task gets a different copy of the RDD. This provides stronger isolation between tasks that
  // might modify state of objects referenced in their closures. This is necessary in Hadoop
  // where the JobConf/Configuration object is not thread-safe.
  var taskBinary: Broadcast[Array[Byte]] = null
  try {
    // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
    // For ResultTask, serialize and broadcast (rdd, func).
    val taskBinaryBytes: Array[Byte] = stage match {
      case stage: ShuffleMapStage =&gt;
        JavaUtils.bufferToArray(
          closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
      case stage: ResultStage =&gt;
        JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
    }

    taskBinary = sc.broadcast(taskBinaryBytes)
  } catch {
    // In the case of a failure during serialization, abort the stage.
    case e: NotSerializableException =&gt;
      abortStage(stage, "Task not serializable: " + e.toString, Some(e))
      runningStages -= stage

      // Abort execution
      return
    case NonFatal(e) =&gt;
      abortStage(stage, s"Task serialization failed: $e\n${Utils.exceptionString(e)}", Some(e))
      runningStages -= stage
      return
  }

  val tasks: Seq[Task[_]] = try {
    stage match {
      case stage: ShuffleMapStage =&gt;
        partitionsToCompute.map { id =&gt;
          val locs = taskIdToLocations(id)
          val part = stage.rdd.partitions(id)
          new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,
            taskBinary, part, locs, stage.latestInfo.taskMetrics, properties)
        }

      case stage: ResultStage =&gt;
        val job = stage.activeJob.get
        partitionsToCompute.map { id =&gt;
          val p: Int = stage.partitions(id)
          val part = stage.rdd.partitions(p)
          val locs = taskIdToLocations(id)
          new ResultTask(stage.id, stage.latestInfo.attemptId,
            taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics)
        }
    }
  } catch {
    case NonFatal(e) =&gt;
      abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
      runningStages -= stage
      return
  }

  if (tasks.size &gt; 0) {
    logInfo("Submitting " + tasks.size + " missing tasks from " + stage + " (" + stage.rdd + ")")
    stage.pendingPartitions ++= tasks.map(_.partitionId)
    logDebug("New pending partitions: " + stage.pendingPartitions)
    taskScheduler.submitTasks(new TaskSet(
      tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))
    stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
  } else {
    // Because we posted SparkListenerStageSubmitted earlier, we should mark
    // the stage as completed here in case there are no tasks to run
    markStageAsFinished(stage, None)

    val debugString = stage match {
      case stage: ShuffleMapStage =&gt;
        s"Stage ${stage} is actually done; " +
          s"(available: ${stage.isAvailable}," +
          s"available outputs: ${stage.numAvailableOutputs}," +
          s"partitions: ${stage.numPartitions})"
      case stage : ResultStage =&gt;
        s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})"
    }
    logDebug(debugString)
  }
}
</code></pre></div></div>

<p>ä¸Šé¢çš„ä»£ç å‡ºç°äº†å¤šæ¬¡<code class="language-plaintext highlighter-rouge">ResultStage</code>å’Œ<code class="language-plaintext highlighter-rouge">ShuffleMapStage</code>ï¼Œå…ˆä»‹ç»ä¸€ä¸‹è¿™ä¸ªstageã€‚</p>

<p>å‰é¢æˆ‘ä»¬è¯´è¿‡ï¼ŒWordCountåªæœ‰ä¸€ä¸ªjobï¼Œç„¶åreduceByKeyæ˜¯shuffleæ“ä½œï¼Œä»¥è¿™ä¸ªä¸ºstageçš„è¾¹ç•Œã€‚é‚£ä¹ˆå‰é¢çš„stageå°±æ˜¯<code class="language-plaintext highlighter-rouge">ShuffleMapStage</code>ï¼Œåé¢çš„stageå°±æ˜¯<code class="language-plaintext highlighter-rouge">ResultStage</code>.å› ä¸ºå‰é¢ä¼šæœ‰shuffleæ“ä½œï¼Œè€Œåé¢æ˜¯æ•´ä¸ªjobçš„è®¡ç®—ç»“æœï¼Œæ‰€ä»¥å«ResultStage.</p>

<p><code class="language-plaintext highlighter-rouge">ResultStage</code>æ˜¯æœ‰ä¸€ä¸ªå‡½æ•°ï¼Œåº”ç”¨äºrddçš„ä¸€äº›partitionæ¥è®¡ç®—å‡ºè¿™ä¸ªactionçš„ç»“æœã€‚ä½†æœ‰äº›actionå¹¶ä¸æ˜¯åœ¨æ¯ä¸ªpartitionéƒ½æ‰§è¡Œçš„ï¼Œæ¯”å¦‚<code class="language-plaintext highlighter-rouge">first()</code>.</p>

<p>æ¥ä¸‹æ¥ä»‹ç»ä¸‹è¿™ä¸ªå‡½æ•°çš„æ‰§è¡Œæµç¨‹ã€‚</p>

<ul>
  <li>é¦–å…ˆæ˜¯è®¡ç®—å‡º <code class="language-plaintext highlighter-rouge">paritionsToCompute</code>ï¼Œå³ç”¨äºè®¡ç®—çš„partitionï¼Œæ•°æ®ã€‚</li>
  <li>ç„¶åå°±æ˜¯<code class="language-plaintext highlighter-rouge">outputCommitCoordinator.stageStart</code>,è¿™ä¸ªç±»æ˜¯ç”¨æ¥è¾“å‡ºåˆ°hdfsä¸Šçš„ï¼Œç„¶åstageStartçš„ä¸¤ä¸ªå‚æ•°ï¼Œå°±æ˜¯ç”¨äºå‘å‡ºä¿¡æ¯ï¼Œä¸¤ä¸ªå‚æ•°åˆ†åˆ«æ˜¯stageIdå’Œä»–è¦ç”¨äºè®¡ç®—çš„partitionæ•°ç›®ã€‚</li>
  <li>ç„¶åå°±æ˜¯è®¡ç®—è¿™ä¸ªstageç”¨äºè®¡ç®—çš„TaskIdå¯¹åº”çš„taskæ‰€åœ¨çš„locationã€‚å› ä¸ºTaskIdå’ŒpartitionIdæ˜¯å¯¹åº”çš„ï¼Œæ‰€ä»¥ä¹Ÿå°±æ˜¯è®¡ç®—partitionIdå¯¹åº”çš„taskLocationã€‚ç„¶åtaskLocationæ˜¯ä¸€ä¸ªhostæˆ–è€…æ˜¯ä¸€ä¸ªï¼ˆhost,executorIdï¼‰äºŒå…ƒç»„ã€‚</li>
  <li><code class="language-plaintext highlighter-rouge">stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</code>è¿™é‡Œåˆ›å»ºæ–°çš„attempt å°±æ˜¯ä»£è¡¨è¿™ä¸ªstageæ‰§è¡Œäº†å‡ æ¬¡ã€‚å› ä¸ºstageå¯èƒ½ä¼šå¤±è´¥çš„ã€‚å¦‚æœå¤±è´¥å°±è¦æ¥ç€æ‰§è¡Œï¼Œè¿™ä¸ªattemptä»0å¼€å§‹ã€‚</li>
  <li>ç„¶åå°±æ˜¯åˆ›å»ºå¹¿æ’­å˜é‡ï¼Œç„¶åbraocastã€‚å¹¿æ’­æ˜¯ç”¨äºexecutoræ¥è§£ætasksã€‚é¦–å…ˆè¦åºåˆ—åŒ–ï¼Œç»™æ¯ä¸ªtaskéƒ½ä¸€ä¸ªå®Œæ•´çš„rddï¼Œè¿™æ ·å¯ä»¥è®©taskç‹¬ç«‹æ€§æ›´å¼ºï¼Œè¿™å¯¹äºéçº¿ç¨‹å®‰å…¨æ˜¯æœ‰å¿…è¦çš„ã€‚å¯¹äºShuffleMapTaskæˆ‘ä»¬åºåˆ—åŒ–çš„æ•°æ®æ˜¯<code class="language-plaintext highlighter-rouge">(rdd,shuffleDep)</code>ï¼Œå¯¹äºresultTask,åºåˆ—åŒ–æ•°æ®ä¸º<code class="language-plaintext highlighter-rouge">(rdd,func)</code>ã€‚</li>
  <li>ç„¶åæ˜¯åˆ›å»ºtasksï¼Œå½“ç„¶Tasksåˆ†ä¸ºshuffleMapTaskå’ŒresultTaskï¼Œè¿™éƒ½æ˜¯è·Ÿstageç±»å‹å¯¹åº”çš„ã€‚è¿™é‡Œåˆ›å»ºtasksï¼Œéœ€è¦ç”¨åˆ°ä¸€ä¸ªå‚æ•°<code class="language-plaintext highlighter-rouge">stage.latestInfo.attemptId</code>,è¿™é‡Œæ˜¯å‰é¢æåˆ°çš„ã€‚</li>
  <li>åˆ›å»ºå®Œtaskså°±æ˜¯åé¢çš„<code class="language-plaintext highlighter-rouge">taskScheduler.submitTasks()</code>ï¼Œè¿™æ ·ä»»åŠ¡å°±äº¤ç”±taskSchedulerè°ƒåº¦äº†ã€‚</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def submitTasks(taskSet: TaskSet) {
  val tasks = taskSet.tasks
  logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks")
  this.synchronized {
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    val stage = taskSet.stageId
    val stageTaskSets =
      taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])
    stageTaskSets(taskSet.stageAttemptId) = manager
    val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =&gt;
      ts.taskSet != taskSet &amp;&amp; !ts.isZombie
    }
    if (conflictingTaskSet) {
      throw new IllegalStateException(s"more than one active taskSet for stage $stage:" +
        s" ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(",")}")
    }
    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)

    if (!isLocal &amp;&amp; !hasReceivedTask) {
      starvationTimer.scheduleAtFixedRate(new TimerTask() {
        override def run() {
          if (!hasLaunchedTask) {
            logWarning("Initial job has not accepted any resources; " +
              "check your cluster UI to ensure that workers are registered " +
              "and have sufficient resources")
          } else {
            this.cancel()
          }
        }
      }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)
    }
    hasReceivedTask = true
  }
  backend.reviveOffers()
}
</code></pre></div></div>

<p>è¿™æ®µä»£ç å‰é¢éƒ¨åˆ†å°±æ˜¯å…ˆåˆ›å»ºtaskManagerï¼Œç„¶ååˆ¤æ–­æ˜¯å¦æœ‰è¶…è¿‡ä¸€ä¸ªæ•°ç›®çš„taskså­˜åœ¨ï¼Œå¦‚æœå†²çªå°±æŠ¥å¼‚å¸¸ã€‚</p>

<p>ç„¶åæŠŠè¿™ä¸ªTaskSetManageråŠ å…¥<code class="language-plaintext highlighter-rouge">schedulableBuilder</code>ï¼Œè¿™ä¸ªå˜é‡åœ¨åˆå§‹åŒ–æ—¶å€™ä¼šé€‰æ‹©è°ƒåº¦ç­–ç•¥ï¼Œæ¯”å¦‚fifoå•¥çš„ï¼ŒåŠ å…¥ä¹‹åå°±ä¼šæŒ‰ç…§ç›¸åº”çš„ç­–ç•¥è¿›è¡Œè°ƒåº¦ã€‚</p>

<p>ç„¶åä¹‹åçš„åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°ï¼Œå’Œæ˜¯å¦å·²ç»æ¥æ”¶è¿‡ä»»åŠ¡ï¼Œ<code class="language-plaintext highlighter-rouge">isLocal</code>ä»£è¡¨æœ¬åœ°æ¨¡å¼ã€‚å¦‚æœéæœ¬åœ°æ¨¡å¼ï¼Œè€Œä¸”è¿˜æ²¡æ¥æ”¶åˆ°è¿‡ä»»åŠ¡ï¼Œå°±ä¼šå»ºç«‹ä¸€ä¸ªTimerTaskï¼Œç„¶åä¸€ç›´æŸ¥çœ‹æœ‰æ²¡æœ‰æ¥æ”¶åˆ°ä»»åŠ¡ï¼Œå› ä¸ºå¦‚æœæ²¡ä»»åŠ¡å°±æ˜¯ç©ºè½¬å—ã€‚</p>

<p>æœ€åbackendå°±ä¼šè®©è¿™ä¸ªtaskså”¤é†’ã€‚<code class="language-plaintext highlighter-rouge">backend.reviveOffers()</code>,è¿™é‡Œæˆ‘ä»¬çš„backendé€šå¸¸æ˜¯<code class="language-plaintext highlighter-rouge">CoarseGrainedSchedulerBackend</code>ï¼Œåœ¨æ‰§è¡ŒreviveOffersä¹‹åï¼Œ<code class="language-plaintext highlighter-rouge">driverEndpoint</code>ä¼šsendæ¶ˆæ¯ï¼Œç„¶åbackendçš„receiveå‡½æ•°ä¼šæ¥æ”¶åˆ°æ¶ˆæ¯ï¼Œç„¶åæ‰§è¡Œæ“ä½œã€‚çœ‹CoarseGrainedSchedulerBackend çš„receiveå‡½æ•°ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def receive: PartialFunction[Any, Unit] = {
...
case ReviveOffers =&gt;
  makeOffers()
...
  }
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def makeOffers() {
  // Filter out executors under killing
  val activeExecutors = executorDataMap.filterKeys(executorIsAlive)
  val workOffers = activeExecutors.map { case (id, executorData) =&gt;
    new WorkerOffer(id, executorData.executorHost, executorData.freeCores)
  }.toSeq
  launchTasks(scheduler.resourceOffers(workOffers))
}
</code></pre></div></div>

<p>ä¸Šé¢ä»£ç æ˜¾ç¤ºç­›é€‰å‡ºå­˜æ´»çš„<code class="language-plaintext highlighter-rouge">Executors</code>ï¼Œç„¶åå°±åˆ›å»ºå‡º<code class="language-plaintext highlighter-rouge">workerOffers</code>,å‚æ•°æ˜¯executorId,host,frescoers.</p>

<h3 id="æ‰§è¡Œtask">æ‰§è¡Œtask###</h3>

<p>ç„¶åå°±launchTasksã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
  for (task &lt;- tasks.flatten) {
    val serializedTask = ser.serialize(task)
    if (serializedTask.limit &gt;= maxRpcMessageSize) {
      scheduler.taskIdToTaskSetManager.get(task.taskId).foreach { taskSetMgr =&gt;
        try {
          var msg = "Serialized task %s:%d was %d bytes, which exceeds max allowed: " +
            "spark.rpc.message.maxSize (%d bytes). Consider increasing " +
            "spark.rpc.message.maxSize or using broadcast variables for large values."
          msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize)
          taskSetMgr.abort(msg)
        } catch {
          case e: Exception =&gt; logError("Exception in error callback", e)
        }
      }
    }
    else {
      val executorData = executorDataMap(task.executorId)
      executorData.freeCores -= scheduler.CPUS_PER_TASK

      logInfo(s"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: " +
        s"${executorData.executorHost}.")

      executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
    }
  }
}
</code></pre></div></div>

<p>ä¸Šé¢çš„ä»£ç æ˜¾ç¤ºå°†taskåºåˆ—åŒ–ï¼Œç„¶åæ ¹æ®task.executorId ç»™ä»–åˆ†é…executorï¼Œç„¶åå°±<code class="language-plaintext highlighter-rouge">executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))</code>.</p>

<p>è¿™é‡Œæœ‰ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">executorEndPoint</code>,ä¹‹å‰å‰é¢æœ‰driverEndPoint(å‡ºç°åœ¨backend.reviveOfferé‚£é‡Œ)ï¼Œè¿™ä¸¤ä¸ªç«¯å£çš„åŸºç±»éƒ½æ˜¯<code class="language-plaintext highlighter-rouge">RpcEndpointRef</code>ã€‚RpcEndpointRefæ˜¯RpcEndPointçš„è¿œç¨‹å¼•ç”¨ï¼Œæ˜¯çº¿ç¨‹å®‰å…¨çš„ã€‚</p>

<p>RpcEndpointæ˜¯ RPC[Remote Procedure Call ï¼šè¿œç¨‹è¿‡ç¨‹è°ƒç”¨]ä¸­å®šä¹‰äº†æ”¶åˆ°çš„æ¶ˆæ¯å°†è§¦å‘å“ªä¸ªæ–¹æ³•ã€‚</p>

<p>åŒæ—¶æ¸…æ¥šçš„é˜è¿°äº†ç”Ÿå‘½å‘¨æœŸï¼Œæ„é€ -&gt; onStart -&gt; receive* -&gt; onStop</p>

<p>è¿™é‡Œreceive* æ˜¯æŒ‡receive å’Œ receiveAndReplyã€‚</p>

<p>ä»–ä»¬çš„åŒºåˆ«æ˜¯ï¼š</p>

<p>receiveæ˜¯æ— éœ€ç­‰å¾…ç­”å¤ï¼Œè€ŒreceiveAndReplyæ˜¯ä¼šé˜»å¡çº¿ç¨‹ï¼Œç›´è‡³æœ‰ç­”å¤çš„ã€‚(å‚è€ƒï¼šhttp://www.07net01.com/2016/04/1434116.html)</p>

<p>ç„¶åè¿™é‡Œçš„driverEndPointå°±æ˜¯ä»£è¡¨è¿™ä¸ªä¿¡æ¯ä¼šå‘ç»™<code class="language-plaintext highlighter-rouge">CoarseGrainedSchedulerBackEnd</code>ï¼ŒexecutorEndPointå°±æ˜¯å‘ç»™<code class="language-plaintext highlighter-rouge">coarseGrainedExecutorBackEnd</code>å½“ç„¶å°±æ˜¯å‘ç»™<code class="language-plaintext highlighter-rouge">coarseGrainedExecutorBackEnd</code>ã€‚æ¥ä¸‹æ¥å»çœ‹ç›¸åº”çš„recieveä»£ç ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def receive: PartialFunction[Any, Unit] = {
...

    case LaunchTask(data) =&gt;
      if (executor == null) {
        exitExecutor(1, "Received LaunchTask command but executor was null")
      } else {
        val taskDesc = ser.deserialize[TaskDescription](data.value)
        logInfo("Got assigned task " + taskDesc.taskId)
        executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,
          taskDesc.name, taskDesc.serializedTask)
      }
...
}
</code></pre></div></div>

<p>è¿™é‡Œå…ˆå°†ä¼ è¿‡æ¥çš„æ•°æ®ååºåˆ—åŒ–ï¼Œç„¶å<code class="language-plaintext highlighter-rouge">executor.launchTask</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def launchTask(
    context: ExecutorBackend,
    taskId: Long,
    attemptNumber: Int,
    taskName: String,
    serializedTask: ByteBuffer): Unit = {
  val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,
    serializedTask)
  runningTasks.put(taskId, tr)
  threadPool.execute(tr)
}
</code></pre></div></div>

<p>è¿™é‡Œæ–°å»ºäº†taskRunnerï¼Œç„¶åä¹‹åäº¤ç”±çº¿ç¨‹æ± æ¥è¿è¡Œï¼Œçº¿ç¨‹æ± æ—¢ç„¶è¦è¿è¡ŒtaskRunnerï¼Œå¿…å®šæ˜¯è¿è¡ŒtaskRunnerçš„runæ–¹æ³•ã€‚çœ‹taskRunnerçš„runæ–¹æ³•ï¼Œä»£ç å¤ªé•¿ï¼Œæ‡’å¾—è´´ï¼Œå¤§æ¦‚æè¿°ä¸‹ã€‚</p>

<p>ä¸»è¦å°±æ˜¯è®¾ç½®å‚æ•°ï¼Œå±æ€§ï¼Œååºåˆ—åŒ–å‡ºtaskç­‰ç­‰ï¼Œä¹‹åå°±è¦è°ƒç”¨task.runTaskæ–¹æ³•ã€‚è¿™é‡Œçš„taskå¯èƒ½æ˜¯<code class="language-plaintext highlighter-rouge">ShuffleMapTask</code>ä¹Ÿå¯èƒ½æ˜¯<code class="language-plaintext highlighter-rouge">ResultTask</code>ï¼Œæ‰€ä»¥æˆ‘ä»¬åˆ†åˆ«çœ‹è¿™ä¸¤ç§taskçš„runæ–¹æ³•ã€‚</p>

<h4 id="shufflemaptask">ShuffleMapTask</h4>

<p>å…ˆçœ‹<code class="language-plaintext highlighter-rouge">ShuffleMapTask</code>ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def runTask(context: TaskContext): MapStatus = {
  // Deserialize the RDD using the broadcast variable.
  val deserializeStartTime = System.currentTimeMillis()
  val ser = SparkEnv.get.closureSerializer.newInstance()
  val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
    ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
  _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime

  var writer: ShuffleWriter[Any, Any] = null
  try {
    val manager = SparkEnv.get.shuffleManager
    writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
    writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]])
    writer.stop(success = true).get
  } catch {
    case e: Exception =&gt;
      try {
        if (writer != null) {
          writer.stop(success = false)
        }
      } catch {
        case e: Exception =&gt;
          log.debug("Could not stop writer", e)
      }
      throw e
  }
}
</code></pre></div></div>

<p>å‰é¢éƒ¨åˆ†ä»£ç å°±æ˜¯ååºåˆ—åŒ–é‚£äº›ï¼Œä¸»è¦çœ‹ä¸­é—´çš„ä»£ç ã€‚è·å¾—shuffleManager,ç„¶ågetWriterã€‚å› ä¸ºshuffleMapTaskæœ‰Shuffleæ“ä½œï¼Œæ‰€ä»¥è¦shuffleWriteã€‚</p>

<h4 id="resulttask">ResultTask####</h4>

<p>çœ‹ä¸‹ResultTaskçš„runTaskã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  override def runTask(context: TaskContext): U = {
    // Deserialize the RDD and the func using the broadcast variables.
    val deserializeStartTime = System.currentTimeMillis()
    val ser = SparkEnv.get.closureSerializer.newInstance()
    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](
      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime

    func(context, rdd.iterator(partition, context))
  }
</code></pre></div></div>
<p>è·Ÿé‚£ä¸ªå·®ä¸å¤šï¼Œåªä¸è¿‡ä¸æ˜¯shuffleWriteï¼Œæ˜¯func.</p>

<h4 id="rdd-è¿­ä»£é“¾">rdd è¿­ä»£é“¾</h4>

<p>çœ‹è¿™è¡Œä»£ç <code class="language-plaintext highlighter-rouge">writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]])</code>ï¼Œçœ‹writeæ–¹æ³•é‡Œé¢çš„å‚æ•°ï¼Œrdd.iteratoræ–¹æ³•ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
  if (storageLevel != StorageLevel.NONE) {
    getOrCompute(split, context)
  } else {
    computeOrReadCheckpoint(split, context)
  }
}
</code></pre></div></div>

<p>è¿™ä¸ªæ–¹æ³•ï¼Œæ˜¯ä»åé¢çš„rddå¼€å§‹è¿­ä»£ï¼Œé¦–å…ˆåˆ¤æ–­è¿™ä¸ªrddæ˜¯å¦æ˜¯å·²ç»è¢«cacheã€‚</p>

<p>å¦‚æœå·²ç»è¢«cacheï¼ŒgetOrComputeï¼Œç›´æ¥getï¼Œæˆ–è€…å¦‚æœæ²¡æ‰¾åˆ°å°±é‡ç®—ä¸€éï¼Œè¿™ä¸ªä»£ç æ¯”è¾ƒç®€å•ï¼Œæˆ‘å°±ä¸è´´äº†ã€‚</p>

<p>å¦‚æœæ²¡æœ‰è¢«cacheï¼Œåˆ™è°ƒç”¨<code class="language-plaintext highlighter-rouge">computeOrReadCheckpoint</code>ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
{
  if (isCheckpointedAndMaterialized) {
    firstParent[T].iterator(split, context)
  } else {
    compute(split, context)
  }
}
</code></pre></div></div>

<p>å¦‚æœæ˜¯æ£€æŸ¥ç‚¹ï¼Œå…ˆä»‹ç»ä¸‹æ£€æŸ¥ç‚¹ã€‚</p>

<h4 id="æ£€æŸ¥ç‚¹">æ£€æŸ¥ç‚¹####</h4>

<p>æ£€æŸ¥ç‚¹æœºåˆ¶çš„å®ç°å’ŒæŒä¹…åŒ–çš„å®ç°æœ‰ç€è¾ƒå¤§çš„åŒºåˆ«ã€‚æ£€æŸ¥ç‚¹å¹¶éç¬¬ä¸€æ¬¡è®¡ç®—å°±å°†ç»“æœè¿›è¡Œå­˜å‚¨ï¼Œè€Œæ˜¯ç­‰åˆ°ä¸€ä¸ªä½œä¸šç»“æŸåå¯åŠ¨ä¸“é—¨çš„ä¸€ä¸ªä½œä¸šå®Œæˆå­˜å‚¨çš„æ“ä½œã€‚</p>

<p>checkPointæ“ä½œçš„å®ç°åœ¨RDDç±»ä¸­ï¼Œ<em>checkPoint</em>æ–¹æ³•ä¼šå®ä¾‹åŒ–ReliableRDDCheckpointDataç”¨äºæ ‡è®°å½“å‰çš„RDD</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/**
 * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
 * directory set with `SparkContext#setCheckpointDir` and all references to its parent
 * RDDs will be removed. This function must be called before any job has been
 * executed on this RDD. It is strongly recommended that this RDD is persisted in
 * memory, otherwise saving it on a file will require recomputation.
 */
def checkpoint(): Unit = RDDCheckpointData.synchronized {
  // NOTE: we use a global lock here due to complexities downstream with ensuring
  // children RDD partitions point to the correct parent partitions. In the future
  // we should revisit this consideration.
  if (context.checkpointDir.isEmpty) {
    throw new SparkException("Checkpoint directory has not been set in the SparkContext")
  } else if (checkpointData.isEmpty) {
    checkpointData = Some(new ReliableRDDCheckpointData(this))
  }
}
</code></pre></div></div>

<p>RDDCheckpointDataç±»å†…éƒ¨æœ‰ä¸€ä¸ªæšä¸¾ç±»å‹ <code class="language-plaintext highlighter-rouge">CheckpointStateÂ </code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** 
 * Enumeration to manage state transitions of an RDD through checkpointing 
 * [ Initialized --&gt; checkpointing in progress --&gt; checkpointed ]. 
 */  
private[spark] object CheckpointState extends Enumeration {  
  type CheckpointState = Value  
  val Initialized, CheckpointingInProgress, Checkpointed = Value  
} 
</code></pre></div></div>

<p>ç”¨äºè¡¨ç¤ºRDDæ£€æŸ¥ç‚¹çš„å½“å‰çŠ¶æ€ï¼Œå…¶å€¼æœ‰Initialized ã€CheckpointingInProgressã€ checkpointedã€‚å…¶è½¬æ¢è¿‡ç¨‹å¦‚ä¸‹
(1)InitializedçŠ¶æ€</p>

<p>è¯¥çŠ¶æ€æ˜¯å®ä¾‹åŒ–ReliableRDDCheckpointDataåçš„é»˜è®¤çŠ¶æ€ï¼Œç”¨äºæ ‡è®°å½“å‰çš„RDDå·²ç»å»ºç«‹äº†æ£€æŸ¥ç‚¹(è¾ƒv1.4.xå°‘ä¸€ä¸ªMarkForCheckPiontçŠ¶æ€)</p>

<p>(2)CheckpointingInProgressçŠ¶æ€</p>

<p>æ¯ä¸ªä½œä¸šç»“æŸåéƒ½ä¼šå¯¹ä½œä¸šçš„æœ«RDDè°ƒç”¨å…¶doCheckPointæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šé¡ºç€RDDçš„å…³ç³»ä¾èµ–é“¾å¾€å‰éå†ï¼Œç›´åˆ°é‡è§å†…éƒ¨RDDCheckpointDataå¯¹è±¡è¢«æ ‡è®°ä¸ºInitializedçš„ä¸ºæ­¢ï¼Œæ­¤æ—¶å°†RDDçš„RDDCheckpointDataå¯¹è±¡æ ‡è®°ä¸ºCheckpointingInProgressï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªä½œä¸šå®Œæˆæ•°æ®çš„å†™å…¥æ“ä½œã€‚</p>

<p>(3)CheckpointedçŠ¶æ€</p>

<p>æ–°å¯åŠ¨ä½œä¸šå®Œæˆæ•°æ®å†™å…¥æ“ä½œä¹‹åï¼Œå°†å»ºç«‹æ£€æŸ¥ç‚¹çš„RDDçš„æ‰€æœ‰ä¾èµ–å…¨éƒ¨æ¸…é™¤ï¼Œå°†RDDå†…éƒ¨çš„RDDCheckpointDataå¯¹è±¡æ ‡è®°ä¸ºCheckpointedï¼Œ<code class="language-plaintext highlighter-rouge">å°†çˆ¶RDDé‡æ–°è®¾ç½®ä¸ºä¸€ä¸ªCheckPointRDDå¯¹è±¡ï¼Œçˆ¶RDDçš„computeæ–¹æ³•ä¼šç›´æ¥ä»ç³»ç»Ÿä¸­è¯»å–æ•°æ®</code>ã€‚</p>

<p>å¦‚ä¸Šåªç®€å•åœ°ä»‹ç»äº†ç›¸å…³æ¦‚å¿µï¼Œè¯¦ç»†ä»‹ç»è¯·å‚çœ‹ï¼š<a href="https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md">https://github.com/JerryLead/SparkInternals/blob/master/markdown/6-CacheAndCheckpoint.md</a></p>

<h3 id="compute-é“¾">compute é“¾</h3>
<p>ä¸Šé¢æœ‰æ£€æŸ¥ç‚¹çš„å°±ç›´æ¥å»çˆ¶Rddçš„computeè¯»å–æ•°æ®äº†ã€‚è€Œéæ£€æŸ¥ç‚¹ï¼Œå°±computeï¼Œcomputeæ˜¯ä¸€ä¸ªé“¾ã€‚
æ‹¿<code class="language-plaintext highlighter-rouge">MapPartitionsRDD</code>ä¸¾ä¸ªä¾‹å­ï¼Œçœ‹çœ‹å®ƒçš„computeæ–¹æ³•ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def compute(split: Partition, context: TaskContext): Iterator[U] =
f(context, split.index, firstParent[T].iterator(split, context))
</code></pre></div></div>

<p>çœ‹è¿™é‡Œ computeè¿˜æ˜¯è°ƒç”¨äº†iteratorï¼Œæ‰€ä»¥è¿˜æ˜¯æ¥ç€å¾€å‰æ‰¾äº†ï¼Œç›´åˆ°æ‰¾åˆ°checkpointæˆ–è€…å°±æ˜¯åˆ°rddå¤´ã€‚</p>

<p>å†çœ‹çœ‹å…¶ä»–çš„rddçš„computeæ–¹æ³•å§ã€‚</p>

<p>çœ‹çœ‹<code class="language-plaintext highlighter-rouge">ShuffleRdd</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
  val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
  SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)
    .read()
    .asInstanceOf[Iterator[(K, C)]]
}
</code></pre></div></div>

<p>ç„¶åè¿™é‡ŒshuffleRddçš„computeæ–¹æ³•å°±æ˜¯ä»shuffle é‚£é‡Œread æ•°æ®ï¼Œè¿™ç®—æ˜¯ä¸€ä¸ªstageçš„å¼€å§‹äº†ã€‚</p>

<p>å½“ç„¶ä¸€ä¸ªstageçš„å¼€å§‹æœªå¿…æ˜¯shuffleReadå¼€å§‹å•¦ï¼Œæ¯”å¦‚textFileï¼Œå®ƒæœ€ç»ˆæ˜¯è¿”å›ä¸€ä¸ªHadoopRddï¼Œç„¶åä»–çš„computeæ–¹æ³•ï¼Œå°±æ˜¯è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ã€‚</p>

<h2 id="å‚è€ƒ">å‚è€ƒ##</h2>

<p>Â <a href="http://blog.csdn.net/jiangpeng59/article/details/53213694">Sparkæ ¸å¿ƒRDDï¼šè®¡ç®—å‡½æ•°compute</a></p>

<p>Â <a href="http://blog.csdn.net/jiangpeng59/article/details/53212416">SparkåŸºç¡€éšç¬”ï¼šæŒä¹…åŒ–&amp;æ£€æŸ¥ç‚¹</a></p>

:ET