I"]/
<p>目录</p>
<ul id="markdown-toc">
  <li><a href="#前言" id="markdown-toc-前言">前言</a></li>
  <li><a href="#创建bucket-表" id="markdown-toc-创建bucket-表">创建bucket 表</a></li>
  <li><a href="#bucket表参数" id="markdown-toc-bucket表参数">Bucket表参数</a></li>
  <li><a href="#insert-into-bucket-table" id="markdown-toc-insert-into-bucket-table">Insert into bucket table</a></li>
</ul>
<p>记一次与bucket table相关的小文件issue</p>

<h3 id="前言">前言</h3>

<p>最近</p>

<p>Spark和Hive中都有bucket table，但是其格式不尽相同。本文不对此进行赘述，本文讲关于Spark的bucket表。</p>

<p>Bucket表的作用相当于一种数据预处理，如果两个bucket 表的bucket数量相同，且对两个表的bucket key进行join，那个可以避免shuffle 操作，需要数据管理者进行一定的设计。</p>

<p>本文分三部分：</p>

<ul>
  <li>bucket 表创建</li>
  <li>bucket表相关参数</li>
  <li>写入bucket表时候的小文件问题处理</li>
</ul>

<h3 id="创建bucket-表">创建bucket 表</h3>

<p>语句格式:</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">TABLE</span> <span class="p">[</span><span class="n">IF</span> <span class="k">NOT</span> <span class="k">EXISTS</span><span class="p">]</span> <span class="p">[</span><span class="n">db_name</span><span class="p">.]</span><span class="k">table_name</span>
  <span class="p">[(</span><span class="n">col_name1</span> <span class="n">col_type1</span> <span class="p">[</span><span class="k">COMMENT</span> <span class="n">col_comment1</span><span class="p">],</span> <span class="p">...)]</span>
  <span class="k">USING</span> <span class="n">data_source</span>
  <span class="p">[</span><span class="k">OPTIONS</span> <span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span> <span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="n">PARTITIONED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name1</span><span class="p">,</span> <span class="n">col_name2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="n">CLUSTERED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name3</span><span class="p">,</span> <span class="n">col_name4</span><span class="p">,</span> <span class="p">...)</span> <span class="n">SORTED</span> <span class="k">BY</span> <span class="p">(</span><span class="n">col_name1</span> <span class="k">ASC</span><span class="p">,</span> <span class="n">col_name2</span> <span class="k">DESC</span><span class="p">)</span> <span class="k">INTO</span> <span class="n">num_buckets</span> <span class="n">BUCKETS</span><span class="p">]</span>
  <span class="p">[</span><span class="k">LOCATION</span> <span class="n">path</span><span class="p">]</span>
  <span class="p">[</span><span class="k">COMMENT</span> <span class="n">table_comment</span><span class="p">]</span>
  <span class="p">[</span><span class="n">TBLPROPERTIES</span> <span class="p">(</span><span class="n">key1</span><span class="o">=</span><span class="n">val1</span><span class="p">,</span> <span class="n">key2</span><span class="o">=</span><span class="n">val2</span><span class="p">,</span> <span class="p">...)]</span>
  <span class="p">[</span><span class="k">AS</span> <span class="n">select_statement</span><span class="p">]</span>
</code></pre></div></div>

<p>Note: 创建bucket表时候用到的<code class="highlighter-rouge">clustered by (key)</code> 和 <code class="highlighter-rouge">sorted by (key)</code>，和在select 数据时候用的<code class="highlighter-rouge">cluster by key</code> 和<code class="highlighter-rouge">sort by key</code> 很相似，但是用法是不同的。</p>

<p>另外在select 时候有<code class="highlighter-rouge">distribute by</code> 和 <code class="highlighter-rouge">cluster by</code>两种语法，<code class="highlighter-rouge">cluster by key</code> = <code class="highlighter-rouge">distribute by key sort by key</code>. <code class="highlighter-rouge">distribute by key</code> = <code class="highlighter-rouge">hash shuffle by key</code>.</p>

<h3 id="bucket表参数">Bucket表参数</h3>

<p>Spark中有两个bucket表相关参数：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>spark.sql.sources.bucketing.enabled  是否将bucekt表看成是bucekt表
spark.sql.sources.bucketing.maxBuckets 允许的最大bucekt 数量，默认是100000
</code></pre></div></div>

<p>关于第一个参数，作用是否将bucket表看成是bucekt表。</p>

<p>引用前面写过的一篇文章。</p>

<blockquote>
  <p>我们并不是每次读取bucket表都是为了进行bucket join，比如说有时候我们会对这个bucket进行etl操作。</p>

  <p>如果只是单纯的对这个bucket表进行一些处理操作，例如就是一个单纯的shuffle操作。而这个bucket表的每个bucket都特别大，例如大于1个G，而在shuffle write阶段要生成3G的数据。那么这时候对每个bucket分配一个task来处理就会非常吃力。</p>

  <p>其实spark sql中有一个参数 <code class="highlighter-rouge">spark.sql.sources.bucketing.enabled</code>,默认是true。如果我们将这个参数设置为false，那么spark就会将一个bucket table看做一个普通的table。</p>

  <p>这意味着什么呢？</p>

  <p>Spark对于普通表，如果他的单个文件大于一个hdfs  block大小(通常是128M)，而且这个文件又是可拆分的(例如text文本，snappy 压缩格式的parquet文件等等)，那么spark会按照这个文件拆分，分配多个task来处理。</p>

  <p>因此，针对我们上面的场景，设置这个参数为false，可以大大的加快map阶段的执行，起到优化的效果。</p>
</blockquote>

<h3 id="insert-into-bucket-table">Insert into bucket table</h3>

<p>spark DataSource 表写入时候用到的exec是 InsertIntoHadoopFsRelationCommand.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">case</span> <span class="k">class</span> <span class="nc">InsertIntoHadoopFsRelationCommand</span><span class="o">(</span>
    <span class="n">outputPath</span><span class="k">:</span> <span class="kt">Path</span><span class="o">,</span>
    <span class="n">staticPartitions</span><span class="k">:</span> <span class="kt">TablePartitionSpec</span><span class="o">,</span>
    <span class="n">ifPartitionNotExists</span><span class="k">:</span> <span class="kt">Boolean</span><span class="o">,</span>
    <span class="n">partitionColumns</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">Attribute</span><span class="o">],</span>
    <span class="n">bucketSpec</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">BucketSpec</span><span class="o">],</span>
    <span class="n">fileFormat</span><span class="k">:</span> <span class="kt">FileFormat</span><span class="o">,</span>
    <span class="n">options</span><span class="k">:</span> <span class="kt">Map</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">String</span><span class="o">],</span>
    <span class="n">query</span><span class="k">:</span> <span class="kt">LogicalPlan</span><span class="o">,</span>
    <span class="n">mode</span><span class="k">:</span> <span class="kt">SaveMode</span><span class="o">,</span>
    <span class="n">catalogTable</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">CatalogTable</span><span class="o">],</span>
    <span class="n">fileIndex</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">FileIndex</span><span class="o">],</span>
    <span class="n">outputColumnNames</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span>
</code></pre></div></div>

<p>我们可以看到里面有bucketSpec 参数。这个参数会传入到FileFormatWriter.write函数.</p>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">FileFormatWriter</span><span class="o">.</span><span class="py">write</span><span class="o">(</span>
  <span class="n">sparkSession</span> <span class="k">=</span> <span class="n">sparkSession</span><span class="o">,</span>
  <span class="n">plan</span> <span class="k">=</span> <span class="n">child</span><span class="o">,</span>
  <span class="n">fileFormat</span> <span class="k">=</span> <span class="n">fileFormat</span><span class="o">,</span>
  <span class="n">committer</span> <span class="k">=</span> <span class="n">committer</span><span class="o">,</span>
  <span class="n">outputSpec</span> <span class="k">=</span> <span class="nv">FileFormatWriter</span><span class="o">.</span><span class="py">OutputSpec</span><span class="o">(</span>
    <span class="nv">qualifiedOutputPath</span><span class="o">.</span><span class="py">toString</span><span class="o">,</span> <span class="n">customPartitionLocations</span><span class="o">,</span> <span class="n">outputColumns</span><span class="o">),</span>
  <span class="n">hadoopConf</span> <span class="k">=</span> <span class="n">hadoopConf</span><span class="o">,</span>
  <span class="n">partitionColumns</span> <span class="k">=</span> <span class="n">partitionColumns</span><span class="o">,</span>
  <span class="n">bucketSpec</span> <span class="k">=</span> <span class="n">bucketSpec</span><span class="o">,</span>
  <span class="n">statsTrackers</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="nf">basicWriteJobStatsTracker</span><span class="o">(</span><span class="n">hadoopConf</span><span class="o">)),</span>
  <span class="n">options</span> <span class="k">=</span> <span class="n">options</span><span class="o">)</span>
</code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">bucketIdExpression</span> <span class="k">=</span> <span class="nv">bucketSpec</span><span class="o">.</span><span class="py">map</span> <span class="o">{</span> <span class="n">spec</span> <span class="k">=&gt;</span>
  <span class="k">val</span> <span class="nv">bucketColumns</span> <span class="k">=</span> <span class="nv">spec</span><span class="o">.</span><span class="py">bucketColumnNames</span><span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">c</span> <span class="k">=&gt;</span> <span class="nv">dataColumns</span><span class="o">.</span><span class="py">find</span><span class="o">(</span><span class="nv">_</span><span class="o">.</span><span class="py">name</span> <span class="o">==</span> <span class="n">c</span><span class="o">).</span><span class="py">get</span><span class="o">)</span>
  <span class="c1">// Use `HashPartitioning.partitionIdExpression` as our bucket id expression, so that we can</span>
  <span class="c1">// guarantee the data distribution is same between shuffle and bucketed data source, which</span>
  <span class="c1">// enables us to only shuffle one side when join a bucketed table and a normal one.</span>
      <span class="nc">HashPartitioning</span><span class="o">(</span><span class="n">bucketColumns</span><span class="o">,</span> <span class="nv">spec</span><span class="o">.</span><span class="py">numBuckets</span><span class="o">).</span><span class="py">partitionIdExpression</span>
    <span class="o">}</span>
</code></pre></div></div>

<p>然后如果指定了bucketSpec，就会添加一个针对bucket columns的HashPartitioning 操作。</p>

:ET