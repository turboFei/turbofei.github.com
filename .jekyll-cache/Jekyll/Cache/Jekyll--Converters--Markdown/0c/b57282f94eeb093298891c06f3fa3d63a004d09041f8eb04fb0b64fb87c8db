I"íD<h3 id="background">Background</h3>

<p>åœ¨MapReduceæ¡†æ¶ä¸­ï¼Œshuffleæ˜¯è¿æ¥Mapå’ŒReduceä¹‹é—´çš„æ¡¥æ¢ï¼ŒMapçš„è¾“å‡ºè¦ç”¨åˆ°Reduceä¸­å¿…é¡»ç»è¿‡shuffleè¿™ä¸ªç¯èŠ‚ï¼Œshuffleçš„æ€§èƒ½é«˜ä½ç›´æ¥å½±å“äº†æ•´ä¸ªç¨‹åºçš„æ€§èƒ½å’Œååé‡ã€‚Sparkä½œä¸ºMapReduceæ¡†æ¶çš„ä¸€ç§å®ç°ï¼Œè‡ªç„¶ä¹Ÿå®ç°äº†shuffleçš„é€»è¾‘ï¼Œæœ¬æ–‡å°±æ·±å…¥ç ”ç©¶Sparkçš„shuffleæ˜¯å¦‚ä½•å®ç°çš„ï¼Œæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Œä¸Hadoop MapReduceçš„shuffleæœ‰ä»€ä¹ˆä¸åŒã€‚</p>

<h2 id="shuffle">Shuffle</h2>

<p>Shuffleæ˜¯MapReduceæ¡†æ¶ä¸­çš„ä¸€ä¸ªç‰¹å®šçš„phaseï¼Œä»‹äºMap phaseå’ŒReduce phaseä¹‹é—´ï¼Œå½“Mapçš„è¾“å‡ºç»“æœè¦è¢«Reduceä½¿ç”¨æ—¶ï¼Œè¾“å‡ºç»“æœéœ€è¦æŒ‰keyå“ˆå¸Œï¼Œå¹¶ä¸”åˆ†å‘åˆ°æ¯ä¸€ä¸ªReducerä¸Šå»ï¼Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯shuffleã€‚ç”±äºshuffleæ¶‰åŠåˆ°äº†ç£ç›˜çš„è¯»å†™å’Œç½‘ç»œçš„ä¼ è¾“ï¼Œå› æ­¤shuffleæ€§èƒ½çš„é«˜ä½ç›´æ¥å½±å“åˆ°äº†æ•´ä¸ªç¨‹åºçš„è¿è¡Œæ•ˆç‡ã€‚</p>

<p>ä¸‹é¢è¿™å¹…å›¾æ¸…æ™°åœ°æè¿°äº†MapReduceç®—æ³•çš„æ•´ä¸ªæµç¨‹ï¼Œå…¶ä¸­shuffle phaseæ˜¯ä»‹äºMap phaseå’ŒReduce phaseä¹‹é—´ã€‚</p>

<p><img src="/img/2014-01-04-spark-shuffle/mapreduce-process.jpg" alt="mapreduce running process" width="640" /></p>

<p>æ¦‚å¿µä¸Šshuffleå°±æ˜¯ä¸€ä¸ªæ²Ÿé€šæ•°æ®è¿æ¥çš„æ¡¥æ¢ï¼Œé‚£ä¹ˆå®é™…ä¸Šshuffleè¿™ä¸€éƒ¨åˆ†æ˜¯å¦‚ä½•å®ç°çš„çš„å‘¢ï¼Œä¸‹é¢æˆ‘ä»¬å°±ä»¥Sparkä¸ºä¾‹è®²ä¸€ä¸‹shuffleåœ¨Sparkä¸­çš„å®ç°ã€‚</p>

<h2 id="spark-shuffleè¿›åŒ–å²">Spark Shuffleè¿›åŒ–å²</h2>

<p>å…ˆä»¥å›¾ä¸ºä¾‹ç®€å•æè¿°ä¸€ä¸‹Sparkä¸­shuffleçš„æ•´ä¸€ä¸ªæµç¨‹ï¼š</p>

<p><img src="/img/2014-01-04-spark-shuffle/spark-shuffle.png" alt="spark shuffle process" width="640" /></p>

<ul>
  <li>é¦–å…ˆæ¯ä¸€ä¸ªMapperä¼šæ ¹æ®Reducerçš„æ•°é‡åˆ›å»ºå‡ºç›¸åº”çš„bucketï¼Œbucketçš„æ•°é‡æ˜¯\(M \times R\)ï¼Œå…¶ä¸­\(M\)æ˜¯Mapçš„ä¸ªæ•°ï¼Œ\(R\)æ˜¯Reduceçš„ä¸ªæ•°ã€‚</li>
  <li>å…¶æ¬¡Mapperäº§ç”Ÿçš„ç»“æœä¼šæ ¹æ®è®¾ç½®çš„partitionç®—æ³•å¡«å……åˆ°æ¯ä¸ªbucketä¸­å»ã€‚è¿™é‡Œçš„partitionç®—æ³•æ˜¯å¯ä»¥è‡ªå®šä¹‰çš„ï¼Œå½“ç„¶é»˜è®¤çš„ç®—æ³•æ˜¯æ ¹æ®keyå“ˆå¸Œåˆ°ä¸åŒçš„bucketä¸­å»ã€‚</li>
  <li>å½“Reducerå¯åŠ¨æ—¶ï¼Œå®ƒä¼šæ ¹æ®è‡ªå·±taskçš„idå’Œæ‰€ä¾èµ–çš„Mapperçš„idä»è¿œç«¯æˆ–æ˜¯æœ¬åœ°çš„block managerä¸­å–å¾—ç›¸åº”çš„bucketä½œä¸ºReducerçš„è¾“å…¥è¿›è¡Œå¤„ç†ã€‚</li>
</ul>

<p>è¿™é‡Œçš„bucketæ˜¯ä¸€ä¸ªæŠ½è±¡æ¦‚å¿µï¼Œåœ¨å®ç°ä¸­æ¯ä¸ªbucketå¯ä»¥å¯¹åº”ä¸€ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥å¯¹åº”æ–‡ä»¶çš„ä¸€éƒ¨åˆ†æˆ–æ˜¯å…¶ä»–ç­‰ã€‚</p>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬åˆ†åˆ«ä»<strong>shuffle write</strong>å’Œ<strong>shuffle fetch</strong>è¿™ä¸¤å—æ¥è®²è¿°ä¸€ä¸‹Sparkçš„shuffleè¿›åŒ–å²ã€‚</p>

<h3 id="shuffle-write">Shuffle Write</h3>

<p>åœ¨Spark 0.6å’Œ0.7çš„ç‰ˆæœ¬ä¸­ï¼Œå¯¹äºshuffleæ•°æ®çš„å­˜å‚¨æ˜¯ä»¥æ–‡ä»¶çš„æ–¹å¼å­˜å‚¨åœ¨block managerä¸­ï¼Œä¸<code class="language-plaintext highlighter-rouge">rdd.persist(StorageLevel.DISk_ONLY)</code>é‡‡å–ç›¸åŒçš„ç­–ç•¥ï¼Œå¯ä»¥å‚çœ‹ï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def run(attemptId: Long): MapStatus = {
  val numOutputSplits = dep.partitioner.numPartitions

  ...
    // Partition the map output.
    val buckets = Array.fill(numOutputSplits)(new ArrayBuffer[(Any, Any)])
    for (elem &lt;- rdd.iterator(split, taskContext)) {
      val pair = elem.asInstanceOf[(Any, Any)]
      val bucketId = dep.partitioner.getPartition(pair._1)
      buckets(bucketId) += pair
    }

    ...

    val blockManager = SparkEnv.get.blockManager
    for (i &lt;- 0 until numOutputSplits) {
      val blockId = "shuffle_" + dep.shuffleId + "_" + partition + "_" + i
      // Get a Scala iterator from Java map
      val iter: Iterator[(Any, Any)] = buckets(i).iterator
      val size = blockManager.put(blockId, iter, StorageLevel.DISK_ONLY, false)
      totalBytes += size
    }
  ...
}
</code></pre></div></div>

<p>æˆ‘å·²ç»å°†ä¸€äº›å¹²æ‰°ä»£ç åˆ å»ã€‚å¯ä»¥çœ‹åˆ°Sparkåœ¨æ¯ä¸€ä¸ªMapperä¸­ä¸ºæ¯ä¸ªReduceråˆ›å»ºä¸€ä¸ªbucketï¼Œå¹¶å°†RDDè®¡ç®—ç»“æœæ”¾è¿›bucketä¸­ã€‚éœ€è¦æ³¨æ„çš„æ˜¯æ¯ä¸ªbucketæ˜¯ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">ArrayBuffer</code>ï¼Œä¹Ÿå°±æ˜¯è¯´Mapçš„è¾“å‡ºç»“æœæ˜¯ä¼šå…ˆå­˜å‚¨åœ¨å†…å­˜ã€‚</p>

<p>æ¥ç€Sparkä¼šå°†ArrayBufferä¸­çš„Mapè¾“å‡ºç»“æœå†™å…¥block manageræ‰€ç®¡ç†çš„ç£ç›˜ä¸­ï¼Œè¿™é‡Œæ–‡ä»¶çš„å‘½åæ–¹å¼ä¸ºï¼š
<code class="language-plaintext highlighter-rouge">shuffle_ + shuffle_id + "_" + map partition id + "_" + shuffle partition id</code>ã€‚</p>

<p>æ—©æœŸçš„shuffle writeæœ‰ä¸¤ä¸ªæ¯”è¾ƒå¤§çš„é—®é¢˜ï¼š</p>

<ol>
  <li>Mapçš„è¾“å‡ºå¿…é¡»å…ˆå…¨éƒ¨å­˜å‚¨åˆ°å†…å­˜ä¸­ï¼Œç„¶åå†™å…¥ç£ç›˜ã€‚è¿™å¯¹å†…å­˜æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„å¼€é”€ï¼Œå½“å†…å­˜ä¸è¶³ä»¥å­˜å‚¨æ‰€æœ‰çš„Map outputæ—¶å°±ä¼šå‡ºç°OOMã€‚</li>
  <li>æ¯ä¸€ä¸ªMapperéƒ½ä¼šäº§ç”ŸReducer numberä¸ªshuffleæ–‡ä»¶ï¼Œå¦‚æœMapperä¸ªæ•°æ˜¯1kï¼ŒReducerä¸ªæ•°ä¹Ÿæ˜¯1kï¼Œé‚£ä¹ˆå°±ä¼šäº§ç”Ÿ1Mä¸ªshuffleæ–‡ä»¶ï¼Œè¿™å¯¹äºæ–‡ä»¶ç³»ç»Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤§çš„è´Ÿæ‹…ã€‚åŒæ—¶åœ¨shuffleæ•°æ®é‡ä¸å¤§è€Œshuffleæ–‡ä»¶åˆéå¸¸å¤šçš„æƒ…å†µä¸‹ï¼Œéšæœºå†™ä¹Ÿä¼šä¸¥é‡é™ä½IOçš„æ€§èƒ½ã€‚</li>
</ol>

<p>åœ¨Spark 0.8ç‰ˆæœ¬ä¸­ï¼Œshuffle writeé‡‡ç”¨äº†ä¸RDD block writeä¸åŒçš„æ–¹å¼ï¼ŒåŒæ—¶ä¹Ÿä¸ºshuffle writeå•ç‹¬åˆ›å»ºäº†<code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>ï¼Œéƒ¨åˆ†è§£å†³äº†0.6å’Œ0.7ç‰ˆæœ¬ä¸­é‡åˆ°çš„é—®é¢˜ã€‚</p>

<p>é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹Spark 0.8çš„å…·ä½“å®ç°ï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>override def run(attemptId: Long): MapStatus = {

  ...

  val blockManager = SparkEnv.get.blockManager
  var shuffle: ShuffleBlocks = null
  var buckets: ShuffleWriterGroup = null

  try {
    // Obtain all the block writers for shuffle blocks.
    val ser = SparkEnv.get.serializerManager.get(dep.serializerClass)
    shuffle = blockManager.shuffleBlockManager.forShuffle(dep.shuffleId, numOutputSplits, ser)
    buckets = shuffle.acquireWriters(partition)

    // Write the map output to its associated buckets.
    for (elem &lt;- rdd.iterator(split, taskContext)) {
      val pair = elem.asInstanceOf[Product2[Any, Any]]
      val bucketId = dep.partitioner.getPartition(pair._1)
      buckets.writers(bucketId).write(pair)
    }

    // Commit the writes. Get the size of each bucket block (total block size).
    var totalBytes = 0L
    val compressedSizes: Array[Byte] = buckets.writers.map { writer:   BlockObjectWriter =&gt;
      writer.commit()
      writer.close()
      val size = writer.size()
      totalBytes += size
      MapOutputTracker.compressSize(size)
    }

    ...

  } catch { case e: Exception =&gt;
    // If there is an exception from running the task, revert the partial writes
    // and throw the exception upstream to Spark.
    if (buckets != null) {
      buckets.writers.foreach(_.revertPartialWrites())
    }
    throw e
  } finally {
    // Release the writers back to the shuffle block manager.
    if (shuffle != null &amp;&amp; buckets != null) {
      shuffle.releaseWriters(buckets)
    }
    // Execute the callbacks on task completion.
    taskContext.executeOnCompleteCallbacks()
    }
  }
}
</code></pre></div></div>

<p>åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ä¸ºshuffle writeæ·»åŠ äº†ä¸€ä¸ªæ–°çš„ç±»<code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>ï¼Œç”±<code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>æ¥åˆ†é…å’Œç®¡ç†bucketã€‚åŒæ—¶<code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>ä¸ºæ¯ä¸€ä¸ªbucketåˆ†é…ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">DiskObjectWriter</code>ï¼Œæ¯ä¸ªwrite handleræ‹¥æœ‰é»˜è®¤100KBçš„ç¼“å­˜ï¼Œä½¿ç”¨è¿™ä¸ªwrite handlerå°†Map outputå†™å…¥æ–‡ä»¶ä¸­ã€‚å¯ä»¥çœ‹åˆ°ç°åœ¨çš„å†™å…¥æ–¹å¼å˜ä¸º<code class="language-plaintext highlighter-rouge">buckets.writers(bucketId).write(pair)</code>ï¼Œä¹Ÿå°±æ˜¯è¯´Map outputçš„key-value pairæ˜¯é€ä¸ªå†™å…¥åˆ°ç£ç›˜è€Œä¸æ˜¯é¢„å…ˆæŠŠæ‰€æœ‰æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­åœ¨æ•´ä½“flushåˆ°ç£ç›˜ä¸­å»ã€‚</p>

<p><code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private[spark]
class ShuffleBlockManager(blockManager: BlockManager) {

  def forShuffle(shuffleId: Int, numBuckets: Int, serializer: Serializer): ShuffleBlocks = {
    new ShuffleBlocks {
      // Get a group of writers for a map task.
      override def acquireWriters(mapId: Int): ShuffleWriterGroup = {
        val bufferSize = System.getProperty("spark.shuffle.file.buffer.kb", "100").toInt * 1024
        val writers = Array.tabulate[BlockObjectWriter](numBuckets) { bucketId =&gt;
          val blockId = ShuffleBlockManager.blockId(shuffleId, bucketId, mapId)
          blockManager.getDiskBlockWriter(blockId, serializer, bufferSize)
        }
        new ShuffleWriterGroup(mapId, writers)
      }

      override def releaseWriters(group: ShuffleWriterGroup) = {
        // Nothing really to release here.
      }
    }
  }
}
</code></pre></div></div>

<p>Spark 0.8æ˜¾è‘—å‡å°‘äº†shuffleçš„å†…å­˜å‹åŠ›ï¼Œç°åœ¨Map outputä¸éœ€è¦å…ˆå…¨éƒ¨å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œå†flushåˆ°ç¡¬ç›˜ï¼Œè€Œæ˜¯record-by-recordå†™å…¥åˆ°ç£ç›˜ä¸­ã€‚åŒæ—¶å¯¹äºshuffleæ–‡ä»¶çš„ç®¡ç†ä¹Ÿç‹¬ç«‹å‡ºæ–°çš„<code class="language-plaintext highlighter-rouge">ShuffleBlockManager</code>è¿›è¡Œç®¡ç†ï¼Œè€Œä¸æ˜¯ä¸rdd cacheæ–‡ä»¶åœ¨ä¸€èµ·äº†ã€‚</p>

<p>ä½†æ˜¯è¿™ä¸€ç‰ˆSpark 0.8çš„shuffle writeä»ç„¶æœ‰ä¸¤ä¸ªå¤§çš„é—®é¢˜æ²¡æœ‰è§£å†³ï¼š</p>

<ul>
  <li>é¦–å…ˆä¾æ—§æ˜¯shuffleæ–‡ä»¶è¿‡å¤šçš„é—®é¢˜ï¼Œshuffleæ–‡ä»¶è¿‡å¤šä¸€æ˜¯ä¼šé€ æˆæ–‡ä»¶ç³»ç»Ÿçš„å‹åŠ›è¿‡å¤§ï¼ŒäºŒæ˜¯ä¼šé™ä½IOçš„ååé‡ã€‚</li>
  <li>å…¶æ¬¡è™½ç„¶Map outputæ•°æ®ä¸å†éœ€è¦é¢„å…ˆåœ¨å†…å­˜ä¸­evaluateæ˜¾è‘—å‡å°‘äº†å†…å­˜å‹åŠ›ï¼Œä½†æ˜¯æ–°å¼•å…¥çš„<code class="language-plaintext highlighter-rouge">DiskObjectWriter</code>æ‰€å¸¦æ¥çš„bufferå¼€é”€ä¹Ÿæ˜¯ä¸€ä¸ªä¸å®¹å°è§†çš„å†…å­˜å¼€é”€ã€‚å‡å®šæˆ‘ä»¬æœ‰1kä¸ªMapperå’Œ1kä¸ªReducerï¼Œé‚£ä¹ˆå°±ä¼šæœ‰1Mä¸ªbucketï¼Œäºæ­¤åŒæ—¶å°±ä¼šæœ‰1Mä¸ªwrite handlerï¼Œè€Œæ¯ä¸€ä¸ªwrite handleré»˜è®¤éœ€è¦100KBå†…å­˜ï¼Œé‚£ä¹ˆæ€»å…±éœ€è¦100GBçš„å†…å­˜ã€‚è¿™æ ·çš„è¯ä»…ä»…æ˜¯bufferå°±éœ€è¦è¿™ä¹ˆå¤šçš„å†…å­˜ï¼Œå†…å­˜çš„å¼€é”€æ˜¯æƒŠäººçš„ã€‚å½“ç„¶å®é™…æƒ…å†µä¸‹è¿™1kä¸ªMapperæ˜¯åˆ†æ—¶è¿è¡Œçš„è¯ï¼Œæ‰€éœ€çš„å†…å­˜å°±åªæœ‰<code class="language-plaintext highlighter-rouge">cores * reducer numbers * 100KB</code>å¤§å°äº†ã€‚ä½†æ˜¯reduceræ•°é‡å¾ˆå¤šçš„è¯ï¼Œè¿™ä¸ªbufferçš„å†…å­˜å¼€é”€ä¹Ÿæ˜¯è›®å‰å®³çš„ã€‚</li>
</ul>

<p>ä¸ºäº†è§£å†³shuffleæ–‡ä»¶è¿‡å¤šçš„æƒ…å†µï¼ŒSpark 0.8.1å¼•å…¥äº†æ–°çš„shuffle consolidationï¼Œä»¥æœŸæ˜¾è‘—å‡å°‘shuffleæ–‡ä»¶çš„æ•°é‡ã€‚</p>

<p>é¦–å…ˆæˆ‘ä»¬ä»¥å›¾ä¾‹æ¥ä»‹ç»ä¸€ä¸‹shuffle consolidationçš„åŸç†ã€‚</p>

<p><img src="/img/2014-01-04-spark-shuffle/spark-shuffle-consolidate.png" alt="spark shuffle  consolidation process" width="640" /></p>

<p>å‡å®šè¯¥jobæœ‰4ä¸ªMapperå’Œ4ä¸ªReducerï¼Œæœ‰2ä¸ªcoreï¼Œä¹Ÿå°±æ˜¯èƒ½å¹¶è¡Œè¿è¡Œä¸¤ä¸ªtaskã€‚æˆ‘ä»¬å¯ä»¥ç®—å‡ºSparkçš„shuffle writeå…±éœ€è¦16ä¸ªbucketï¼Œä¹Ÿå°±æœ‰äº†16ä¸ªwrite handlerã€‚åœ¨ä¹‹å‰çš„Sparkç‰ˆæœ¬ä¸­ï¼Œæ¯ä¸€ä¸ªbucketå¯¹åº”çš„æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œå› æ­¤åœ¨è¿™é‡Œä¼šäº§ç”Ÿ16ä¸ªshuffleæ–‡ä»¶ã€‚</p>

<p>è€Œåœ¨shuffle consolidationä¸­æ¯ä¸€ä¸ªbucketå¹¶éå¯¹åº”ä¸€ä¸ªæ–‡ä»¶ï¼Œè€Œæ˜¯å¯¹åº”æ–‡ä»¶ä¸­çš„ä¸€ä¸ªsegmentï¼ŒåŒæ—¶shuffle consolidationæ‰€äº§ç”Ÿçš„shuffleæ–‡ä»¶æ•°é‡ä¸Spark coreçš„ä¸ªæ•°ä¹Ÿæœ‰å…³ç³»ã€‚åœ¨ä¸Šé¢çš„å›¾ä¾‹ä¸­ï¼Œjobçš„4ä¸ªMapperåˆ†ä¸ºä¸¤æ‰¹è¿è¡Œï¼Œåœ¨ç¬¬ä¸€æ‰¹2ä¸ªMapperè¿è¡Œæ—¶ä¼šç”³è¯·8ä¸ªbucketï¼Œäº§ç”Ÿ8ä¸ªshuffleæ–‡ä»¶ï¼›è€Œåœ¨ç¬¬äºŒæ‰¹Mapperè¿è¡Œæ—¶ï¼Œç”³è¯·çš„8ä¸ªbucketå¹¶ä¸ä¼šå†äº§ç”Ÿ8ä¸ªæ–°çš„æ–‡ä»¶ï¼Œè€Œæ˜¯è¿½åŠ å†™åˆ°ä¹‹å‰çš„8ä¸ªæ–‡ä»¶åé¢ï¼Œè¿™æ ·ä¸€å…±å°±åªæœ‰8ä¸ªshuffleæ–‡ä»¶ï¼Œè€Œåœ¨æ–‡ä»¶å†…éƒ¨è¿™æœ‰16ä¸ªä¸åŒçš„segmentã€‚å› æ­¤ä»ç†è®ºä¸Šè®²shuffle consolidationæ‰€äº§ç”Ÿçš„shuffleæ–‡ä»¶æ•°é‡ä¸º\(C \times R\)ï¼Œå…¶ä¸­\(C\)æ˜¯Sparké›†ç¾¤çš„core numberï¼Œ\(R\)æ˜¯Reducerçš„ä¸ªæ•°ã€‚</p>

<p>éœ€è¦æ³¨æ„çš„æ˜¯å½“ \(M=C\)æ—¶shuffle consolidationæ‰€äº§ç”Ÿçš„æ–‡ä»¶æ•°å’Œä¹‹å‰çš„å®ç°æ˜¯ä¸€æ ·çš„ã€‚</p>

<p>Shuffle consolidationæ˜¾è‘—å‡å°‘äº†shuffleæ–‡ä»¶çš„æ•°é‡ï¼Œè§£å†³äº†ä¹‹å‰ç‰ˆæœ¬ä¸€ä¸ªæ¯”è¾ƒä¸¥é‡çš„é—®é¢˜ï¼Œä½†æ˜¯writer handlerçš„bufferå¼€é”€è¿‡å¤§ä¾ç„¶æ²¡æœ‰å‡å°‘ï¼Œè‹¥è¦å‡å°‘writer handlerçš„bufferå¼€é”€ï¼Œæˆ‘ä»¬åªèƒ½å‡å°‘Reducerçš„æ•°é‡ï¼Œä½†æ˜¯è¿™åˆä¼šå¼•å…¥æ–°çš„é—®é¢˜ï¼Œä¸‹æ–‡å°†ä¼šæœ‰è¯¦ç»†ä»‹ç»ã€‚</p>

<p>è®²å®Œäº†shuffle writeçš„è¿›åŒ–å²ï¼Œæ¥ä¸‹æ¥è¦è®²ä¸€ä¸‹shuffle fetchäº†ï¼ŒåŒæ—¶è¿˜è¦è®²ä¸€ä¸‹Sparkçš„aggregatorï¼Œè¿™ä¸€å—å¯¹äºSparkå®é™…åº”ç”¨çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</p>

<h3 id="shuffle-fetch-and-aggregator">Shuffle Fetch and Aggregator</h3>

<p>Shuffle writeå†™å‡ºå»çš„æ•°æ®è¦è¢«Reducerä½¿ç”¨ï¼Œå°±éœ€è¦shuffle fetcherå°†æ‰€éœ€çš„æ•°æ®fetchè¿‡æ¥ï¼Œè¿™é‡Œçš„fetchåŒ…æ‹¬æœ¬åœ°å’Œè¿œç«¯ï¼Œå› ä¸ºshuffleæ•°æ®æœ‰å¯èƒ½ä¸€éƒ¨åˆ†æ˜¯å­˜å‚¨åœ¨æœ¬åœ°çš„ã€‚Sparkå¯¹shuffle fetcherå®ç°äº†ä¸¤å¥—ä¸åŒçš„æ¡†æ¶ï¼šNIOé€šè¿‡socketè¿æ¥å»fetchæ•°æ®ï¼›OIOé€šè¿‡netty serverå»fetchæ•°æ®ã€‚åˆ†åˆ«å¯¹åº”çš„ç±»æ˜¯<code class="language-plaintext highlighter-rouge">BasicBlockFetcherIterator</code>å’Œ<code class="language-plaintext highlighter-rouge">NettyBlockFetcherIterator</code>ã€‚</p>

<p>åœ¨Spark 0.7å’Œæ›´æ—©çš„ç‰ˆæœ¬ä¸­ï¼Œåªæ”¯æŒ<code class="language-plaintext highlighter-rouge">BasicBlockFetcherIterator</code>ï¼Œè€Œ<code class="language-plaintext highlighter-rouge">BasicBlockFetcherIterator</code>åœ¨shuffleæ•°æ®é‡æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹performanceå§‹ç»ˆä¸æ˜¯å¾ˆå¥½ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ·»åŠ äº†æ–°çš„shuffle fetcheræ¥è¯•å›¾å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚å¯¹äºæ—©æœŸshuffleæ€§èƒ½çš„è¯„æµ‹å¯ä»¥å‚çœ‹<a href="https://groups.google.com/forum/#!msg/shark-users/IHOb2u5HXSk/huTWyosI1n4J">Spark usergroup</a>ã€‚å½“ç„¶ç°åœ¨<code class="language-plaintext highlighter-rouge">BasicBlockFetcherIterator</code>çš„æ€§èƒ½ä¹Ÿå·²ç»å¥½äº†å¾ˆå¤šï¼Œä½¿ç”¨çš„æ—¶å€™å¯ä»¥å¯¹è¿™ä¸¤ç§å®ç°éƒ½è¿›è¡Œæµ‹è¯•æ¯”è¾ƒã€‚</p>

<p>æ¥ä¸‹æ¥è¯´ä¸€ä¸‹aggregatorã€‚æˆ‘ä»¬éƒ½çŸ¥é“åœ¨Hadoop MapReduceçš„shuffleè¿‡ç¨‹ä¸­ï¼Œshuffle fetchè¿‡æ¥çš„æ•°æ®ä¼šè¿›è¡Œmerge sortï¼Œä½¿å¾—ç›¸åŒkeyä¸‹çš„ä¸åŒvalueæŒ‰åºå½’å¹¶åˆ°ä¸€èµ·ä¾›Reducerä½¿ç”¨ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥å‚çœ‹ä¸‹å›¾ï¼š</p>

<p><img src="/img/2014-01-04-spark-shuffle/mapreduce-shuffle.png" alt="mapreduce shuffle process" width="640" /></p>

<p>æ‰€æœ‰çš„merge sortéƒ½æ˜¯åœ¨ç£ç›˜ä¸Šè¿›è¡Œçš„ï¼Œæœ‰æ•ˆåœ°æ§åˆ¶äº†å†…å­˜çš„ä½¿ç”¨ï¼Œä½†æ˜¯ä»£ä»·æ˜¯æ›´å¤šçš„ç£ç›˜IOã€‚</p>

<p>é‚£ä¹ˆSparkæ˜¯å¦ä¹Ÿæœ‰merge sortå‘¢ï¼Œè¿˜æ˜¯ä»¥åˆ«çš„æ–¹å¼å®ç°ï¼Œä¸‹é¢æˆ‘ä»¬å°±ç»†ç»†è¯´æ˜ã€‚</p>

<p>é¦–å…ˆè™½ç„¶Sparkå±äºMapReduceä½“ç³»ï¼Œä½†æ˜¯å¯¹ä¼ ç»Ÿçš„MapReduceç®—æ³•è¿›è¡Œäº†ä¸€å®šçš„æ”¹å˜ã€‚Sparkå‡å®šåœ¨å¤§å¤šæ•°ç”¨æˆ·çš„caseä¸­ï¼Œshuffleæ•°æ®çš„sortä¸æ˜¯å¿…é¡»çš„ï¼Œæ¯”å¦‚word countï¼Œå¼ºåˆ¶åœ°è¿›è¡Œæ’åºåªä¼šä½¿æ€§èƒ½å˜å·®ï¼Œå› æ­¤Sparkå¹¶ä¸åœ¨Reducerç«¯åšmerge sortã€‚æ—¢ç„¶æ²¡æœ‰merge sorté‚£Sparkæ˜¯å¦‚ä½•è¿›è¡Œreduceçš„å‘¢ï¼Ÿè¿™å°±è¦è¯´åˆ°aggregatoräº†ã€‚</p>

<p>aggregatoræœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªhashmapï¼Œå®ƒæ˜¯ä»¥map outputçš„keyä¸ºkeyï¼Œä»¥ä»»æ„æ‰€è¦combineçš„ç±»å‹ä¸ºvalueçš„hashmapã€‚å½“æˆ‘ä»¬åœ¨åšword count reduceè®¡ç®—countå€¼çš„æ—¶å€™ï¼Œå®ƒä¼šå°†shuffle fetchåˆ°çš„æ¯ä¸€ä¸ªkey-value pairæ›´æ–°æˆ–æ˜¯æ’å…¥åˆ°hashmapä¸­(è‹¥åœ¨hashmapä¸­æ²¡æœ‰æŸ¥æ‰¾åˆ°ï¼Œåˆ™æ’å…¥å…¶ä¸­ï¼›è‹¥æŸ¥æ‰¾åˆ°åˆ™æ›´æ–°valueå€¼)ã€‚è¿™æ ·å°±ä¸éœ€è¦é¢„å…ˆæŠŠæ‰€æœ‰çš„key-valueè¿›è¡Œmerge sortï¼Œè€Œæ˜¯æ¥ä¸€ä¸ªå¤„ç†ä¸€ä¸ªï¼Œçœä¸‹äº†å¤–éƒ¨æ’åºè¿™ä¸€æ­¥éª¤ã€‚ä½†åŒæ—¶éœ€è¦æ³¨æ„çš„æ˜¯reducerçš„å†…å­˜å¿…é¡»è¶³ä»¥å­˜æ”¾è¿™ä¸ªpartitionçš„æ‰€æœ‰keyå’Œcountå€¼ï¼Œå› æ­¤å¯¹å†…å­˜æœ‰ä¸€å®šçš„è¦æ±‚ã€‚</p>

<p>åœ¨ä¸Šé¢word countçš„ä¾‹å­ä¸­ï¼Œå› ä¸ºvalueä¼šä¸æ–­åœ°æ›´æ–°ï¼Œè€Œä¸éœ€è¦å°†å…¶å…¨éƒ¨è®°å½•åœ¨å†…å­˜ä¸­ï¼Œå› æ­¤å†…å­˜çš„ä½¿ç”¨è¿˜æ˜¯æ¯”è¾ƒå°‘çš„ã€‚è€ƒè™‘ä¸€ä¸‹å¦‚æœæ˜¯group by keyè¿™æ ·çš„æ“ä½œï¼ŒReduceréœ€è¦å¾—åˆ°keyå¯¹åº”çš„æ‰€æœ‰valueã€‚åœ¨Hadoop MapReduceä¸­ï¼Œç”±äºæœ‰äº†merge sortï¼Œå› æ­¤ç»™äºˆReducerçš„æ•°æ®å·²ç»æ˜¯group by keyäº†ï¼Œè€ŒSparkæ²¡æœ‰è¿™ä¸€æ­¥ï¼Œå› æ­¤éœ€è¦å°†keyå’Œå¯¹åº”çš„valueå…¨éƒ¨å­˜æ”¾åœ¨hashmapä¸­ï¼Œå¹¶å°†valueåˆå¹¶æˆä¸€ä¸ªarrayã€‚å¯ä»¥æƒ³è±¡ä¸ºäº†èƒ½å¤Ÿå­˜æ”¾æ‰€æœ‰æ•°æ®ï¼Œç”¨æˆ·å¿…é¡»ç¡®ä¿æ¯ä¸€ä¸ªpartitionè¶³å¤Ÿå°åˆ°å†…å­˜èƒ½å¤Ÿå®¹çº³ï¼Œè¿™å¯¹äºå†…å­˜æ˜¯ä¸€ä¸ªéå¸¸ä¸¥å³»çš„è€ƒéªŒã€‚å› æ­¤Sparkæ–‡æ¡£ä¸­å»ºè®®ç”¨æˆ·æ¶‰åŠåˆ°è¿™ç±»æ“ä½œçš„æ—¶å€™å°½é‡å¢åŠ partitionï¼Œä¹Ÿå°±æ˜¯å¢åŠ Mapperå’ŒReducerçš„æ•°é‡ã€‚</p>

<p>å¢åŠ Mapperå’ŒReducerçš„æ•°é‡å›ºç„¶å¯ä»¥å‡å°partitionçš„å¤§å°ï¼Œä½¿å¾—å†…å­˜å¯ä»¥å®¹çº³è¿™ä¸ªpartitionã€‚ä½†æ˜¯æˆ‘ä»¬åœ¨shuffle writeä¸­æåˆ°ï¼Œbucketå’Œå¯¹åº”äºbucketçš„write handleræ˜¯ç”±Mapperå’ŒReducerçš„æ•°é‡å†³å®šçš„ï¼Œtaskè¶Šå¤šï¼Œbucketå°±ä¼šå¢åŠ çš„æ›´å¤šï¼Œç”±æ­¤å¸¦æ¥write handleræ‰€éœ€çš„bufferä¹Ÿä¼šæ›´å¤šã€‚åœ¨ä¸€æ–¹é¢æˆ‘ä»¬ä¸ºäº†å‡å°‘å†…å­˜çš„ä½¿ç”¨é‡‡å–äº†å¢åŠ taskæ•°é‡çš„ç­–ç•¥ï¼Œå¦ä¸€æ–¹é¢taskæ•°é‡å¢å¤šåˆä¼šå¸¦æ¥bufferå¼€é”€æ›´å¤§çš„é—®é¢˜ï¼Œå› æ­¤é™·å…¥äº†å†…å­˜ä½¿ç”¨çš„ä¸¤éš¾å¢ƒåœ°ã€‚</p>

<p>ä¸ºäº†å‡å°‘å†…å­˜çš„ä½¿ç”¨ï¼Œåªèƒ½å°†aggregatorçš„æ“ä½œä»å†…å­˜ç§»åˆ°ç£ç›˜ä¸Šè¿›è¡Œï¼ŒSparkç¤¾åŒºä¹Ÿæ„è¯†åˆ°äº†Sparkåœ¨å¤„ç†æ•°æ®è§„æ¨¡è¿œè¿œå¤§äºå†…å­˜å¤§å°æ—¶æ‰€å¸¦æ¥çš„é—®é¢˜ã€‚å› æ­¤<a href="https://github.com/apache/incubator-spark/pull/303">PR303</a>æä¾›äº†å¤–éƒ¨æ’åºçš„å®ç°æ–¹æ¡ˆï¼Œç›¸ä¿¡åœ¨Spark 0.9 releaseçš„æ—¶å€™ï¼Œè¿™ä¸ªpatchåº”è¯¥èƒ½mergeè¿›å»ï¼Œåˆ°æ—¶å€™å†…å­˜çš„ä½¿ç”¨é‡å¯ä»¥æ˜¾è‘—åœ°å‡å°‘ã€‚</p>

<h2 id="end">End</h2>

<p>æœ¬æ–‡è¯¦ç»†åœ°ä»‹ç»äº†Sparkçš„shuffleå®ç°æ˜¯å¦‚ä½•è¿›åŒ–çš„ï¼Œä»¥åŠé‡åˆ°é—®é¢˜è§£å†³é—®é¢˜çš„è¿‡ç¨‹ã€‚shuffleä½œä¸ºSparkç¨‹åºä¸­å¾ˆé‡è¦çš„ä¸€ä¸ªç¯èŠ‚ï¼Œç›´æ¥å½±å“äº†Sparkç¨‹åºçš„æ€§èƒ½ï¼Œç°å¦‚ä»Šçš„Sparkç‰ˆæœ¬è™½ç„¶shuffleå®ç°è¿˜å­˜åœ¨ç€ç§ç§é—®é¢˜ï¼Œä½†æ˜¯ç›¸æ¯”äºæ—©æœŸç‰ˆæœ¬ï¼Œå·²ç»æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ã€‚å¼€æºä»£ç å°±æ˜¯å¦‚æ­¤ä¸åœåœ°è¿­ä»£æ¨è¿›ï¼Œéšç€Sparkçš„æ™®åŠç¨‹åº¦è¶Šæ¥è¶Šé«˜ï¼Œè´¡çŒ®çš„äººè¶Šæ¥è¶Šå¤šï¼Œç›¸ä¿¡åç»­çš„ç‰ˆæœ¬ä¼šæœ‰æ›´å¤§çš„æå‡ã€‚</p>
:ET