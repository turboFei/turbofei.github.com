I"´h
<p>ç›®å½•</p>

<ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#shuffle" id="markdown-toc-shuffle">Shuffle##</a>    <ul>
      <li><a href="#bypassmergesortshufflewriter" id="markdown-toc-bypassmergesortshufflewriter">BypassMergeSortShuffleWriter###</a></li>
      <li><a href="#sortshufflewriter" id="markdown-toc-sortshufflewriter">SortShuffleWriter</a></li>
      <li><a href="#unsafeshufflewriter" id="markdown-toc-unsafeshufflewriter">unsafeShuffleWriter</a></li>
      <li><a href="#blockstoreshufflereader" id="markdown-toc-blockstoreshufflereader">BlockStoreShuffleReader###</a></li>
    </ul>
  </li>
  <li><a href="#æ€»ç»“" id="markdown-toc-æ€»ç»“">æ€»ç»“##</a></li>
</ul>

<h3 id="background">Background</h3>
<p>spark shufféƒ¨åˆ†æ˜¯sparkæºç çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œshuffleå‘ç”Ÿåœ¨stageçš„äº¤ç•Œå¤„ï¼Œå¯¹äºsparkçš„æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œæºç æ›´æ–°åï¼Œsparkçš„shuffleæœºåˆ¶ä¹Ÿä¸ä¸€æ ·ï¼Œæœ¬æ–‡åˆ†æspark2.0çš„shuffleå®ç°ã€‚</p>

<p>æœ¬æ–‡åŸºäºspark2.0ã€‚</p>

<h2 id="shuffle">Shuffle##</h2>

<p>shuffleæ˜¯Mapreduceæ¡†æ¶ä¸­ä¸€ä¸ªç‰¹å®šçš„phaseï¼Œä»‹äºMapå’ŒReduceä¹‹é—´ã€‚shuffleçš„è‹±æ–‡æ„æ€æ˜¯æ··æ´—ï¼ŒåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œshuffle write å’Œshuffle readã€‚è¿™é‡Œæœ‰ä¸€ç¯‡æ–‡ç« :<a href="http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/">è¯¦ç»†æ¢ç©¶Sparkçš„shuffleå®ç°</a>ï¼Œè¿™ç¯‡æ–‡ç« å†™äº2014å¹´ï¼Œè®²çš„æ˜¯æ—©æœŸç‰ˆæœ¬çš„shuffleå®ç°ã€‚éšç€æºç çš„æ›´æ–°ï¼Œshuffleæœºåˆ¶ä¹Ÿåšå‡ºäº†ç›¸åº”çš„ä¼˜åŒ–ï¼Œä¸‹é¢åˆ†æspark-2.0çš„shuffleæœºåˆ¶ã€‚</p>

<p><code class="language-plaintext highlighter-rouge">shuffleWriter</code>æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå…·ä½“å®ç°æœ‰ä¸‰ç§ï¼Œ<code class="language-plaintext highlighter-rouge">BypassMergeSortShuffleWriter</code>,<code class="language-plaintext highlighter-rouge">sortShuffleWriter</code>,<code class="language-plaintext highlighter-rouge">UnsafeShuffleWriter</code>.</p>

<h3 id="bypassmergesortshufflewriter">BypassMergeSortShuffleWriter###</h3>

<p>-_-,æˆ‘å…ˆç¿»è¯‘ä¸‹è¿™ä¸ªç±»å¼€å¤´ç»™çš„æ³¨é‡Šï¼Œæ³¨é‡Šæ˜¯å¾ˆå¥½çš„å…¨å±€ç†è§£ä»£ç çš„å·¥å…·ï¼Œè¦å¥½å¥½ç†è§£ã€‚å¦‚ä¸‹ï¼š</p>

<p>è¿™ä¸ªç±»å®ç°äº†åŸºäºsort-shuffleçš„hashé£æ ¼çš„shuffle fallback pathï¼ˆå›é€€è·¯å¾„ï¼Ÿæ€ä¹ˆç¿»ï¼‰ã€‚è¿™ä¸ªwriteè·¯å¾„æŠŠæ•°æ®å†™åˆ°ä¸åŒçš„æ–‡ä»¶é‡Œï¼Œæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªreduceåˆ†åŒºï¼Œç„¶åæŠŠè¿™äº›æ–‡ä»¶æ•´åˆåˆ°ä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶çš„ä¸åŒåŒºåŸŸæœåŠ¡ä¸åŒçš„reducerã€‚æ•°æ®ä¸æ˜¯ç¼“å­˜åœ¨å†…å­˜ä¸­ã€‚è¿™ä¸ªç±»æœ¬è´¨ä¸Šå’Œä¹‹å‰çš„<code class="language-plaintext highlighter-rouge">HashShuffleReader</code>ï¼Œé™¤äº†è¿™ä¸ªç±»çš„è¾“å‡ºæ ¼å¼å¯ä»¥é€šè¿‡<code class="language-plaintext highlighter-rouge">org.apache.spark.shuffle.IndexShuffleBlockResolver</code>æ¥è°ƒç”¨ã€‚è¿™ä¸ªå†™è·¯å¾„å¯¹äºæœ‰è®¸å¤šreduceåˆ†åŒºçš„shuffleæ¥è¯´æ˜¯ä¸é«˜æ•ˆçš„ï¼Œå› ä¸ºä»–åŒæ—¶æ‰“å¼€å¾ˆå¤šserializerså’Œæ–‡ä»¶æµã€‚å› æ­¤åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰ä¼šé€‰æ‹©è¿™ä¸ªè·¯å¾„ï¼š</p>

<p>1ã€æ²¡æœ‰æ’åº  2ã€æ²¡æœ‰èšåˆæ“ä½œ  3ã€partitionçš„æ•°é‡å°äºbypassMergeThreshold</p>

<p>è¿™ä¸ªä»£ç æ›¾ç»æ˜¯ExternalSorterçš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯ä¸ºäº†å‡å°‘ä»£ç å¤æ‚åº¦å°±ç‹¬ç«‹äº†å‡ºæ¥ã€‚å¥½ï¼Œç¿»è¯‘ç»“æŸã€‚-_-</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@Override
public void write(Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException {
  assert (partitionWriters == null);
  if (!records.hasNext()) {
    partitionLengths = new long[numPartitions];
    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, null);
    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
    return;
  }
  final SerializerInstance serInstance = serializer.newInstance();
  final long openStartTime = System.nanoTime();
  partitionWriters = new DiskBlockObjectWriter[numPartitions];
  for (int i = 0; i &lt; numPartitions; i++) {
    final Tuple2&lt;TempShuffleBlockId, File&gt; tempShuffleBlockIdPlusFile =
      blockManager.diskBlockManager().createTempShuffleBlock();
    final File file = tempShuffleBlockIdPlusFile._2();
    final BlockId blockId = tempShuffleBlockIdPlusFile._1();
    partitionWriters[i] =
      blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);
  }
  // Creating the file to write to and creating a disk writer both involve interacting with
  // the disk, and can take a long time in aggregate when we open many files, so should be
  // included in the shuffle write time.
  writeMetrics.incWriteTime(System.nanoTime() - openStartTime);

  while (records.hasNext()) {
    final Product2&lt;K, V&gt; record = records.next();
    final K key = record._1();
    partitionWriters[partitioner.getPartition(key)].write(key, record._2());
  }

  for (DiskBlockObjectWriter writer : partitionWriters) {
    writer.commitAndClose();
  }

  File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);
  File tmp = Utils.tempFileWith(output);
  try {
    partitionLengths = writePartitionedFile(tmp);
    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);
  } finally {
    if (tmp.exists() &amp;&amp; !tmp.delete()) {
      logger.error("Error while deleting temp file {}", tmp.getAbsolutePath());
    }
  }
  mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
}
</code></pre></div></div>

<p>å‰é¢éƒ½å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æ ¹æ®keyçš„å“ˆå¸Œå€¼å†™åˆ°ä¸åŒçš„æ–‡ä»¶é‡Œé¢ï¼Œç„¶åå°±æ˜¯<code class="language-plaintext highlighter-rouge">writePartitionedFile</code>å’Œ<code class="language-plaintext highlighter-rouge">writeIndexFileAndCommit</code>ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/**
 * Concatenate all of the per-partition files into a single combined file.
 *
 * @return array of lengths, in bytes, of each partition of the file (used by map output tracker).
 */
private long[] writePartitionedFile(File outputFile) throws IOException {
  // Track location of the partition starts in the output file
  final long[] lengths = new long[numPartitions];
  if (partitionWriters == null) {
    // We were passed an empty iterator
    return lengths;
  }

  final FileOutputStream out = new FileOutputStream(outputFile, true);
  final long writeStartTime = System.nanoTime();
  boolean threwException = true;
  try {
    for (int i = 0; i &lt; numPartitions; i++) {
      final File file = partitionWriters[i].fileSegment().file();
      if (file.exists()) {
        final FileInputStream in = new FileInputStream(file);
        boolean copyThrewException = true;
        try {
          lengths[i] = Utils.copyStream(in, out, false, transferToEnabled);
          copyThrewException = false;
        } finally {
          Closeables.close(in, copyThrewException);
        }
        if (!file.delete()) {
          logger.error("Unable to delete file for partition {}", i);
        }
      }
    }
    threwException = false;
  } finally {
    Closeables.close(out, threwException);
    writeMetrics.incWriteTime(System.nanoTime() - writeStartTime);
  }
  partitionWriters = null;
  return lengths;
}
</code></pre></div></div>

<p>è¿™ä¸ªå°±æ˜¯æŒ‰é¡ºåºæŠŠä¹‹å‰å†™çš„åˆ†åŒºæ–‡ä»¶é‡Œçš„æ•°æ®åˆå¹¶åˆ°ä¸€ä¸ªå¤§æ–‡ä»¶é‡Œé¢ï¼Œç„¶åè¿”å›æ¯ä¸ªåˆ†åŒºæ–‡ä»¶çš„é•¿åº¦ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/**
 * Write an index file with the offsets of each block, plus a final offset at the end for the
 * end of the output file. This will be used by getBlockData to figure out where each block
 * begins and ends.
 *
 * It will commit the data and index file as an atomic operation, use the existing ones, or
 * replace them with new ones.
 *
 * Note: the `lengths` will be updated to match the existing index file if use the existing ones.
 * */
def writeIndexFileAndCommit(
    shuffleId: Int,
    mapId: Int,
    lengths: Array[Long],
    dataTmp: File): Unit = {
  val indexFile = getIndexFile(shuffleId, mapId)
  val indexTmp = Utils.tempFileWith(indexFile)
  try {
    val out = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(indexTmp)))
    Utils.tryWithSafeFinally {
      // We take in lengths of each block, need to convert it to offsets.
      var offset = 0L
      out.writeLong(offset)
      for (length &lt;- lengths) {
        offset += length
        out.writeLong(offset)
      }
    } {
      out.close()
    }

    val dataFile = getDataFile(shuffleId, mapId)
    // There is only one IndexShuffleBlockResolver per executor, this synchronization make sure
    // the following check and rename are atomic.
    synchronized {
      val existingLengths = checkIndexAndDataFile(indexFile, dataFile, lengths.length)
      if (existingLengths != null) {
        // Another attempt for the same task has already written our map outputs successfully,
        // so just use the existing partition lengths and delete our temporary map outputs.
        System.arraycopy(existingLengths, 0, lengths, 0, lengths.length)
        if (dataTmp != null &amp;&amp; dataTmp.exists()) {
          dataTmp.delete()
        }
        indexTmp.delete()
      } else {
        // This is the first successful attempt in writing the map outputs for this task,
        // so override any existing index and data files with the ones we wrote.
        if (indexFile.exists()) {
          indexFile.delete()
        }
        if (dataFile.exists()) {
          dataFile.delete()
        }
        if (!indexTmp.renameTo(indexFile)) {
          throw new IOException("fail to rename file " + indexTmp + " to " + indexFile)
        }
        if (dataTmp != null &amp;&amp; dataTmp.exists() &amp;&amp; !dataTmp.renameTo(dataFile)) {
          throw new IOException("fail to rename file " + dataTmp + " to " + dataFile)
        }
      }
    }
  } finally {
    if (indexTmp.exists() &amp;&amp; !indexTmp.delete()) {
      logError(s"Failed to delete temporary index file at ${indexTmp.getAbsolutePath}")
    }
  }
}
</code></pre></div></div>

<p>è§£é‡Šä¸‹è¿™æ®µä»£ç ï¼Œä¸Šæ¥å…ˆå†™indexTmpï¼Œæ˜¯æŠŠåˆ†åŒºæ–‡ä»¶é•¿åº¦å†™è¿›å»ï¼Œä¾¿äºç´¢å¼•éœ€è¦çš„é‚£éƒ¨åˆ†æ•°æ®ã€‚ç„¶åå°±åˆ¤æ–­è¿™ä¸ªä»»åŠ¡æ˜¯ä¸æ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œåˆ°è¿™é‡Œï¼Œå¦‚æœä¹‹å‰æ‰§è¡ŒæˆåŠŸè¿‡ï¼Œé‚£å°±ä¸ç”¨å†™äº†ï¼Œç›´æ¥ç”¨ä»¥å‰çš„ç»“æœå°±è¡Œã€‚</p>

<p>å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰§è¡Œåˆ°è¿™é‡Œï¼Œé‚£ä¹ˆå°±æŠŠä¹‹å‰çš„indexTmpé‡å‘½åä¸ºindexFileï¼ŒdataTmpé‡å‘½åä¸ºdataFileç„¶åè¿”å›ã€‚</p>

<p>è¿™é‡Œè¦æ³¨æ„ä¸‹ï¼Œæ¯ä¸ªexecutorä¸Šé¢åªæœ‰ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">IndexShuffleBlockResolver</code>ï¼Œè¿™ä¸ªç®¡ç†è¿™ä¸ªexecutorä¸Šæ‰€æœ‰çš„indexFile.</p>

<p>ç­‰è¿™ä¸ªindexFileä¹Ÿå†™å¥½ä¹‹åï¼Œå°±è¿”å›<code class="language-plaintext highlighter-rouge">mapStatus</code>ã€‚shuffleWriteå°±ç»“æŸäº†ã€‚</p>

<h3 id="sortshufflewriter">SortShuffleWriter</h3>

<p>é¦–å…ˆæè¿°ä¸‹å¤§æ¦‚ã€‚å› ä¸ºæ˜¯sortï¼Œæ‰€ä»¥è¦æ’åºï¼Œè¿™é‡Œå°±ç”¨åˆ°äº†ExternalSoterè¿™ä¸ªæ•°æ®ç»“æ„ã€‚ç„¶åæŠŠè¦å¤„ç†çš„æ•°æ®å…¨éƒ¨æ’å…¥åˆ°ExternalSorteré‡Œé¢ï¼Œåœ¨æ’å…¥çš„è¿‡ç¨‹ä¸­æ˜¯ä¸æ’åºçš„ï¼Œå°±æ˜¯æ’å…¥ï¼Œæ’å…¥æ•°æ®æ˜¯(partitionId,key,value)ã€‚ç„¶åæ˜¯è°ƒç”¨<code class="language-plaintext highlighter-rouge"> sorter.writePartitionedFile</code>,åœ¨è¿™é‡Œä¼šæ’åºï¼Œä¼šæŒ‰ç…§partitionIdå’Œkeyï¼ˆæˆ–è€…keyçš„hashcodeï¼‰è¿›è¡Œæ’åºï¼Œå…¶ä»–çš„å°±å’Œä¸Šé¢bypassShuffleWriterçš„å·®ä¸å¤šäº†ï¼Œæœ€åä¹Ÿæ˜¯å†™åˆ°ä¸€ä¸ªindexFileé‡Œé¢ã€‚è¿”å›mapStatusã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** Write a bunch of records to this task's output */
override def write(records: Iterator[Product2[K, V]]): Unit = {
  sorter = if (dep.mapSideCombine) {
    require(dep.aggregator.isDefined, "Map-side combine without Aggregator specified!")
    new ExternalSorter[K, V, C](
      context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
  } else {
    // In this case we pass neither an aggregator nor an ordering to the sorter, because we don't
    // care whether the keys get sorted in each partition; that will be done on the reduce side
    // if the operation being run is sortByKey.
    new ExternalSorter[K, V, V](
      context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)
  }
  sorter.insertAll(records)

  // Don't bother including the time to open the merged output file in the shuffle write time,
  // because it just opens a single file, so is typically too fast to measure accurately
  // (see SPARK-3570).
  val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
  val tmp = Utils.tempFileWith(output)
  try {
    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
    val partitionLengths = sorter.writePartitionedFile(blockId, tmp)
    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)
    mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)
  } finally {
    if (tmp.exists() &amp;&amp; !tmp.delete()) {
      logError(s"Error while deleting temp file ${tmp.getAbsolutePath}")
    }
  }
}
</code></pre></div></div>

<p>è¿™é‡Œé¢ExternalSorteræ˜¯æ ¸å¿ƒã€‚çœ‹å®ƒçš„æºç ï¼Œå®ƒå­˜æ•°æ®æ˜¯ä½¿ç”¨çš„ä¸¤ç§æ•°æ®ç»“æ„ã€‚<code class="language-plaintext highlighter-rouge">PartitionedAppendOnlyMap</code>ã€<code class="language-plaintext highlighter-rouge">PartitionedPairBuffer</code>ï¼Œå…¶ä¸­æœ‰èšåˆæ“ä½œä½¿ç”¨mapï¼Œæ²¡æœ‰èšåˆæ“ä½œä½¿ç”¨bufferã€‚PartitionedAppendOnlyMap ç»§æ‰¿äº†SizeTrackingAppendOnlyMap å’ŒWritablePartitionedPairCollection ã€‚ å…¶ä¸­SizeTrackingAppendOnlyMapæ˜¯ç”¨äºé¢„æµ‹ç©ºé—´ï¼ˆSizeTrackerï¼‰ï¼Œç„¶ååŠ å­˜å‚¨æ•°æ®ï¼ˆAppendOnlyMapï¼‰,ç„¶åWritablePartitionedPairCollectionæ˜¯ç”¨äºæ’å…¥æ•°æ®æ—¶å€™æ’å…¥partitionIdï¼ˆinsert(partition: Int, key: K, value: V)ï¼‰åŠ ä¸Šé‡Œé¢å®ç°äº†å¯¹æ•°æ®æŒ‰ç…§partitionIdå’ŒKeyæ’åºçš„æ–¹æ³•ã€‚</p>

<p>æˆ‘ä¸»è¦æ˜¯å¯¹AppendOnlyMapæ€ä¹ˆå­˜å‚¨æ•°æ®æ¯”è¾ƒæ„Ÿå…´è¶£ã€‚çœ‹ä¸‹AppendOnlyMapã€‚</p>

<p>çœ‹æºç ï¼Œå®ƒå­˜å‚¨æ•°æ®æ˜¯<code class="language-plaintext highlighter-rouge">private var data = new Array[AnyRef](2 * capacity)</code>,æ˜¯ä½¿ç”¨æ•°ç»„å­˜å‚¨çš„ï¼Œkeyå’ŒvalueæŒ¨ç€ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†èŠ‚çœç©ºé—´ã€‚</p>

<p>ç„¶åmapçš„Updateå’ŒchangeValueå‡½æ•°æ˜¯å·®ä¸å¤šçš„ï¼Œåªä¸è¿‡åè€…çš„changeValueæ˜¯ç”±è®¡ç®—å‡½æ•°è®¡ç®—çš„valueï¼Œæ‰€ä»¥æˆ‘ä»¬å°±çœ‹updateæ–¹æ³•ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** Set the value for a key */
def update(key: K, value: V): Unit = {
  assert(!destroyed, destructionMessage)
  val k = key.asInstanceOf[AnyRef]
  if (k.eq(null)) {
    if (!haveNullValue) {
      incrementSize()
    }
    nullValue = value
    haveNullValue = true
    return
  }
  var pos = rehash(key.hashCode) &amp; mask
  var i = 1
  while (true) {
    val curKey = data(2 * pos)
    if (curKey.eq(null)) {
      data(2 * pos) = k
      data(2 * pos + 1) = value.asInstanceOf[AnyRef]
      incrementSize()  // Since we added a new key
      return
    } else if (k.eq(curKey) || k.equals(curKey)) {
      data(2 * pos + 1) = value.asInstanceOf[AnyRef]
      return
    } else {
      val delta = i
      pos = (pos + delta) &amp; mask
      i += 1
    }
  }
}
</code></pre></div></div>

<p>çœ‹æºç å¯ä»¥çœ‹å‡ºï¼Œè¿™é‡Œæ’å…¥æ•°æ®ï¼Œé‡‡ç”¨çš„äºŒæ¬¡æ¢æµ‹æ³•ã€‚java.util.collectionçš„HashMapåœ¨hashå†²çªæ—¶å€™é‡‡ç”¨çš„æ˜¯é“¾æ¥æ³•ï¼Œè€Œè¿™é‡Œçš„äºŒæ¬¡æ¢æµ‹æ³•ç¼ºç‚¹å°±æ˜¯åˆ é™¤å…ƒç´ æ—¶å€™æ¯”è¾ƒå¤æ‚ï¼Œä¸èƒ½ç®€å•çš„æŠŠæ•°ç»„ä¸­çš„ç›¸åº”ä½ç½®è®¾ä¸ºnullï¼Œè¿™æ ·å°±æ²¡åŠæ³•æŸ¥æ‰¾å…ƒç´ ï¼Œé€šå¸¸æ˜¯æŠŠè¢«åˆ é™¤çš„å…ƒç´ æ ‡è®°ä¸ºå·²åˆ é™¤ï¼Œä½†æ˜¯åˆéœ€è¦å æ®é¢å¤–çš„ç©ºé—´ã€‚ä½†æ˜¯æ­¤å¤„æ˜¯appendOnlyMapï¼Œä¹Ÿå°±æ˜¯åªä¼šè¿½åŠ ï¼ˆæ’å…¥æˆ–è€…æ›´æ–°ï¼‰ï¼Œä¸ä¼šåˆ é™¤ï¼Œæ‰€ä»¥è¿™ä¸ªè‡ªå®šä¹‰çš„mapæ›´çœå†…å­˜ã€‚</p>

<p>ç„¶åè¿™ä¸ªAppendOnlyMapä¼šåœ¨growMapçš„æ—¶å€™é‡æ–°hashã€‚åœ¨sorter.insertallæ—¶å€™æ˜¯ä¸æ’åºçš„ã€‚</p>

<p>ç„¶åwritePartitionedFile é‡Œé¢è°ƒç”¨<code class="language-plaintext highlighter-rouge">collection.destructiveSortedWritablePartitionedIterator(comparator)	</code>ä¼šå¯¹æ•°æ®æ’åºï¼Œä¹‹åå°±è·Ÿä¸Šä¸€å°èŠ‚é‡Œé¢çš„writePartitionedFileå·®ä¸å¤šäº†ï¼Œæ— éå°±æ˜¯æŠŠå†…å­˜é‡Œé¢çš„æ•°æ®å’Œspillçš„æ•°æ®åˆå¹¶ä¹‹åå†™å…¥å¤§æ–‡ä»¶é‡Œé¢ï¼Œä¹‹åçš„writeIndexFileæ˜¯ä¸€æ ·çš„ï¼Œå°±ä¸ç»†è¯´ã€‚</p>

<h3 id="unsafeshufflewriter">unsafeShuffleWriter</h3>

<p>è¿™é‡Œä¹‹æ‰€ä»¥å«ä½œunsafeï¼Œæ˜¯å› ä¸ºè¦æ“çºµå †å¤–å†…å­˜ï¼ŒæŠŠæ•°æ®å†™åˆ°å †å¤–ï¼Œå †å¤–å†…å­˜æ˜¯ä¸å—jvmæ§åˆ¶çš„ï¼Œéœ€è¦æ‰‹åŠ¨è¿›è¡Œç”³è¯·å†…å­˜ä¸é‡Šæ”¾å†…å­˜ç©ºé—´ï¼Œæ‰€ä»¥æ˜¯unsafeçš„ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@Override
public void write(scala.collection.Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException {
  // Keep track of success so we know if we encountered an exception
  // We do this rather than a standard try/catch/re-throw to handle
  // generic throwables.
  boolean success = false;
  try {
    while (records.hasNext()) {
      insertRecordIntoSorter(records.next());
    }
    closeAndWriteOutput();
    success = true;
  } finally {
    if (sorter != null) {
      try {
        sorter.cleanupResources();
      } catch (Exception e) {
        // Only throw this error if we won't be masking another
        // error.
        if (success) {
          throw e;
        } else {
          logger.error("In addition to a failure during writing, we failed during " +
                       "cleanup.", e);
        }
      }
    }
  }
}
</code></pre></div></div>

<p>é™¤äº†æ˜¯å†™åˆ°å †å¤–ï¼Œå…¶ä»–åº”è¯¥è·ŸsortShuffleWriter å·®ä¸å¤šå§ï¼Œæ‡’å¾—å†™äº†ï¼Œä»¥åå‘ç°æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„å†è¡¥å……ã€‚</p>

<h3 id="blockstoreshufflereader">BlockStoreShuffleReader###</h3>

<p>å‰é¢ä¸‰ä¸ªshuffleWriterï¼Œshuffleåˆ†ä¸ºshuffleWriterå’ŒshuffleReaderã€‚shuffleReadråªæœ‰ä¸€ä¸ªå…·ä½“å®ç°ç±»å°±æ˜¯BlockStoreShuffleReaderã€‚çœ‹å¼€å¤´æ³¨é‡Šä¸ºï¼šè¯»å–ï¼ˆstartPartitionå’ŒendPartitionï¼‰ä¹‹é—´çš„partitionçš„æ•°æ®ï¼Œä»å…¶ä»–èŠ‚ç‚¹ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/** Read the combined key-values for this reduce task */
override def read(): Iterator[Product2[K, C]] = {
  val blockFetcherItr = new ShuffleBlockFetcherIterator(
    context,
    blockManager.shuffleClient,
    blockManager,
    mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),
    // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility
    SparkEnv.get.conf.getSizeAsMb("spark.reducer.maxSizeInFlight", "48m") * 1024 * 1024,
    SparkEnv.get.conf.getInt("spark.reducer.maxReqsInFlight", Int.MaxValue))

  // Wrap the streams for compression based on configuration
  val wrappedStreams = blockFetcherItr.map { case (blockId, inputStream) =&gt;
    serializerManager.wrapForCompression(blockId, inputStream)
  }

  val serializerInstance = dep.serializer.newInstance()

  // Create a key/value iterator for each stream
  val recordIter = wrappedStreams.flatMap { wrappedStream =&gt;
    // Note: the asKeyValueIterator below wraps a key/value iterator inside of a
    // NextIterator. The NextIterator makes sure that close() is called on the
    // underlying InputStream when all records have been read.
    serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator
  }

  // Update the context task metrics for each record read.
  val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()
  val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](
    recordIter.map { record =&gt;
      readMetrics.incRecordsRead(1)
      record
    },
    context.taskMetrics().mergeShuffleReadMetrics())

  // An interruptible iterator must be used here in order to support task cancellation
  val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)

  val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
    if (dep.mapSideCombine) {
      // We are reading values that are already combined
      val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
      dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
    } else {
      // We don't know the value type, but also don't care -- the dependency *should*
      // have made sure its compatible w/ this aggregator, which will convert the value
      // type to the combined type C
      val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]
      dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
    }
  } else {
    require(!dep.mapSideCombine, "Map-side combine without Aggregator specified!")
    interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
  }

  // Sort the output if there is a sort ordering defined.
  dep.keyOrdering match {
    case Some(keyOrd: Ordering[K]) =&gt;
      // Create an ExternalSorter to sort the data. Note that if spark.shuffle.spill is disabled,
      // the ExternalSorter won't spill to disk.
      val sorter =
        new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
      sorter.insertAll(aggregatedIter)
      context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)
      context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)
      context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)
      CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())
    case None =&gt;
      aggregatedIter
  }
}
</code></pre></div></div>

<p>é¦–å…ˆæ˜¯å»ºç«‹ä¸€ä¸ª<code class="language-plaintext highlighter-rouge">ShuffleBlockFetcherIterator</code>ï¼Œä¼ å…¥çš„å‚æ•°æœ‰<code class="language-plaintext highlighter-rouge">mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition)</code>,è¿™ä¸ªæ˜¯å¿…é¡»çš„ï¼Œåªå–éœ€è¦çš„partitionçš„æ•°æ®ã€‚</p>

<p>ç‚¹è¿›å»ShuffleBlockFetcherIteratorè¿™ä¸ªç±»ï¼Œå‘ç°è¿™ä¸ªç±»ä¼šè‡ªåŠ¨è°ƒç”¨initialize()æ–¹æ³•ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private[this] def initialize(): Unit = {
  // Add a task completion callback (called in both success case and failure case) to cleanup.
  context.addTaskCompletionListener(_ =&gt; cleanup())

  // Split local and remote blocks.
  val remoteRequests = splitLocalRemoteBlocks()
  // Add the remote requests into our queue in a random order
  fetchRequests ++= Utils.randomize(remoteRequests)
  assert ((0 == reqsInFlight) == (0 == bytesInFlight),
    "expected reqsInFlight = 0 but found reqsInFlight = " + reqsInFlight +
    ", expected bytesInFlight = 0 but found bytesInFlight = " + bytesInFlight)

  // Send out initial requests for blocks, up to our maxBytesInFlight
  fetchUpToMaxBytes()

  val numFetches = remoteRequests.size - fetchRequests.size
  logInfo("Started " + numFetches + " remote fetches in" + Utils.getUsedTimeMs(startTime))

  // Get Local Blocks
  fetchLocalBlocks()
  logDebug("Got local blocks in " + Utils.getUsedTimeMs(startTime))
}
</code></pre></div></div>

<p>è¿™ä¸ªæ–¹æ³•é‡Œé¢ä¼š<code class="language-plaintext highlighter-rouge">fetchUpToMaxBytes()</code>å’Œ<code class="language-plaintext highlighter-rouge">fetchLocalBlocks()</code>,ä¸€ä¸ªæ˜¯å–è¿œç¨‹æ•°æ®ä¸€ä¸ªæ˜¯å–æœ¬åœ°æ•°æ®ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private def fetchUpToMaxBytes(): Unit = {
  // Send fetch requests up to maxBytesInFlight
  while (fetchRequests.nonEmpty &amp;&amp;
    (bytesInFlight == 0 ||
      (reqsInFlight + 1 &lt;= maxReqsInFlight &amp;&amp;
        bytesInFlight + fetchRequests.front.size &lt;= maxBytesInFlight))) {
    sendRequest(fetchRequests.dequeue())
  }
}
</code></pre></div></div>

<p>è¿™é‡Œä¼šè®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼Œé¿å…è¿‡åº¦è´Ÿè½½çš„ã€‚<code class="language-plaintext highlighter-rouge">sendRequest</code>æ¥è¯·æ±‚æ•°æ®ã€‚</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>private[this] def sendRequest(req: FetchRequest) {
  logDebug("Sending request for %d blocks (%s) from %s".format(
    req.blocks.size, Utils.bytesToString(req.size), req.address.hostPort))
  bytesInFlight += req.size
  reqsInFlight += 1

  // so we can look up the size of each blockID
  val sizeMap = req.blocks.map { case (blockId, size) =&gt; (blockId.toString, size) }.toMap
  val remainingBlocks = new HashSet[String]() ++= sizeMap.keys
  val blockIds = req.blocks.map(_._1.toString)

  val address = req.address
  shuffleClient.fetchBlocks(address.host, address.port, address.executorId, blockIds.toArray,
    new BlockFetchingListener {
      override def onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit = {
        // Only add the buffer to results queue if the iterator is not zombie,
        // i.e. cleanup() has not been called yet.
        ShuffleBlockFetcherIterator.this.synchronized {
          if (!isZombie) {
            // Increment the ref count because we need to pass this to a different thread.
            // This needs to be released after use.
            buf.retain()
            remainingBlocks -= blockId
            results.put(new SuccessFetchResult(BlockId(blockId), address, sizeMap(blockId), buf,
              remainingBlocks.isEmpty))
            logDebug("remainingBlocks: " + remainingBlocks)
          }
        }
        logTrace("Got remote block " + blockId + " after " + Utils.getUsedTimeMs(startTime))
      }

      override def onBlockFetchFailure(blockId: String, e: Throwable): Unit = {
        logError(s"Failed to get block(s) from ${req.address.host}:${req.address.port}", e)
        results.put(new FailureFetchResult(BlockId(blockId), address, e))
      }
    }
  )
}
</code></pre></div></div>

<p>åé¢ä¸€å¤§å †ä»£ç ï¼Œåæ­£å°±æ˜¯å–æ•°æ®å—ï¼Œå°±ä¸ç»†çœ‹äº†ã€‚</p>

<p>å–å®Œæ•°æ®ä¹‹åï¼Œå°±é€šè¿‡dep.mapSideCombineåˆ¤æ–­æ˜¯å¦åœ¨mapç«¯åšäº†èšåˆæ“ä½œï¼Œå¦‚æœåšäº†èšåˆæ“ä½œï¼Œè¿™é‡Œçš„(k,v)çš„vå°±æ˜¯CompactBufferç±»å‹ï¼Œå°±è°ƒç”¨combineCombinersByKeyï¼Œå¦‚æœåœ¨mapç«¯æ²¡æœ‰èšåˆï¼Œå°±è¿˜æ˜¯valueç±»å‹ï¼Œå°±combineValuesByKeyã€‚</p>

<p>ä¹‹åå°±åˆ¤æ–­æ˜¯å¦å®šä¹‰äº†æ’åºï¼Œå¦‚æœéœ€è¦æ’åºå°±ç”¨ExternalSorteræ’åºã€‚</p>

<p>åˆ°è¿™é‡Œshuffleè¿‡ç¨‹å°±ç»“æŸå•¦ã€‚</p>

<h2 id="æ€»ç»“">æ€»ç»“##</h2>

<p>å‰ä¸¤ç§shuffleWriterï¼ˆUnsafeShuffleWriteræ²¡ç»†çœ‹ï¼‰é‡Œçš„shuffleWriteç«¯æœ€åå¾—åˆ°çš„æ–‡ä»¶éƒ½åªæ˜¯ä¸€ä¸ªIndexFileï¼Œè¿™è·Ÿ<a href="http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/">æ—©æœŸçš„shuffleæœºåˆ¶</a>è¿˜æ˜¯ä¸ä¸€æ ·çš„ã€‚</p>
:ET